paper_id,meta_review
iclr_2018_ryQu7f-RZ,"This paper analyzes a problem with the convergence of Adam, and presents a solution. It identifies an error in the convergence proof of Adam (which also applies to related methods such as RMSProp) and gives a simple example where it fails to converge. The paper then repairs the algorithm in a way that guarantees convergence without introducing much computational or memory overhead. There ought to be a lot of interest in this paper: Adam is a widely used algorithm, but sometimes underperforms SGD on certain problems, and this could be part of the explanation. The fix is both principled and practical. Overall, this is a strong paper, and I recommend acceptance.
"
iclr_2018_BJ8vJebC-,"The pros and cons of this paper cited by the reviewers can be summarized below:

Pros:
* The paper is a first attempt to investigate an under-studied area in neural MT (and potentially other applications of sequence-to-sequence models as well)
* This area might have a large impact; existing models such as Google Translate fail badly on the inputs described here
* Experiments are very carefully designed and thorough
* Experiments on not only synthetic but also natural noise add significant reliability to the results
* Paper is well-written and easy to follow

Cons:
* There may be better architectures for this problem than the ones proposed here
* Even the natural noise is not entirely natural, e.g. artificially constrained to exist within words
* Paper is not a perfect fit to ICLR (although ICLR is attempting to cast a wide net, so this alone is not a critical criticism of the paper)

This paper had uniformly positive reviews and has potential for large real-world impact."
iclr_2018_Hk2aImxAb,"As stated by reviewer 3 ""This paper introduces a new model to perform image classification with limited computational resources at test time. The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer.""
As stated by reviewer 2 ""My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017). "".  The authors assert novelty in the sense that they provide a solution to improve computational efficiency and focus on this aspect of the problem. Overall, the technical innovation is not huge, but I think this could be a very useful idea in practice.
"
iclr_2018_HJGXzmspb,"High quality paper, appreciated by reviewers, likely to be of substantial interest to the community. It's worth an oral to facilitate a group discussion."
iclr_2018_HJGv1Z-AW,"Important problem (analyzing the properties of emergent languages in multi-agent reference games), a number of interesting analyses (both with symbolic and pixel inputs), reaching a finding that varying the environment and restrictions on language result in variations in the learned communication protocols (which in hindsight is that not surprising, but that's hindsight). While the pixel experiments are not done with real images, it's an interesting addition the literature nonetheless.  "
iclr_2018_Hkbd5xZRb,"This work introduces a trainable signal representation for spherical signals (functions defined in the sphere) which are rotationally equivariant by design, by extending CNNs to the corresponding group SO(3). The method is implemented efficiently using fast Fourier transforms on the sphere and illustrated with compelling tasks such as 3d shape recognition and molecular energy prediction.

Reviewers agreed this is a solid, well-written paper, which demonstrates the usefulness of group invariance/equivariance beyond the standard Euclidean translation group in real-world scenarios. It will be a great addition to the conference. "
iclr_2018_S1CChZ-CZ,"this submission presents a novel way in which a neural machine reader could be improved. that is, by learning to reformulate a question specifically for the downstream machine reader. all the reviewers found it positive, and so do i."
iclr_2018_rJTutzbA-,"The reviewers unanimously recommended that this paper be accepted, as it contains an important theoretical result that there are problems for which heavy-ball momentum cannot outperform SGD. The theory is backed up by solid experimental results, and the writing is clear. While the reviewers were originally concerned that the paper was missing a discussion of some related algorithms (ASVRG and ASDCA) that were handled in discussion.
"
iclr_2018_Hk6kPgZA-,"This paper attracted strong praise from the reviewers, who felt that it was of high quality and originality.  The broad problem that is being tackled is clearly of great importance.

This paper also attracted the attention of outside experts, who were more skeptical of the claims made by the paper. The technical merits do not seem to be in question, but rather, their interpretation/application. The perception by a community as to whether an important problem has been essentially solved can affect the choices made by other reviewers when they decide what work to pursue themselves, evaluate grants, etc. It's important that claims be conservative and highlight the ways in which the present work does not fully address the broader problem of adversarial examples.

Ultimately, it has been decided that the paper will be of great interest to the community. The authors have also been entrusted with the responsibility to consider the issues raised by the outside expert (and then echoed by the AC) in their final revisions.

One final note: In their responses to the outside expert, the authors several times remark that the guarantees made in the paper are, in form, no different from standard learning-theoretic claims: ""This criticism, however, applies to many learning-theoretic results (including those applied in deep learning)."" I don't find any comfort in this statement. Learning theorists have often focused on the form of the bounds (sqrt(m) dependence and, say, independence from the # of weights) and then they resort to empirical observations of correlation to demonstrate that the value of the bound is predictive for generalization. because the bounds are often meaningless (""vacuous"") when evaluated on real data sets. (There are some recent examples bucking this trend.) In a sense, learning theorists have gotten off easy. Adversarial examples, however, concern security, and so there is more at stake. The slack we might afford learning theorists is not appropriate in this new context. I would encourage the authors to clearly explain any remaining work that needs to be done to move from ""good enough for learning theory"" to ""good enough for security"". The authors promise to outline important future work / open problems for the community. I definitely encourage this.




"
iclr_2018_HktK4BeCZ,"The reviewers are unanimous in finding the work in this paper highly novel and significant.  They have provided detailed discussions to back up this assessment.  The reviewer comments surprisingly included a critique that  ""the scientific content of the work has critical conceptual flaws"" (!)  However, the author rebuttal persuaded the reviewers that the concerns were largely addressed."
iclr_2018_HkL7n1-0b,"This paper proposes a new generative model that has the stability of variational autoencoders (VAE) while producing better samples. The authors clearly compare their work to previous efforts that combine VAEs and Generative Adversarial Networks with similar goals.  Authors show that the proposed algorithm is a generalization of Adversarial Autoencoder (AAE) and minimizes Wasserstein distance between model and target distribution. The paper is well written with convincing results. Reviewers agree that the algorithm is novel and practical; and close connections of the algorithm to related approaches are clearly discussed with useful insights.  Overall, the paper is strong and I recommend acceptance."
iclr_2018_B1QRgziT-,"This paper presents impressive results on scaling GANs to ILSVRC2012 dataset containing a large number of classes. To achieve this, the authors propose ""spectral normalization"" to normalize weights and stabilize training which turns out to help in overcoming mode collapse issues.  The presented methodology is principled and well written. The authors did a good job in addressing reviewer's comments and added more comparative results on related approaches to demonstrate the superiority of the proposed methodology. The reviewers agree that this is a great step towards improving the training of GANs.  I recommend acceptance."
iclr_2018_BJOFETxR-,"There was some debate between the authors and an anonymous commentator on this paper.  The feeling of the commentator was that existing work (mostly from the PL community) was not compared to appropriately and, in fact, performs better than this approach.  The authors point out that their evaluation is hard to compare directly but that they disagreed with the assessment.  They modified their texts to accommodate some of the commentator's concerns; agreed to disagree on others; and promised a fuller comparison to other work in the future.

I largely agree with the authors here and think this is a good and worthwhile paper for its approach.

PROS:
1. well written
2. good ablation study
3. good evaluation including real bugs identified in real software projects
4. practical for real world usage

CONS:
1. perhaps not well compared to existing PL literature or on existing datasets from that community
2. the architecture (GGNN) is not a novel contribution"
iclr_2018_B1gJ1L2aW,The paper characterizes the latent space of adversarial examples and introduces the concept of local intrinsic dimenstionality (LID). LID  can be used to detect adversaries as well build better attacks as it characterizes the space in which DNNs might be vulnerable. The experiments strongly support their claim.
iclr_2018_HkwZSG-CZ,"Viewing language modeling as a matrix factorization problem, the authors argue that the low rank of word embeddings used by such models limits their expressivity and show that replacing the softmax in such models with a mixture of softmaxes provides an effective way of overcoming this bottleneck. This is an interesting and well-executed paper that provides potentially important insight. It would be good to at least mention prior work related to the language modeling as matrix factorization perspective (e.g. Levy & Goldberg, 2014)."
iclr_2018_Sk2u1g-0-,Looks like a great contribution to ICLR. Continuous adaptation in nonstationary (and competitive) environments is something that an intelligent agent acting in the real world would need to solve and this paper suggests that a meta-learning approach may be quite appropriate for this task.
iclr_2018_S1JHhv6TW,"This paper proposes improvements to WaveNet by showing that increasing connectivity provides superior models to increasing network size. The reviewers found both the mathematical treatment of the topic and the experiments to be of higher quality that most papers they reviewed, and were unanimous in recommending it for acceptance in the conference. I see no reason not to give it my strongest recommendation as well."
iclr_2018_HkfXMz-Ab,"This paper presents a novel and interesting sketch-based approach to conditional program generation. I will say upfront that it is worth of acceptance, based on its contribution and the positivity of the reviews. I am annoyed to see that the review process has not called out the authors' lack of references to the decently body of existing work on generating structure on neural sketch programming and on generating under grammatical constraint. The authors' will need look no further than the proceedings of the *ACL conferences of the last few years to find papers such as:
* Dyer, Chris, et al. ""Recurrent Neural Network Grammars."" Proceedings of NAACL-HLT (2016).
* Kuncoro, Adhiguna, et al. ""What Do Recurrent Neural Network Grammars Learn About Syntax?."" Proceedings of EACL (2016).
* Yin, Pengcheng, and Graham Neubig. ""A Syntactic Neural Model for General-Purpose Code Generation."" Proceedings of ACL (2017).
* Rabinovich, Maxim, Mitchell Stern, and Dan Klein. ""Abstract Syntax Networks for Code Generation and Semantic Parsing."" Proceedings of ACL (2017).

Or other work on neural program synthesis, with sketch based methods:
* Gaunt, Alexander L., et al. ""Terpret: A probabilistic programming language for program induction."" arXiv preprint arXiv:1608.04428 (2016).
* Riedel, Sebastian, Matko Bosnjak, and Tim Rocktäschel. ""Programming with a differentiable forth interpreter."" CoRR, abs/1605.06640 (2016).

Likewise the references to the non-neural program synthesis and induction literature are thin, and the work is poorly situated as a result.

It is a disappointing but mild failure of the scientific process underlying peer review for this conference that such comments were not made. The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting."
iclr_2018_Hk99zCeAb,"The main contribution of the paper is a technique for training GANs which consists in progressively increasing the resolution of generated images by gradually enabling layers in the generator and the discriminator. The method is novel, and outperforms the state of the art in adversarial image generation both quantitatively and qualitatively. The evaluation is carried out on several datasets; it also contains an ablation study showing the effect of contributions (I recommend that the authors follow the suggestions of AnonReviewer2 and further improve it). Finally, the source code is released which should facilitate the reproducibility of the results and further progress in the field.

AnonReviewer1 has noted that the authors have revealed their names through GitHub, thus violating the double-blind submission requirement of ICLR; if not for this issue, the reviewer’s rating would have been 8. While these concerns should be taken very seriously, I believe that in this particular case the paper should still be accepted for the following reasons:
1) the double blind rule is new for ICLR this year, and posting the paper on arxiv is allowed;
2) the author list has been revealed through the supplementary material (Github page) rather than the paper itself;
3) all reviewers agree on the high impact of the paper, so having it presented and discussed at the conference would be very useful for the community."
iclr_2018_H1tSsb-AW,The reviewers are satisfied that this paper makes a good contribution to policy gradient methods.
iclr_2018_BkisuzWRW,The authors have proposed a method for imitating a given control trajectory even if it is sparsely sampled. The method relies on a parametrized skill function and uses a triplet loss for learning a stopping metric and for a dynamics consistency loss. The method is demonstrated with real robots on a navigation task and a knot-tying task. The reviewers agree that it is a novel and interesting alternative to pure RL which should inspire good discussion at the conference.
iclr_2018_rkRwGg-0Z,"Very solid paper exploring an interpretation of LSTMs.
good reviewss"
iclr_2018_Hy7fDog0b,"All three reviewers were positive about the paper, finding it to be on an interesting topic and with broad applicability. The results were compelling and thus the paper is accepted. "
iclr_2018_rJWechg0Z,"This paper presents a nice approach to domain adaptation that improves empirically upon previous work, while also simplifying tuning and learning.
"
iclr_2018_B1zlp1bRW,"This paper is generally very strong. I do find myself agreeing with the last reviewer though, that tuning hyperparameters on the test set should not be done, even if others have done it in the past. (I say this having worked on similar problems myself.) I would strongly encourage the authors to re-do their experiments with a better tuning regime."
iclr_2018_ryUlhzWCZ,"This paper proposes a theoretically-motivated method for combining reinforcement learning and imitation learning. There was some disagreement amongst the reviewers, but the AC was satisfied with the authors' rebuttal."
iclr_2018_SJJinbWRZ,"The reviewers agree that the paper presents nice results on model based RL with an ensemble of models. The limited novelty of the methods is questioned by one reviewer and briefly by the others, but they all agree that this paper's results justify its acceptance."
iclr_2018_Hy6GHpkCW,"This work presents a RNN tailored to generate sketch drawings. The model has novel elements and advances specific to the considered task, and allows for free generation as well as generation with (partial) input. The results are very satisfactory. Importantly, as part of this work a large dataset of sketch drawings is released. The only negative aspect is the insufficient evaluation, as pointed out by R1 who points out the need for baselines and evaluation metrics. R1’s concerns have been acknowledged by the authors but not really addressed in the revision. Still, this is a very interesting contribution."
iclr_2018_SJaP_-xAb,"In this paper the authors show how to allow deep neural network training on logged contextual bandit feedback. The newly introduced framework comprises a new kind of output layer and an associated training procedure. This is a solid piece of work and a significant contribution to the literature, opening up the way for applications of deep neural networks when losses based on manual feedback and labels is not possible. "
iclr_2018_Byt3oJ-0W,"This paper with the self-explanatory title was well received by the reviewers and, additionally, comes with available code. The paper builds on prior work (Sinkhorn operator) but shows additional, significant amount of work to enable its application and inference in neural networks.  There were no major criticisms by the reviewers, other than obvious directions for improvement which should have been already incorporated in the paper, issues with clarity and a little more experimentation. To some extent, the authors addressed the issues in the revised version.   "
iclr_2018_rk07ZXZRb,"This is a paper introducing a hierarchical RL method which incorporates the learning of a latent space, which enables the sharing of learned skills.

The reviewers unanimously rate this as a good paper. They suggest that it can be further improved by demonstrating the effectiveness through more experiments, especially since this is a rather generic framework. To some extent, the authors have addressed this concern in the rebuttal.
"
iclr_2018_S1DWPP1A-,"This paper aims to improve on the intrinsically motivated goal exploration framework by additionally incorporating representation learning for the space of goals. The paper is well motivated and follows a significant direction of research, as agreed by all reviewers. In particular, it provides a means for learning in complex environments, where manually designed goal spaces would not be available in practice. There had been significant concerns over the presentation of the paper, but the authors put great effort in improving the manuscript according to the reviewers’ suggestions, raising the average rating by 2 points after the rebuttal. "
iclr_2018_ryRh0bb0Z,"This paper presents an unsupervised GAN-based model for disentagling the multiple views of the data and their content.

Overall it seems that this paper was well received by the reviewers, who find it novel and significant . The consensus is that the results are promising.

There are some concerns, but the major ones listed below have been addressed in the rebuttal. Specifically:
-	R3 had a concern about the experimental evaluation, which has been addressed in the rebuttal.
-	R2 had a concern about a problem inherent in this setting (what is treated as “content”), and the authors have clarified in the discussion the assumptions under which such methods operate.
-	R1 had concerns related to how the proposed model fits in the literature. Again, the authors have addressed this concern adequately.
"
iclr_2018_SyYe6k-CW,"This paper is not aimed at introducing new methodologies (and does not claim to do so), but instead it aims at presenting a well-executed empirical study. The presentation and outcomes of this study are quite instructive, and with the ever-growing list of academic papers, this kind of studies are a useful regularizer. "
iclr_2018_H15odZ-C-,"The paper presents a modified sampling method for improving the quality of interpolated samples in deep generative models.

There is not a great amount of technical contributions in the paper, however it is written in a very clear way, makes interesting observations and analyses and shows promising results. Therefore, it should be of interest to the ICLR community."
iclr_2018_B1X0mzZCW,"This paper introduces a student-teacher method for learning from labels of varying quality (i.e. varying fidelity data). This is an interesting idea which shows promising results.

Some further connections to various kinds of semi-supervised and multi-fidelity learning would strengthen the paper, although understandably it is not easy to cover the vast literature, which also spans different scientific domains. One reviewer had a concern about some design decisions that seemed ad-hoc, but at least the authors have intuitively and experimentally justified them."
iclr_2018_SJzRZ-WCZ,"This paper characterizes the induced geometry of the latent space of deep generative models. The motivation is established well, such that the paper convincingly discusses the usefulness derived from these insights. For example, the results uncover issues with the currently used methods for variance estimation in deep generative models. The technique invoked to mitigate this issue does feel somehow ad-hoc, but at least it is well motivated.

One of the reviewers correctly pointed out that there is limited novelty in the theoretical/methodological aspect. However, I agree with the authors’ rebuttal in that characterizing geometries on stochastic manifolds is much less studied and demonstrated, especially in the deep learning community. Therefore, I believe that this paper will be found useful by readers of the ICLR community, and will stimulate future research. "
iclr_2018_Hk3ddfWRW,"This paper presents a sampling inference method for learning in multi-modal demonstration scenarios. Reference to imitation learning causes some confusion with the IRL domain, where this terminology is usually encountered. Providing a real application to robot reaching, while a relatively simple task in robotics, increases the difficulty and complexity of the demonstration. That makes it impressive, but also difficult to unpick the contributions and reproduce even the first demonstration. It's understandable at a meeting on learning representations that the reviewers wanted to understand why existing methods for learning multi-modal distributions would not work, and get a better understanding of the tradeoffs and limitations of the proposed method. The CVAE comparison added to the appendix during the rebuttal period just pushed this paper over the bar. The demonstration is simplified, so much easier to reproduce, making it more feasible others will attempt to reproduce the claims made here.
"
iclr_2018_H1zriGeCZ,"This paper introduces an algorithm for optimization of discrete hyperparameters based on compressed sensing, and compares against standard gradient-free optimization approaches.

As the reviewers point out, the provable guarantees (as is usually the case) don't quite make it to the main results section, but are still refreshing to see in hyperparameter optimization.

The method itself is relatively simple compared to full-featured Bayesopt (spearmint), although not as widely applicable.
"
iclr_2018_H1Xw62kRZ,"Below is a summary of the pros and cons of the proposed paper:

Pros:
* Proposes a novel method to tune program synthesizers to generate correct programs and prune search space, leading to better and more efficient synthesis
* Shows small but substantial gains on a standard benchmark

Cons:
* Reviewers and commenters cited a few clarity issues, although these have mostly been resolved
* Lack of empirical comparison with relevant previous work (e.g. Parisotto et al.) makes it hard to determine their relative merit

Overall, this seems to be a solid, well-evaluated contribution and seems to me to warrant a poster presentation.

Also, just a few notes from the area chair to potentially make the final version better:

The proposed method is certainly different from the method of Parisotto et al., but it is attempting to solve the same problem: the lack of consideration of the grammar in neural program synthesis models. The relative merit is stated to be that the proposed method can be used when there is no grammar specification, but the model of Parisotto et al. also learns expansion rules from data, so no explicit grammar specification is necessary (as long as a parser exists, which is presumably necessary to perform the syntax checking that is core to the proposed method). It would have been ideal to see an empirical comparison between the two methods, but this is obviously a lot of work. It would be nice to have the method acknowledged more prominently in the description, perhaps in the introduction, however.

It is nice to see a head-nod to Guu et al.'s work on semantic parsing (as semantic parsing from natural language is also highly relevant). There is obviously a lot of work on generating structured representations from natural lanugage, and the following two might be particularly relevant given their focus on grammar-based formalisms for code synthesis from natural language:

* ""A Syntactic Neural Model for General-purpose Code Generation"" Yin and Neubig ACL 2017.
* ""Abstract Syntax Networks for Code Generation and Semantic Parsing"" Rabinovich et al. ACL 2017
"
iclr_2018_HJzgZ3JCW,"The paper presents a modification of the Winograd convolution algorithm that reduces the number of multiplications in a forward pass of a CNN with minimal loss of accuracy. The reviewers brought up the strong results, the readability of the paper, and the thoroughness of the experiments. One concern brought up was the applicability to deeper network structures. This was acknowledged by the authors to be a subject of future work. Another issue raised was the question of theoretical vs. actual speedup. Again, this was acknowledged by the authors to be an eventual goal but subject to further systems work and architecture optimizations. The reviewers were consistent in their support of the paper. I follow their recommendation: Accept.
"
iclr_2018_Sk6fD5yCb,"This paper describes a new library for forward propagation of binary CNNs. R1 for clarification on the contributions and novelty, which the authors provided. They subsequently updated their score. I think that optimized code with permissive licensing (as R2 points out) benefits the community. The paper will benefit those who decide to work with the library."
iclr_2018_r11Q2SlRW,"This paper proposes a real-time method for synthesizing human motion of highly complex styles. The key concern raised by R2 was that the method did not depart greatly from a standard LSTM: parts of the generated sequences are conditioned on generated data as opposed to ground truth data. However, the reviewer thought the idea was sensible and the results were very good in practice. R1 also agreed that the results were very good and asked for a more detailed analysis of conditioning length and some clarification. R3 brought up similarities to Professor Forcing (Goyal et al. 2016) -- also noted by R2 -- and Learning Human Motion Models for Long-term Predictions (Ghosh et al. 2017) -- noting not peer-reviewed. R3 also raised the open issue of how to best evaluate sequence prediction models like these. They brought up an interesting point, which was that the synthesized motions were low quality compared to recent works by Holden et al., however, they acknowledged that by rendering the characters this exposed the motion flaws. The authors responded to all of the reviews, committing to a comparison to Scheduled Sampling, though a comparison to Professor Forcing was proving difficult in the review timeline. While this paper may not receive the highest novelty score, I agree with the reviewers that it has merit. It is well written, has clear and reasonably thorough experiments, and the results are indeed good."
iclr_2018_SyMvJrdaW,"This paper proposes a “warp operator” based on Taylor expansion that can replace a block of layers in a residual network, allowing for parallelization. Taking advantage of multi-GPU parallelization the paper shows increased speedup with similar performance on CIFAR-10 and CIFAR-100. R1 asked for clarification on rotational symmetry. The authors instead removed the discussion that was causing confusion (replacing with additional experimental results that had been requested). R2 had the most detailed review and thought that the idea and analysis were interesting. They also had difficulty following the discussion of symmetry (noted above). They also pointed out several other issues around clarity and had several suggestions for improving the experiments which seem to have been taken to heart by the authors, who detailed their changes in response to this review. There was also an anonymous public comment that pointed out a “fatal mathematical flaw and weak experiments”. There was a lengthy exchange between this reviewer and the authors, and the paper was actually corrected and clarified in the process. This anonymous poster was rather demanding of the authors, asking for latex-formatted equations, pseudo-code, and giving direction on how to respond to his/her rebuttal. I don't agree with the point that the paper is flawed by ""only"" presenting a speed-up over ResNet, and furthermore the comment of ""not everyone has access to parallelization"" isn’t a fair criticism of the paper."
iclr_2018_HktRlUlAZ,"The paper proposes a new deep architecture based on polar transformation for improving rotational invariance. The proposed method is interesting and the experimental results strong classification performance on small/medium-scale datasets (e.g., rotated MNIST and its variants with added translations and clutters, ModelNet40, etc.). It will be more impressive and impactful if the proposed method can bring performance improvement on large-scale, real datasets with potentially cluttered scenes (e.g., Imagenet, Pascal VOC, MS-COCO, etc.)."
iclr_2018_H1VGkIxRZ,"The reviewers agree that the method is simple, the results are quite good, and the paper is well written. The issues the reviewers brought up have been adequately addressed. There is a slight concern about novelty, however the approach will likely be quite useful in practice."
iclr_2018_Skj8Kag0Z,"This paper provides a simple technique for stabilizing GAN training, and works over a variety of GAN models.

One of the reviewers expressed concerns with the value of the theory. I think that it would be worth emphasizing that similar arguments could be made for alternating gradient descent, and simultaneous gradient descent. In this case, if possible, it would be good to highlight how the convergence of the prediction method approach differs from the alternating descent approach. Otherwise, highlight that this theory simply shows that the prediction method is not a completely crazy idea (in that it doesn't break existing theory).

Practically, I think the experiments are sufficiently interesting to show that this approach has promise. I don't see the updated results for Stacked GAN for a fixed set of epochs (20 and 40 at different learning rates). Perhaps put this below Table 1."
iclr_2018_rJXMpikCZ,"The authors appear to have largely addressed the concerns of the reviewers and commenters regarding related work and experiments. The results are strong, and this will likely be a useful contribution for the graph neural network literature."
iclr_2018_BywyFQlAW,"The submission formulates self paced learning as a specific iterative mini-max optimization, which incorporates both a risk minimization step and a submodular maximization for selecting the next training examples.

The strengths of the paper lie primarily in the theoretical analysis, while the experiments are somewhat limited to simple datasets: News20, MNIST, & CIFAR10.  Additionally, the main paper is probably too long in its current form, and could benefit from some of the proof details being moved to the appendix.

"
iclr_2018_B1n8LexRZ,"This paper presents a learned inference architecture which generalizes HMC. It defines a parameterized family of MCMC transition operators which share the volume preserving structure of HMC updates, which allows the acceptance ratio to be computed efficiently. Experiments show that the learned operators are able to mix significantly faster on some simple toy examples, and evidence is presented that it can improve posterior inference for a deep latent variable model. This paper has not quite demonstrated usefulness of the method, but it is still a good proof of concept for adaptive extensions of HMC.

"
iclr_2018_H1Yp-j1Cb,"This paper presents a GAN training algorithm motivated by online learning. The method is shown to converge to a mixed Nash equilibrium in the case of a shallow discriminator. In the initial version of the paper, reviewers had concerns about weak baselines in the experiments, but the updated version includes comparisons against a variety of modern GAN architectures which have been claimed to fix mode dropping. This seems to address the main criticism of the reviewers. Overall, this paper seems like a worthwhile addition to the GAN literature."
iclr_2018_rkQkBnJAb,"This is another paper, similar in spirit to the Wasserstein GAN and Cramer GAN, which uses ideas from optimal transport theory to define a more stable GAN architecture. It combines both a primal representation (with Sinkhorn loss) with a minibatch-based energy distance between distributions.
The experiments show that the OT-GAN produces sharper samples than a regular GAN on various datasets. While more could probably be done to distinguish the model from WGANs and Cramer GANs, this paper seems like a worthwhile contribution to the GAN literature and merits publication.
"
iclr_2018_S1HlA-ZAZ,"This paper presents a distributed memory architecture based on a generative model with a VAE-like training criterion. The claim is that this approach is easier to train than other memory-based architectures. The model seems sound, and it is described clearly. The experimental validation seems a bit limited: most of the comparisons are against plain VAEs, which aren't a memory-based architecture. The discussion of ""one-shot generalization"" is confusing, since the task is modified without justification to have many categories and samples per category. The experiment of Section 4.4 seems promising, but this needs to be expanded to more tasks and baselines since it's the only experiment that really tests the Kanerva Machine as a memory architecture. Despite these concerns, I think the idea is promising and this paper contributes usefully to the discussion, so I recommend acceptance."
iclr_2018_r1gs9JgRZ,"meta score: 8

The paper explores mixing 16- and 32-bit floating point arithmetic for NN training with CNN and LSTM experiments on a variety of tasks

Pros:
 - addresses an important practical problem
 - very wide range of experimentation, reported in depth

Cons:
 - one might say the novelty was minor, but the novelty comes from the extensive analysis and experiments"
iclr_2018_Sy8XvGb0-,This paper clearly surveys a set of methods related to using generative models to produce samples with desired characteristics.  It explores several approaches and extensions to the standard recipe to try to address some weaknesses.  It also demonstrates a wide variety of tasks.  The exposition and figures are well-done.
iclr_2018_ByOExmWAb,"This paper makes progress on the open problem of text generation with GANs, by a sensible combination of novel approaches.   The method was described clearly, and is somewhat original.   The only problem is the hand-engineering of the masking setup.
"
iclr_2018_B1jscMbAW,"The paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks, mimicking a class of standard algorithms.  The paper is clearly written, and the experiments are diverse.  It also seems to point in the direction of a wider class of algorithm-inspired neural net architectures."
iclr_2018_HyjC5yWCW,"R3 summarizes the reasons for the decision on this paper: ""The universal learning algorithm approximator result is a nice result, although I do not agree with the other reviewer that it is a  ""significant contribution to the theoretical understanding of meta-learning,"" which the authors have reinforced (although it can probably be considered a significant contribution to the theoretical understanding of MAML in particular). Expressivity of the model or algorithm is far from the main or most significant consideration in a machine learning problem, even in the standard supervised learning scenario. Questions pertaining to issues such as optimization and model selection are just as, if not more, important. These sorts of ideas are explored in the empirical part of the paper, but I did not find the actual experiments in this section to be very compelling. Still, I think the universal learning algorithm approximator result is sufficient on its own for the paper to be accepted."""
iclr_2018_S1ANxQW0b,"The main idea of policy-as-inference is not new, but it seems to be the first application of this idea to deep RL, and is somewhat well motivated.  The computational details get a bit hairy, but the good experimental results and the inclusion of ablation studies pushes this above the bar.
"
iclr_2018_SyX0IeWAW,"This paper presents a fairly straightforward algorithm for learning a set of sub-controllers that can be re-used between tasks.  The development of these concepts in a relatively clear way is a nice contribution.  However, the real problem is how niche the setup is.  However, it's over the bar in general."
iclr_2018_B1EA-M-0Z,"This paper presents several theoretical results linking deep, wide neural networks to GPs.  It even includes illuminating experiments.

Many of the results were already developed in earlier works. However, many at ICLR may be unaware of these links, and we hope this paper will contribute to the discussion.
"
iclr_2018_SyqShMZRb,"This paper presents a more complex version of the grammar-VAE, which can be used to generate structured discrete objects for which a grammar is known, by adding a second 'attribute grammar', inspired by Knuth.

Overall, the idea is a bit incremental, but the space is wide open and I think that structured encoder/decoders is an important direction.  The experiments seem to have been done carefully (with some help from the reviewers) and the results are convincing."
iclr_2018_rywDjg-RW,"The pros and cons of this paper cited by the reviewers can be summarized below:

Pros:
* The method proposed here is highly technically sophisticated and appropriate for the problem of program synthesis from examples
* The results are convincing, demonstrating that the proposed method is able to greatly speed up search in an existing synthesis system

Cons:
* The contribution in terms of machine learning or representation learning is minimal (mainly adding an LSTM to an existing system)
* The overall system itself is quite complicated, which might raise the barrier of entry to other researchers who might want to follow the work, limiting impact

In our decision, the fact that the paper significantly moves forward the state of the art in this area outweighs the concerns about lack of machine learning contribution or barrier of entry."
iclr_2018_rJl3yM-Ab,"The pros and cons of this paper cited by the reviewers can be summarized below:

Pros:
* Solid experimental results against strong baselines on a task of great interest
* Method presented is appropriate for the task
* Paper is presented relatively clearly, especially after revision

Cons:
* The paper is somewhat incremental. The basic idea of aggregating across multiple examples was presented in Kadlec et al. 2016, but the methodology here is different.
"
iclr_2018_B1ZvaaeAZ,"This paper explores the training of CNNs which have reduced-precision activations. By widening layers, it shows less of an accuracy hit on ILSVRC-12 compared to other recent reduced-precision networks. R1 was extremely positive on the paper, impressed by its readability and the quality of comparison to previous approaches (noting that results with 2-bit activations and 4-bit weights matched FP baselines). This seems very significant to me. R1 also pointed out that the technique used the same hyperparameters as the original training scheme, improving reproducibility/accessibility. R1 asked about application to MobileNets, and the authors reported some early results showing that the technique also worked with smaller network/architectures designed for low-memory hardware. R2 was less positive on the paper, with the main criticism being that the overall technical contribution of the paper was limited. They also were concerned that the paper seemed to motivate based on reducing memory footprint, but the results were focused on reducing computation. R3 liked the simplicity of the idea and comprehensiveness of the results. Like R2, they thought the paper was limited novelty. In their response to R3, the authors defended the novelty of the paper. I tend to side with the authors that very few papers target quantization at no accuracy loss. Moreover, the paper targets training, which also receives much less attention in the model compression / reduced precision literature. Is the architecture really novel? No. But does the experimental work investigate an important tradeoff? Yes."
iclr_2018_rkmu5b0a-,"This paper presents an analysis of using multiple generators in a GAN setup, to address the mode-collapse problem. R1 was generally positive about the paper, raising the concern on how to choose the number of generators, and also whether parameter sharing was essential. The authors reported back on parameter sharing, showing its benefits yet did not have any principled method of selecting the number of generators. R2 was less positive about the paper, pointing out that mixture GANs and multiple generators have been tried before. They also raised concern with the (flawed) Inception score as the basis for comparison. R2 also pointed out that fixing the mixing proportions to uniform was an unrealistic assumption. The authors responded to these claims, clarifying the differences between this paper and the previous mixture GAN/multiple generator papers, and reporting FID scores. R3 was generally positive, also citing some novelty concerns similar to that of R2. I acknowledge the authors detailed responses to the reviews (in particular in response to R2) and I believe that the majority of concerns expressed have now been addressed. I also encourage the authors to include the FID scores in the final version of the paper."
iclr_2018_rkHVZWZAZ,"This paper presents a nice set of results on a new RL algorithm. The main downside is the limitation to the Atari domain, but otherwise the ablation studies are nice and the results are strong."
iclr_2018_HkUR_y-RZ,"This paper generally presents a nice idea, and some of the modifications to searn/lols that the authors had to make to work with neural networks are possibly useful to others. Some weaknesses exist in the evaluation that everyone seems to agree on, but disagree about importance (in particular, comparison to things like BLS and Mixer on problems other than MT).

A few side-comments (not really part of meta-review, but included here anyway):
- Treating rollin/out as a hyperparameter is not unique to this paper; this was also done by Chang et al., NIPS 2016, ""A credit assignment compiler...""
- One big question that goes unanswered in this paper is ""why does learned rollin (or mixed rollin) not work in the MT setting."" If the authors could add anything to explain this, it would be very helpful!
- Goldberg & Nivre didn't really introduce the _idea_ of dynamic oracles, they simply gave it that name (e.g., in the original Searn paper, and in most of the imitation learning literature, what G&n call a ""dynamic oracle"" everyone else just calls an ""oracle"" or ""expert"")"
iclr_2018_SyZipzbCb,"As identified by most reviewers, this paper does a very thorough empirical evaluation of a relatively straightforward combination of known techniques for distributed RL. The work also builds on ""Distributed prioritized experience replay"", which could be noted more prominently in the introduction."
iclr_2018_ry80wMW0W,"Overall this paper seems to make an interesting contribution to the problem of subtask discovery, but unfortunately this only works in a tabular setting, which is quite limiting."
iclr_2018_rJl63fZRb,This paper is somewhat incremental on recent prior work in a hot area; it has some weaknesses but does move the needle somewhat on these problems.
iclr_2018_S1D8MPxA-,"The paper proposes a new sparse matrix representation based on Viterbi algorithm with high and fixed index compression ratio regardless of the pruning rate.  The method allows for faster parallel decoding and achieves improved compression of index data storage requirement over existing methods (e.g., magnitude-based pruning) while maintaining the pruning rate. The quality of paper seems solid and of interest to a subset of the ICLR audience."
iclr_2018_ByS1VpgRZ,"The paper proposes a simple modification to conditional GANs, where the discriminator involves an inner product term between the condition vector y and the feature vector of x. This formulation is reasonable and well motivated from popular models (e.g., log-linear, Gaussians). Experimentally, the proposed method is evaluated on conditional image generation and super-resolution tasks, demonstrating improved qualitative and qualitative performance over the existing state-of-the-art (AC-GAN).
"
iclr_2018_S1v4N2l0-,"The paper proposes a new way of learning image representations from unlabeled data by predicting the image rotations. The problem formulation implicitly encourages the learned representation to be informative about the (foreground) object and its rotation. The idea is simple, but it turns out to be very effective. The authors demonstrate strong performance in multiple transfer learning scenarios, such as  ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification."
iclr_2018_rJGZq6g0-,"An interesting paper, generally well-written. Though it would be nice to see that the methods and observations generalize to other datasets, it is probably too much to ask as datasets with required properties do not seem to exist.  There is a clear consensus to accept the paper.

+ an interesting extension of previous work on emergent communications (e.g., referential games)
+ well written paper
 "
iclr_2018_rytstxWAW,"Graph neural networks (incl. GCNs) have been shown effective on a large range of tasks. However, it has been so far hard (i.e. computationally expensive or requiring the use of heuristics) to apply them to large graphs. This paper aims to address this problem and the solution is clean and elegant. The reviewers generally find it well written and interesting. There were some concerns about the comparison to GraphSAGE (an alternative approach), but these have been addressed in a subsequent revision.

+ an important problem
+ a simple approach
+ convincing results
+ clear and well written
"
iclr_2018_H1vEXaxA-,"The paper considers learning an NMT systems while pivoting through images. The task is formulated as a referential game. From the modeling and set-up perspective it is similar to previous work in the area of emergent communication / referential games, e.g., Lazaridou et al (ICLR 17) and especially to Havrylov & Titov (NIPS 17), as similar techniques are used to handle the variable-length channel (RNN encoders / decoders + the ST Gumbel-Softmax estimator).  However, its multilingual version is interesting and the results are sufficiently convincing (e.g., comparison to Nakayama and Nishida, 17). The paper would more attractive for those interested in emergent communication than the NMT community, as the set-up (using pivoting through images) may be perceived somewhat exotic by the NMT community. Also, the model is not attention-based (unlike SoA in seq2seq / NMT), and it is not straightforward to incorporate attention (see R2 and author response).

+ an interesting framing of the weakly-supervised MT problem
+ well written
+ sufficiently convincing results
- the set-up and framework (e.g., non-attention based) is questionable from practical perspective
"
iclr_2018_rJvJXZb0W,"Though the approach is not terribly novel, it is quite effective (as confirmed on a wide range of evaluation tasks). The approach is simple and likely to be useful in applications. The paper is well written.

+ simple and efficient
+ high quality evaluation
+ strong results
- novelty is somewhat limited
"
iclr_2018_S1sqHMZCb,"An interesting application of graph neural networks to robotics. The body of a robot is represented as a graph, and the agent’s policy is defined using a graph neural network (GNNs/GCNs) over the graph structure.

The GNN-based policy network perform on par with best methods on traditional benchmarks, but shown to be very effective for transfer scenarios: changing robot size or disabling its components.  I believe that the reviewers' concern that the original experiments focused solely on centepedes and snakes were (at least partially) addressed in the author response: they showed that their GNN-based model outperforms MLPs on a dataset of 2D walkers.

Overall:
-- an interesting application
-- modeling robot morphology is an under-explored direction
-- the paper is  well written
-- experiments are sufficiently convincing (esp. after addressing the concerns re diversity and robustness).

"
iclr_2018_HkMvEOlAb,"The reviewers concerns regarding novelty and the experimental evaluation have been resolved accordingly and all recommend acceptance. I would recommend removing the term ""unsupervised"" in clustering, as it is redundant. Clustering is, by default, assumed to be unsupervised.

There is some interest in extending this to non-vision domains, however this is beyond the scope of the current work."
iclr_2018_HJIoJWZCZ,"The general consensus is that this method provides a practical and interesting approach to unsupervised domain adaptation. One reviewer had concerns with comparing to state of the art baselines, but those have been addressed in the revision.

There were also issues concerning correctness due to a typo. Based on the responses, and on the pseudocode, it seems like there wasn't an issue with the results, just in the way the entropy objective was reported.

You may want to consider reporting the example given by reviewer 2 as a negative example where you expect the method to fail. This will be helpful for researchers using and building on your paper."
iclr_2018_r1lUOzWCW,"This paper does an excellent job at helping to clarify the relationship between various, recently proposed GAN models. The empirical contribution is small, but the KID metric will hopefully be a useful one for researchers. It would be really useful to show that it maintains its advantage when the dimensionality of the images increases (e.g., on Imagenet 128x128)."
iclr_2018_Hk5elxbRW,"The submission proposes a loss surrogate for top-k classification, as in the official imagenet evaluation.  The approach is well motivated, and the paper is very well organized with thorough technical proofs in the appendix, and a well presented main text.  The main results are: 1) a theoretically motivated surrogate, 2) that gives up to a couple percent improvement over cross-entropy loss in the presence of label noise or smaller datasets.

It is a bit disappointing that performance is limited in the ideal case and that it does not more gracefully degrade to epsilon better than cross entropy loss.  Rather, it seems to give performance epsilon worse than cross-entropy loss in an ideal case with clean labels and lots of data.  Nevertheless, it is a step in the right direction for optimizing the error measure to be used during evaluation.  The reviewers uniformly recommended acceptance."
iclr_2018_B1Lc-Gb0Z,"The submission proposes optimization with hard-threshold activations.  This setting can lead to compressed networks, and is therefore an interesting setting if learning can be achieved feasibly.  This leads to a combinatorial optimization problem due to the non-differentiability of the non-linearity.  The submission proceeds to analyze the resulting problem and propose an algorithm for its optimization.

Results show slight improvement over a recent variant of straight-through estimation (Hinton 2012, Bengio et al. 2013), called saturated straight-through estimation (Hubara et al., 2016).  Although the improvements are somewhat modest, the submission is interesting for its framing of an important problem and improvement over a popular setting."
iclr_2018_H1WgVz-AZ,"The submission modifies the SPEN framework for structured prediction by adding an inference network in place of the usual combinatorial optimization based inference.  The resulting architecture has some similarity to a GAN, and significantly increases the speed of inference.

The submission provides links between two seemingly different frameworks: SPENs and GANs.  By replacing inference with a network output, the connection is made, but importantly, this massively speeds up inference and may mark an important step forward in structured prediction with deep learning.  "
iclr_2018_rypT3fb0b,"The paper proposes to regularize via a family of structured sparsity norms on the weights of a deep network.  A proximal algorithm is employed for optimization, and results are shown on synthetic data, MNIST, and CIFAR10.

Pros: the regularization scheme is reasonably general, the optimization is principled, the presentation is reasonable, and all three reviewers recommend acceptance.

Cons: the regularization is conceptually not terribly different from other kinds of regularization proposed in the literature.  The experiments are limited to quite simple data sets."
iclr_2018_S1XolQbRW,"The submission proposes a method for quantization.  The approach is reasonably straightforward, and is summarized in Algorithm 1.  It is the analysis which is more interesting, showing the relationship between quantization and adding Gaussian noise (Appendix B) - motivating quantization as regularization.

The submission has a reasonable mix of empirical and theoretical results, motivating a simple-to-implement algorithm.  All three reviewers recommended acceptance."
iclr_2018_HyH9lbZAW,"Thank you for submitting you paper to ICLR. The paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks. The framework extends Jonhson et al. (2016) and Khan & Lin (2017). The reviewers are all in agreement that the paper is suitable for publication. The paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity. The experiments are not the strongest suit of the paper and, although the revision has improved this aspect, I would encourage a more comprehensive evaluation of the proposed methods. Nevertheless, this is a strong paper."
iclr_2018_H1mCp-ZRZ,Thank you for submitting you paper to ICLR. The reviewers agree that the paper’s development of action-dependent baselines for reducing variance in policy gradient is a strong contribution and that the use of Stein's identity to provide a principled way to think about control variates is sensible. The revision clarified an number of the reviewers’ questions and the resulting paper is suitable for publication in ICLR.
iclr_2018_rkcQFMZRb,"Thank you for submitting you paper to ICLR. The reviewers and authors have engaged well and the revision has improved the paper. The reviewers are all in agreement that the paper substantially expands the prior work in this area,  e.g. by Balle et al. (2016, 2017), and is therefore suitable for publication. Although I understand that the authors have not optimised their compression method for runtime yet, a comment about this prospect in the main text would be a sensible addition."
iclr_2018_H1kG7GZAW,"Thank you for submitting you paper to ICLR. The reviewers are all in agreement that the paper is suitable for publication, each revising their score upwards in response to the revision that has made the paper stronger.

The authors may want to consider adding a discussion about whether the simple standard Gaussian prior, which is invariant under transformation by an orthogonal matrix, is a sensible one if the objective is to find disentangled representations. Alternatives, such as sparse priors, might be more sensible if a model-based solution to this problem is sought."
iclr_2018_rJNpifWAb,"Thank you for submitting you paper to ICLR. The idea is simple, but easy to implement and effective. The paper examines the performance fairly thoroughly across a number of different scenarios showing that the method consistently reduces variance. How this translates into final performance is complex of course, but faster convergence is demonstrated and the revised experiments in table 2 show that it can lead to improvements in accuracy.  "
iclr_2018_r1l4eQW0Z,"Thank you for submitting you paper to ICLR. This paper was enhanced noticeably in the rebuttal period and two of the reviewers improved their score as a result. There is a good range of experimental work on a number of different tasks. The addition of the comparison with Liu & Feng, 2016 to the appendix was sensible. Please make sure that the general conclusions drawn from this are explained in the main text and also the differences to Tran et al., 2017 (i.e. that the original model can also be implicit in this case)."
iclr_2018_Skdvd2xAZ,"This paper gives a scalable Laplace approximation which makes use of recently proposed Kronecker-factored approximations to the Gauss-Newton matrix. The approach seems sound and useful. While it is a rather natural extension of existing methods, it is well executed, and the ideas seem worth putting out there.
"
iclr_2018_B1IDRdeCW,"This paper analyzes mathematically why weights of trained networks can be replaced with ternary weights without much loss in accuracy. Understanding this is an important problem, as binary or ternary weights can be much more efficient on limited hardware, and we've seen much empirical success of binarization schemes. This paper shows that the continuous angles and dot products are well approximated in the discretized network. The paper concludes with an input rotation trick to fix discretization failures in the first layer.

Overall, the contribution seems substantial, and the reviewers haven't found any significant issues. One reviewer wasn't convinced of the problem's importance, but I disagree here. I think the paper will plausibly be helpful for guiding architectural and algorithmic decisions. I recommend acceptance.
"
iclr_2018_B1ae1lZRb,"Meta score: 7

The paper combined low precision computation with different approaches to teacher-student knowledge distillation.  The experimentation is good, with good experimental analysis.  Very clearly written.  The main contribution is in the different forms of teacher-student training combined with low precision.

Pros:
 - good practical contribution
 - good experiments
 - good analysis
 - well written
Cons:
 - limited originality"
iclr_2018_H1Dy---0Z,"meta score: 8

The paper present a distributed architecture using prioritized experience replay for deep reinforcement learning.  It is well-written and the experimentation is extremely strong.  The main issue is the originality - technically, it extends previous work in a limited way;  the main contribution is practical, and this is validated by the experiments.  The experimental support is such that the paper has meaningful conclusions and will surely be of interest to people working in the field.  Thus I would say it is comfortably over the acceptance threshold.

Pros:
 - good motivation and literature review
 - strong experimentation
 - well-written and clearly presented
 - details in the appendix are very helpful
Cons:
 - possibly limited originality in terms of modelling advances
"
iclr_2018_B1Gi6LeRZ,"meta score: 8

This is a good paper which augments the data by mixing sound classes, and then learns the  mixing ratio.  Experiments performed on a number of sound classification results

Pros
 - novel approach, clearly explained
 - very good set of experimentation with excellent results
 - good approach to mixing using perceptual criteria

Cons
 - discussion doesn't really generalise beyond sound recognition

"
iclr_2018_ryiAv2xAZ,"Meta score: 6

The paper approaches the problem of identifying out-of-distribution data by modifying the objective function to include a generative term.  Experiments on a number of image datasets.

Pros:
 - clearly expressed idea, well-supported by experimentation
 - good experimental results
 - well-written

Cons:
 - slightly limited novelty
 - could be improved by linking to work on semi-supervised learning approaches using GANs

The authors note that ICLR submission 267 (https://openreview.net/forum?id=H1VGkIxRZ) covers similar ground to theirs."
iclr_2018_SkFAWax0-,"Meta score: 7

This paper presents a novel architecture for neural network based TTS using a memory buffer architecture.  The authors have made good efforts to evaluate this system against other state-of-the-art neural TTS systems, although this is hampered by the need for re-implementation and the evident lack of optimal hyperparameters for e.g. tacotron.  TTS is hard to evaluate against existing approaches, since it requires subjective user evaluation.  But overall, despite its limtations, this is a good and interesting paper which I would like to see accepted

Pros:
 - novel architecture
 - good experimentation on multiple databases
 - good response to reviewer comments
 - good results

Cons:
 - some problems with the experimental comparison (baselines compared against)
 - writing could be clearer, and sometimes it feels like the authors are slightly overclaiming

I take  the point that this might be more suitable for a speech conference, but it seems to me that paper offers enough to the ICLR community for it to be worth accepting.

"
iclr_2018_rkr1UDeC-,"meta score: 7

The paper introduces an online distillation technique to parallelise large scale training.  Although the basic idea is not novel, the presented experimentation indicates that the authors' have made the technique work.  Thus this paper should be of interest to practitioners.

Pros:
 - clearly written, the approach is well-explained
 - good experimentation on large-scale common crawl data with 128-256 GPUs
 - strong experimental results

Cons:
 - the idea itself is not novel
 - the range of experimentation could be wider (e.g. different numbers of GPUs) but this is expensive!

Overall the novelty is in making this approach work well in practice, and demonstrating it experimentally."
iclr_2018_BJ0hF1Z0b,"This paper uses known methods for learning a differentially private models and applies it to the task of learning a language model, and find they are able to maintain accuracy results on large datasets. Reviewers found the method convincing and original saying it was ""interesting and very important to the machine learning ... community"", and that in terms of results it was a ""very strong empirical paper, with experiments comparable to industrial scale"". There were some complaints as to the clarity of the work, with requests for more clear explanations of the methods used.

"
iclr_2018_SJ-C6JbRW,"This paper provides a game-based interface to have Turkers compete to analyze data for a learning task over multiple rounds. Reviewers found the work interesting and clear written, saying ""the paper is easy to follow and the evaluation is meaningful."" They also note that there is clear empirical benefit ""the results seem to suggest that MTD provides an improvement over non-HITL methods."" They also like the task compared to synthetic grounding experiments. There was some concern about the methodology of the experiments but the authors provide reasonable explanations and clarification.

One final concern that I hope the readers take into account. While the reviewers were convinced by the work and did not require it, I feel like the work does not engage enough with the literature of crowd-sourcing in other disciplines. While there are likely some unique aspects to ML use of crowdsourcing, there are many papers about encouraging crowd-workers to produce more useful data. "
iclr_2018_Hyg0vbWC-,"This paper presents a new multi-document summarization task of trying to write a wikipedia article based on its sources. Reviewers found the paper and the task clear to understand and well-explained. The modeling aspects are clear as well, although lacking justification. Reviewers are split on the originality of the task, saying that it is certainly big, but wondering if that makes it difficult to compare with. The main split was the feeling that ""the paper presents strong quantitative results and qualitative examples. "" versus a frustration that the experimental results did not take into account extractive baselines or analysis. However the authors provide a significantly updated version of the work targeting many of these concerns, which does alleviate some of the main issues. For these reasons, despite one low review, my recommendation is that this work be accepted as a very interesting contribution.

"
iclr_2018_rkYTTf-AZ,"This work presents some of the first results on unsupervised neural machine translation. The group of reviewers is highly knowledgeable in machine translation, and they were generally very impressed by the results and the think it warrants a whole new area of research noting ""the fact that this is possible at all is remarkable."". There were some concerns with the clarity of the details presented and how it might be reproduced, but it seems like much of this was cleared up in the discussion. The reviewers generally praise the thoroughness of the method, the experimental clarity, and use of ablations. One reviewer was less impressed, and felt more comparison should be done."
iclr_2018_HkAClQgA-,"This work extends upon recent ideas to build a complete summarization system using clever attention, copying, and RL training. Reviewers like the work but have some criticisms. Particularly in terms of its originality and potential significance  noting ""It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us."". Still reviewers note the experimental results are of high quality performing excellent on several datasets and building ""a strong summarization model."" Furthermore the model is extensively tested including in ""human readability and relevance assessments "".  The work itself is well written and clear."
iclr_2018_BJRZzFlRb,"This paper proposes an offline neural method using concrete/gumbel for learning a sparse codebook for use in NLP tasks such as sentiment analysis and MT. The method outperforms other methods using pruning and other sparse coding methods, and also produces somewhat interpretable codes. Reviewers found the paper to be simple, clear, and effective. There was particular praise for the strength of the results and the practicality of application. There were some issues, such as only being applicable to input layers, and not being able to be applied end-to-end. The author also did a very admirable job of responding to questions about analysis with clear and comprehensive additional experiments. "
iclr_2018_SkhQHMW0W,"This work proposes a hybrid system for large-scale distributed and federated training of commonly used deep networks. This problem is of broad interest and these methods have the potential to be significantly impactful, as is attested by the active and interesting discussion on this work. At first there were questions about the originality of this study, but it seems that the authors have now added extra references and comparisons.

Reviewers were split about the clarity of the paper itself. One notes that ""on the whole clearly presented"", but another finds it too dense, disorganized and needing of more clear explanation. Reviewers were also concerned that methods were a bit heuristic and could benefit from more details. There were also many questions about these details in the discussion forum, these should make it into the next version.  The main stellar aspect of the work were the experimental results, and reviewers call them ""thorough"" and note they are convincing. "
iclr_2018_B14TlG-RW,"This work replaces the RNN layer of square with a self-attention and convolution, achieving a big speed up and performance gains, particularly with data augmentation. The work is mostly clear presented, one reviewer found it ""well-written"" although there was a complaint the work did not clear separate out the novel aspects. In terms of results the work is clearly of high quality, producing top numbers on the shared task. There were some initial complaints of only using the SQuAD dataset, but the authors have now included additional results that diversify the experiments. Perhaps the largest concern is novelty. The idea of non-RNN self-attention is now widely known, and there are several systems that are applying it. Reviewers felt that while this system does it well, it is maybe less novel or significant than other possible work. "
iclr_2018_Sy2ogebAW,"This work presents new results on unsupervised machine translation using a clever combination of techniques. In terms of originality, the reviewers find that the paper over-claims, and promises a breakthrough, which they do not feel is justified.
However there is ""more than enough new content"" and ""preliminary"" results on a new task. The experimental quality also has some issues, there is a lack of good qualitative analysis, and reviewers felt the claims about semi-supervised work had issues. Still the main number is a good start, and the authors are correct to note that there is another work with similarly promising results. Of the two works, the reviewers found the other more clearly written, and with better experimental analysis, noting that they both over claim in terms of novelty. The most promising aspect of the work, will likely be the significance of this task going forward, as there is now more interest in the use of multi-lingual embeddings and nmt as a benchmark task. "
iclr_2018_BkwHObbRZ,"I recommend acceptance based on the reviews. The paper makes novel contributions to learning one-hidden layer neural networks and designing new objective function with no bad local optima.

 There is one point that the paper is missing. It only mentions Janzamin et al in the passing. Janzamin et al propose using score function framework for designing alternative objective function. For the case of Gaussian input that this paper considers, the score function reduces to Hermite polynomials. Lack of discussion about this connection is weird. There should be proper acknowledgement of prior work. Also missing are some of the key papers on tensor decomposition and its analysis

I think there are enough contributions in the paper for acceptance irrespective of the above aspect.  "
iclr_2018_SysEexbRb,I recommend acceptance based on the positive reviews. The paper analyzes critical points for linear neural networks and shallow ReLU networks. Getting characterization of critical points for shallow ReLU networks is a great first step.
iclr_2018_rJm7VfZA-,"The paper considers Markov potential games (MPGs),  where the agents share some common resource. They consider MPGs with continuous state-action variables, coupled constraints and nonconvex rewards, which is novel. The reviews are all positive and point out the novel contributions in the paper"
iclr_2018_SyProzZAW,"All the reviewers are agree on the significance of the topic of understanding expressivity of deep networks. This paper makes good progress in analyzing the ability of deep networks to fit multivariate polynomials. They show exponential depth advantage for general sparse polynomials.

 I am very surprised that the paper misses the original contribution of Andrew Barron. He analyzes the size of the shallow neural networks needed to fit a wide class of functions including polynomials. The deep learning community likes to think that everything has been invented in the current decade.

@article{barron1994approximation,
  title={Approximation and estimation bounds for artificial neural networks},
  author={Barron, Andrew R},
  journal={Machine Learning},
  volume={14},
  number={1},
  pages={115--133},
  year={1994},
  publisher={Springer}
}"
iclr_2018_B1QgVti6Z,"Based on the positive reviews, I recommend acceptance. The paper analyzes when empirical risk is close to the population version, when empirical saddle points are close to the population version and empirical gradients are close to the population version."
iclr_2018_Hk9Xc_lR-,I recommend acceptance. The two positive reviews point out the theoretical contributions. The authors have responded extensively to the negative review and I see no serious flaw as claimed by the negative review.
iclr_2018_SyZI0GWCZ,"The reviewers all agree this is a well written and interesting paper describing a novel black box adversarial attack.   There were missing relevant references in the original submission, but these have been added.  I would suggest the authors follow the reviewer suggestions on claims of generality beyond CNN; although there may not be anything obvious stopping this method from working more generally, it hasn't been tested in this work.   Even if you keep the title you might be more careful to frame the body in the context of CNN's."
iclr_2018_rJQDjk-0b,"The reviewers agree that the proposed method is theoretically interesting, but disagree on whether it has been properly experimentally validated.   My view is that the the theoretical contribution is interesting enough to warrant inclusion in the conference, and so I will err on the side of accepting."
iclr_2018_ryup8-WCW,"The authors make an empirical study of the ""dimension"" of a neural net optimization problem, where the ""dimension"" is defined by the minimal random linear parameter subspace dimension where a (near) solution to the problem is likely to be found.   I agree with reviewers that in light of the authors' revisions, the results are interesting enough to be presented at the conference."
iclr_2018_rkO3uTkAZ,"
I am going to recommend acceptance of this paper despite being worried about the issues raised by reviewer 1.  In particular,

1:  the best possible inception score would be obtained by copying the training dataset
2:  the highest visual quality samples would be obtained by copying the training dataset
3:  perturbations (in the hidden space of a convnet) of training data might not be perturbations in l2, and so one might not find a close nearest neighbor with an l2 search
4:  it has been demonstrated in other works that perturbations of convnet features of training data (e.g. trained as auto-encoders) can make convincing ""new samples""; or more generally, paths between nearby samples in the hidden space of a convnet can be convincing new samples.

These together suggest the possibility that the method presented is not necessarily doing a great job as a generative model or as a density model (it may be, we just can't tell...), but it is doing a good job at hacking the metrics (inception score, visual quality).      This is not an issue with only this paper, and I do not want to punish the authors of this papers for the failings of the field; but this work, especially because of its explicit use of training examples in the memory,  nicely exposes the deficiencies in our community's methodology for evaluating GANs and other generative models.

"
iclr_2018_H1uR4GZRZ,"This is a borderline paper.  The reviewers are happy with the simplicity of the proposed method and the fact that it can be applied after training; but are concerned by the lack of theory explaining the results.  I will recommend accepting, but I would ask the authors add the additional experiments they have promised, and would also suggest experiments on imagenet."
iclr_2018_HkxF5RgC-,"The reviewers find the work interesting and well made, but are concerned that ICLR is not the right venue for the work.  I will recommend that the paper be accepted, but ask the authors to add the NMT results to the main paper (any other non-synthetic applications they could add would be helpful)."
iclr_2018_ByKWUeWA-,"The reviewers agree that the method is original and mostly well communicated, but have some doubts about the significance of the work.   "
iclr_2018_S18Su--CW,"This paper is borderline.  The reviewers agree that the method is novel and interesting, but have concerns about scalability and weakness to attacks with larger epsilon.  I will recommend accepting; but I think the paper would be well served by imagenet experiments, and hope the authors are able to include these for the final version"
iclr_2018_HyrCWeWCb,"This paper adapts (Nachum et al 2017) to continuous control via TRPO.   The work is incremental (not in the dirty sense of the word popular amongst researchers, but rather in the sense of ""building atop a closely related work""), nontrivial,  and shows empirical promise.    The reviewers would like more exploration of the sensitivity of the hyper-parameters."
iclr_2018_rk49Mg-CW,Not quite enough for an oral but a very solid poster.
iclr_2018_HkXWCMbRW,"Some reviewers seem to assign novelty to the compression and classification formulation; however, semi-supervised autoencoders have been used for a long time. Taking the compression task more seriously as is done in this paper is less explored.

The paper provides some extensive experimental evaluation and was edited to make the paper more concise at the request of reviewers. One reviewer had a particularly strong positive rating, due to the quality of the presentation, experiments and discussion. I think the community would like this work and it should be accepted.
"
iclr_2018_ByJIWUnpW,"With an 8-6-6 rating all reviewers agreed that this paper is past the threshold for acceptance.

The  quality of the paper appears to have increased during the review cycle due to interactions with the reviewers. The paper addresses issues related to the quality of heterogeneous data sources. The paper does this through the framework of graph convolutional networks (GCNs). The work proposes a data quality level concept defined at each vertex in a graph based on a local variation of the vertex. The quality level is used as a regularizer constant in the objective function. Experimental work shows that this formulation is important in the context of time-series prediction.

Experiments are performed on a dataset that is less prominent in the ML and ICLR community, from two commercial weather services Weather Underground and WeatherBug; however, experiments with reasonable baseline models using a ""Forecasting mean absolute error (MAE)"" metric seem to be well done.

The biggest weakness of this work was a lack of comparison with some more traditional time-series modelling approaches. However, the authors added an auto-regressive model into the baselines used for comparison. Some more details on this model would help.

I tend to agree with the author's assertion that: ""there is limited work in ICLR  on data quality, but it is definitely one essential hurdle for any representation learning model to work in practice. "".

For these reasons I recommend a poster.

"
iclr_2018_Sy21R9JAW,"With scores of 7-7-6  and the justification below the AC recommends acceptance.

One of the reviewers summarizes why this is a good paper as follows:

""This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances:
- This gives a more unified way of understanding, and implementing the methods.
- The paper points out situations when the methods are equivalent
- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity
- The paper proposes a new objective function to measure joint sensitivity""
"
iclr_2018_SyJ7ClWCb,"A well written paper proposing some reasonable approaches to counter adversarial images. Proposed approaches include non-differentiable and randomized methods. Anonymous commentators pushed upon and cleared up some important issues regarding white, black and gray ""box"" settings. The approach appears to be a plausible defence strategy. One reviewers is a hold out on acceptance, but is open to the idea. The authors responded to the points of this reviewer sufficiently. The AC recommends accept."
iclr_2018_HkwVAXyCW,"This paper explores what might be characterized as an adaptive form of ZoneOut.
With the improvements and clarifications added to the paper during the rebuttal the paper could be accepted.
"
iclr_2018_rkPLzgZAZ,"Important problem (modular continual RL) and novel contributions. The initial submission was judged to be a little dense and hard to read, but the authors have been responsive in responding and updating the paper. I support accepting this paper. "
iclr_2018_BydLzGb0Z,"Simple idea (which is a positive) to regularize RNNs, broad applicability, well-written paper. Initially, there were concerns about  comparisons, but he authors have provided additional experiments that have made the paper stronger. "
iclr_2018_S1J2ZyZ0Z,Important problem and all reviewers recommend acceptance. I agree.
iclr_2018_H1UOm4gA-,"This manuscript was reviewed by 3 expert reviewers and their evaluation is generally positive. The authors have responded to the questions asked and the reviewers are satisfied with the responses. Although the 2D environments are underwhelming (compared to 3D environments such as SUNCG, Doom, Thor, etc), one thing that distinguishes this paper from other concurrent submissions on the similar topics is the demonstration that ""words learned only from a VQA-style supervision condition can be successfully interpreted in an instruction-following setting."" "
iclr_2018_Sy0GnUxCb,"This paper received divergent reviews (7, 3, 9). The main contributions of the paper -- that multi-agent competition serves as a natural curriculum, opponent sampling strategies, and the characterization of emergent complex strategies -- are certainly of broad interest (although the first is essentially the same observation as AlphaZero, but the different environment makes this of broader interest).

In the discussion between R2 and the authors, I am sympathetic to (a subset of) both viewpoints.

To be fair to the authors, discovery (in this case, characterization of emergent behavior) can be often difficult to quantify. R2's initial review was unnecessary harsh and combative. The points presented by R2 as evidence of poor evaluation have clear answers by the authors. It would have been better to provide suggestions for what the authors could try, rather than raise philosophical objections that the authors cannot experimentally rebut.

On the other hand, I am disappointed that the authors were asked a reasonable, specific, quantifiable request by R2 --
""By the end of Section 5.2, you allude to transfer learning phenomena. It would be nice to study these transfer effects in your results with a quantitative methodology.”
-- and they chose to respond with informal and qualitative assessments. It doesn't matter if the results are obvious visually, why not provide quantitative evaluation when it is specifically asked?

Overall, we recommend this paper for acceptance, and ask the authors to incorporate feedback from R2. "
iclr_2018_B1mvVm-C-,"All reviewers recommend accepting this paper, and this AC agrees. "
iclr_2018_SJa9iHgAZ,"The paper presents an interesting view of ResNets and the findings should be of broad interest. R1 did not update their score/review, but I am satisfied with the author response, and recommend this paper for acceptance. "
iclr_2018_Hk6WhagRW,"All reviewers agree the paper proposes an interesting setup and the main finding that ""prosocial agents are able to learn to ground symbols using RL, but self-interested agents are not"" progresses work in this area. R3 asked a number of detail-oriented questions and while they did not update their review based on the author response, I am satisfied by the answers. "
iclr_2018_SygwwGbRW,"Important problem (navigation in unseen 3D environments, Doom in this case), interesting hybrid approach (mixing neural networks and path-planning). Initially, there were concerns about evaluation (proper baselines, ambiguous environments, etc). The authors have responded with updated experiments that are convincing to the reviewers. R1 did not participate in the discussion and their review has been ignored. I am supportive of this paper. "
iclr_2018_B12Js_yRb,"Initially this paper received mixed reviews. After reading the author response, R1 and and R3 recommend acceptance.

R2, who recommended rejecting the paper, did not participate in discussions, did not respond to author explanations, did not respond to AC emails, and did not submit a final recommendation. This AC does not agree with the concerns raised by R2 (e.g. I don't find this model to be unprincipled).

The concerns raised by R1 and R3 were important (especially e.g. comparisons to NMS) and the authors have done a good job adding the required experiments and providing explanations.

Please update the manuscript incorporating all feedback received here, including comparisons reported to the concurrent ICLR submission on counting. "
iclr_2018_HJsjkMb0Z,"This paper constructs a variant of deep CNNs which is provably invertible, by replacing spatial pooling with multiple shifted spatial downsampling, and capitalizing on residual layers to define a simple, invertible representation. The authors show that the resulting representation is equally effective at large-scale object classification, opening up a number of interesting questions.

Reviewers agreed this is an strong contribution, despite some comments about the significance of the result; ie, why is invertibility a ""surprising"" property for learnability, in the sense that F(x) = {x,  phi(x)}, where phi is a standard CNN satisfies both properties: invertible and linear measurements of F producing good classification. All in all, this will be a great contribution to the conference. "
iclr_2018_BkUHlMZ0b,"This paper proposes a new metric to evaluate the robustness of neural networks to adversarial attacks. This metric comes with theoretical guarantees and can be efficiently computed on large-scale neural networks.

Reviewers were generally positive about the strengths of the paper, especially after major revisions during the rebuttal process. The AC believes this paper will contribute to the growing body of literature in robust training of neural networks.  "
iclr_2018_r1vuQG-CW,"This paper implements Group convolutions on inputs defined over hexagonal lattices instead of square lattices, using the roto-translation group. The internal symmetries of the hexagonal grid allow for a larger discrete rotation group than when using square pixels, leading to improved performance on CIFAR and aerial datasets.

The paper is well-written and the reviewers were positive about its results. That said, the AC wonders what is the main contribution of this work relative to existing related works (such as Group Equivarant CNNS, Cohen & Welling'16, or steerable CNNs, Cohen & Welling'17). While it is true that extending GCNNs to hexagonal lattices is a non-trivial implementation task, the contribution lacks significance in the mathematical/learning fronts, which are perhaps the ones ICLR audience will care more about. Besides, the numerical results, while improved versus their square lattice counterparts, are not a major improvement over the state-of-the-art.

In summary, the AC believes this is a borderline paper. The unanimous favorable reviews tilt the decision towards acceptance. "
iclr_2018_rJzIBfZAb,"This paper presents new results on adversarial training, using the framework of robust optimization. Its minimax nature allows for principled methods of both training and attacking neural networks.

The reviewers were generally positive about its contributions, despite some concerns about 'overclaiming'. The AC recommends acceptance, and encourages the authors to also relate this work with the concurrent ICLR submission (https://openreview.net/forum?id=Hk6kPgZA-) which addresses the problem using a similar approach. "
iclr_2018_By4HsfWAZ,"This paper proposes to use data-driven deep convolutional architectures for modeling advection diffusion. It is well motivated and comes with convincing numerical experiments.
Reviewers agreed that this is a worthy contribution to ICLR with the potential to trigger further research in the interplay between deep learning and physics. "
iclr_2018_ryazCMbR-,"This paper studies trainable deep encoders/decoders in the context of coding theory, based on recurrent neural networks. It presents highly promising results showing that one may be able to use learnt encoders and decoders on channels where no predefined codes are known.

Besides these encouraging aspects, there are important concerns that the authors are encouraged to address; in particular, reviewers noted that the main contribution of this paper is mostly on the learnt encoding/decoding scheme rather than in the replacement of Viterbi/BCJR. Also, complexity should be taken into account when comparing different decoding schemes.

Overall, the AC leans towards acceptance, since this paper may trigger further research in this direction. "
iclr_2018_rJYFzMZC-,"this submission proposes a novel extension of existing recurrent networks that focus on capturing long-term dependencies via tracking entities/their statesand tested it on a new task. there's a concern that the proposed approach is heavily engineered toward the proposed task and may not be applicable to other tasks, which i fully agree with. i however find the proposed approach and the authors' justification to be thorough enough, and for now, recommend it to be accepted."
iclr_2018_BkeqO7x0-,"this work adapts cycle GAN to the problem of decipherment with some success. it's still an early result, but all the reviewers have found it to be interesting and worthwhile for publication."
iclr_2018_Sy-dQG-Rb,"this submission proposes an efficient parametrization of a recurrent neural net by using two transition functions (one large and one small) to reduce the amount of computation (though, without actual improvement on GPU.) the reviewers found the submission very positive.

please, do not forget to include all the result and discussion on the proposed approach's relationship to VCRNN which was presented at the same conference just a year ago."
iclr_2018_SyJS-OgR-,"this submission proposes a learning algorithm for resnets based on their interpreration of them as a discrete approximation to a continuous-time dynamical system.  all the reviewers have found the submission to be clearly written, well motivated and have proposed an interesting and effective learning algorithm for resnets."
iclr_2018_HktJec1RZ,"this submission introduces soft local reordering to the recently proposed SWAN layer [Wang et al., 2017] to make it suitable for machine translation. although only in small-scale experiments, the results are convincing."
iclr_2018_ByJHuTgA-,"this submission demonstrates an existing loop-hole (?) in rushing out new neural language models by carefully (and expensively) running hyperparameter tuning of baseline approaches. i feel this is an important contribution, but as pointed out by some reviewers, i would have liked to see whether the conclusion stands even with a more realistic data (as pointed out by some in the field quite harshly, perplexity on PTB should not be considered seriously, and i believe the same for the other two corpora used in this submission.) that said, it's an important paper in general which will work as an alarm to the current practice in the field, and i recommend it to be accepted."
iclr_2018_rkfOvGbCW,"the proposed approach nicely incorporates various ideas from recent work into a single meta-learning (or domain adaptation or incremental learning or ...) framework. although better empirical comparison to existing (however recent they are) approaches would have made it stronger, the reviewers all found this submission to be worth publication, with which i agree."
iclr_2018_HJJ23bW0b,"this submission presents the positive impact of using orthogonal random features instead of unstructured random features for predictive state recurrent neural nets. there's been some sentiment by the reviewers that the contribution is rather limited, but after further discussion with another AC and PC's, we have concluded that it may be limited but a solid follow-up on the previous work on predictive state RNN. "
iclr_2018_rJUYGxbCW,"The paper studies the use of PixelCNN density models for the detection of adversarial images, which tend to lie in low-probability parts of image space. The work is novel, relevant to the ICLR community, and appears to be technically sound.

A downside of the paper is its limited empirical evaluation: there evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defense on a dataset like ImageNet."
iclr_2018_Bys4ob-Rb,"The paper presents a differentiable upper bound on the performance of classifier on an adversarially perturbed example (with small perturbation in the L-infinity sense). The paper presents novel ideas, is well-written, and appears technically sound. It will likely be of interest to the ICLR community.

The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defenses on a dataset like ImageNet."
iclr_2018_BkJ3ibb0-,"The paper studied defenses against adversarial examples by training a GAN and, at inference time, finding the GAN-generated sample that is nearest to the (adversarial) input example. Next, it classifies the generated example rather than the input example. This defense is interesting and novel. The CelebA experiments the authors added in their revision suggest that the defense can be effective on high-resolution RGB images."
iclr_2018_rkZvSe-RZ,"The paper studies a defense against adversarial examples that re-trains convolutional networks on adversarial examples constructed to attack pre-trained networks. Whilst the proposed approach is not very original, the paper does present a solid empirical baseline for these kinds of defenses. In particular, it goes beyond the ""toy"" experiments that most other studies in this space perform by experimenting on ImageNet. This is important as there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to ImageNet. The importance of the baseline method studied in this paper is underlined by its frequent application in the recent NIPS competition on adversarial examples."
iclr_2018_SJyVzQ-C-,"The paper studies a dropout variant, called fraternal dropout. The paper is somewhat incremental in that the proposed approach is closely related to expectation linear dropout. Having said that, fraternal dropout does improve a state-of-the-art language model on PTB and WikiText2 by ~0.5-1.7 perplexity points. The paper is well-written and appears technically sound.

Some reviewers complain that the authors could have performed a more careful hyperparameter search on the fraternal dropout model. The authors appear to have partly addressed those concerns, which frankly, I don't really agree with either. By doing only a limited hyperparameter optimization, the authors are putting their ""own"" method at a disadvantage. If anything, the fact that their method gets strong performance despite this disadvantage (compared to very strong baseline models) is an argument in favor of fraternal dropout."
iclr_2018_SJcKhk-Ab,"All the reviews like the theoretical result presented in the paper which relates the gating mechanism of LSTMS (and GRUs) to time invariance / warping. The theoretical result is great and is used to propose a heuristic for setting biases when time invariance scales are known. The experiments are not mind-boggling, but none of the reviewers seem to think that's a show stopper. "
iclr_2018_HyUNwulC-,"Paper presents a way in which linear RNNs can be computed (fprop, bprop) using parallel scan. They show big improvements in speedups and show application on really long sequences. Reviews were generally favorable."
iclr_2018_HkTEFfZRb,"Paper was well written and rebuttal was well thought out and convincing.

The reviewers agree that the paper showed BNNs were good (relatively speaking) at resisting adversarial examples. Some question was raised about whether the methods would work on larger datasets and models. The authors offered some experiments in this regard in the rebuttal to this end. Also, a public comment appeared to follow up on CIFAR and report correlated results. "
iclr_2018_S1jBcueAb,"Paper explore depth-wise separable convolutions for sequence to sequence models with convolutions encoders.
R1 and R3 liked the paper and the results. R3 thought the presentation of the convolutional space was nice, but the experiments were hurried. Other reviewers thought the paper as a whole had dense parts and need cleaning up, but the authors seem to have only done this partially.
From the reviewers comments, I'm giving this a borderline accept. I would have been feeling much more comfortable with the decision if the authors had incorporated the reviewers' suggestions more thoroughly.."
iclr_2018_rywHCPkAW,"The paper proposes to add noise to the weights of a policy network during learning in Deep-RL settings and finds that this results in better performance on DQN, A3C and other algorithms that use other exploration strategies. Unfortunately, the paper does not do a thorough job of exploring the reasons and doesn't offer a comparison to other methods that have been out on arxiv for several months before the submission, in spite of reviewers and anonymous requests. Otherwise I might have supported recommending the paper for a talk. "
iclr_2018_Hkc-TeZ0W,"The authors provide an alternative method to [1] for placement of ops in blocks. The results are shown to be an improvement over prior RL based placement in [1] and superior to *some* (maybe not the best) earlier methods for operations placements. The paper seems to have benefited strongly from reviewer feedback and seems like a reasonable contribution. We hope that the implementation may be made available to the community.

[1] Mirhoseini A, Pham H, Le Q V, et al. Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. "
iclr_2018_BJJLHbb0-," + Empirically convincing and clearly explained application: a novel deep learning architecture and approach is shown to significantly outperform state-of-the-art in unsupervised anomaly detection.
 - No clear theoretical foundation and justification is provided for the approach
 - Connexion and differentiation from prior work on simulataneous learning representation and fitting a Gaussian mixture to it would deserve a much more thorough discussion / treatment.
"
iclr_2018_BySRH6CpW,"Well written paper on a novel application of the local reprarametrisation trick to learn networks with discrete weights. The approach achieves state-of-the-art results.

Note: I apreciate that the authors added a comparison to the Gumbel-softmax continuous relaxation approach during the review period, following the suggestion of a reviewer. This additional comparison strengthens the paper."
iclr_2018_BJ_wN01C-,"Clearly explained, well motivated and empirically supported algorithm for training deep networks while simultaneously learning their sparse connectivity.
The approach is similar to previous work (in particular Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011) but is novel in that it satisfies a hard constraint on the network sparsity, which could be an advantage to match neuromorphic hardware limitations."
iclr_2018_SJQHjzZ0-,"+ clearly written and thorough empirical comparison of several metrics/divergences for evaluating GANs, prominently parametric-critic based divergences.
 - little technical novelty with respect to prior work. As noted by reviewers and an anonymous commentator:  using an Independent critic for evaluation has been proposed and used in practice before.
 +  the contribution of the work thus lies primarily in its well-done and extensive empirical comparisons of multiple metrics and models    "
iclr_2018_BkLhaGZRW,"  + Original regularizer that encourages discriminator representation entropy is shown to improve GAN training.
  + good supporting empirical validation
  - While intuitively reasonable, no compelling theory is given to justify the approach
  - The regularizer used in practice is a heap of heuristic approximations (continuous relaxation of a rough approximate measure of the joint entropy of a binarized activation vector)
  - The writing and the mathematical exposition could be clearer and more precise"
iclr_2018_r1NYjfbR-,"The paper got mixed scores of 4 (R1), 6 (R3), 8 (R2). R1 initially gave up after a few pages of reading, due to clarity problems. But looking over the revised version was much happier, so raised their score to 7. R2, who is knowledge about the area, was very positive about the paper, feeling it is a very interesting idea. R3 was also cautiously positive. The authors have absorbed the comments by the reviewers to make significant changes to the paper. The AC feels the idea is interesting, even if the experimental results aren't that compelling, so feels the paper can be accepted.
"
iclr_2018_BJGWO9k0Z,"The paper got generally positive scores of 6,7,7. The reviewers found the paper to be novel but hard to understand. The AC feels the paper should be accepted but the authors should revise their paper to take into account the comments from the reviewers to improve clarity."
iclr_2018_HkNGsseC-,"The paper received scores of 8 (R1), 6 (R2), 6 (R3). R1's review is brief, and also is optimistic that these results demonstrated on ConvACs generalize to real convnets.  R2 and R3 feel this might be a potential problem. R2 advocates weak accept and given that R1 is keen on the paper, the AC feels it can be accepted.

"
iclr_2018_HJ94fqApW,"The paper received scores either side of the borderline: 6 (R1), 5 (R2), 7 (R3). R1 and R3 felt the idea to be interesting, simple and effective. R2 raised a number of concerns which the rebuttal addressed satisfactorily. Therefore the AC feels the paper can be accepted."
iclr_2018_SJiHXGWAZ,"The paper received highly diverging scores: 5 (R1) ,9 (R2), 4(R3). Both R1 and R3 complained about the comparisons to related methods. R3 suggested some kNN and GP baselines, while R1 mentioned concurrent work using deepnets for trafffic prediction.

R3 is real expert on field. R2 and R1, not so.
R2 review very positive, but vacuous.
Rebuttal seems to counter R1 and R3 well.

It's a close all but the AC is inclined to accept since it's an interesting application of (graph-based) deepnets."
iclr_2018_SkHDoG-Cb,"Split opinions on paper: 6 (R1), 3 (R2), 6 (R3). Much of the debate centered on the novelty of the algorithm. R2 felt that the paper was a straight-forward combination of CycleGAN with S+U, while R3 felt it made a significant contribution. The AC has looked at the paper and the reviews and discussion. The topic is very interesting and topical. The experiments are ok, but would be helped a lot by including the real/synth car data currently in appendix B: seeing the method work on natural images is much more compelling. The approach still seems a bit incremental: yes, it's not a straight combination but the extra stuff isn't so profound. The AC is inclined to accept, just because this is an interesting problem."
iclr_2018_ryH20GbRW,All three reviewers recommend acceptance. The authors did a good job at the rebuttal which swayed the first reviewer to increase the final rating. This is a clear accept.
iclr_2018_HkCsm6lRb,"All three reviewers recommend acceptance. Good work, accept"
iclr_2018_r1wEFyWCW,"This paper incorporates attention in the PixelCNN model and shows how to use MAML to enable few-shot density estimation. The paper received mixed reviews (7,6,4). After rebuttal the first reviewer updated the score to accept. The AC shares the concern of novelty with the first reviewer. However, it is also not trivial to incorporate attention and MAML in PixelCNN, thus the AC decided to accept the paper. "
iclr_2018_rknt2Be0-,"This paper investigates emergence of language from raw pixels in a two-agent setting. The paper received divergent reviews, 3,6,9. Two ACs discussed this paper, due to a strong opinion from both positive and negative reviewers. The ACs agree that the score ""9"" is too high: the notion of compositionality is used in many places in the paper (and even in the title), but never explicitly defined. Furthermore, the zero-shot evaluation is somewhat disappointing. If the grammar extracted by the authors in sec. 3.2 did indeed indicate the compositional nature of the emergent communication, the authors should have shown that they could in fact build a message themselves, give it to the listener with an image and ask it to answer. On the other hand, ""3"" is also too low of a score. In this renaissance of emergent communication protocol with multi-agent deep learning systems, one missing piece has been an effort toward seriously analyzing the actual properties of the emergent communication protocol.  This is one of the few papers that have tackled this aspect more carefully. The ACs decided to accept the paper. However, the authors should take the reviews and comments seriously when revising the paper for the camera ready."
iclr_2018_rkN2Il-RZ,"This paper initially received borderline reviews. The main concern raised by all reviewers was a limited experimental evaluation (synthetic only). In rebuttal, the authors provided new results on the CelebA dataset, which turned the first reviewer positive. The AC agrees there is merit to this approach, and generally appreciates the idea of compositional concept learning."
iclr_2018_HJCXZQbAZ,"This paper marries the idea of Gaussian word embeddings and order embeddings, by imposing order among probabilistic word embeddings. Two reviewers vote for acceptance, and one finds the novelty of the paper incremental. The reviewer stuck to this view even after rebuttal, however, acknowledges the improvement in results. The AC read the paper, and agrees that the novelty is somewhat limited, however, the idea is still quite interesting, and the results are promising. The AC was missing more experiments on other tasks originally presented by Vendrov et al. Overall, this paper is slightly over the bar."
iclr_2018_BkN_r2lR-,"This paper builds on top of Cycle GAN ideas where the main idea is to jointly optimize the domain-level translation function with an instance-level matching objective. Initially the paper received two negative reviews (4,5) and a positive (7). After the rebuttal and several back and forth between the first reviewer and the authors, the reviewer was finally swayed by the new experiments. While not officially changing the score, the reviewer recommended acceptance. The AC agrees that the paper is interesting and of value to the ICLR audience."
iclr_2018_B17JTOe0-,"This work shows how activation patterns of units reminiscent of grid and border cells emerge in RNNs trained on navigation tasks. While the ICLR audience is not mainly focused on neuroscience, the findings of the paper are quite intriguing, and grid cells are sufficiently well-known and ""mainstream"" that this may interest many people."
iclr_2018_HJhIM0xAW,"This work shows interesting potential applications of known machine learning techniques to the practical problem of how to devise a retina prosthesis that is the most perceptually useful. The paper suffers from a few methodological problems pointed out by the reviewers (e.g., not using the more powerful neural network encoding in the subsequent experiments of the paper), but is still interesting and inspiring in its current state."
iclr_2018_BJj6qGbRW,All reviewers agree that the proposed method is novel and experiments do a good job in establishing its value for few-shot learning. Most the concerns raised by the reviewers on experimental protocols have been addressed in the author response and revised version.
iclr_2018_S1nQvfgA-,"The paper proposes a GAN based approach for disentangling identity (or class information) from style. The supervision needed is the identity label for each image. Overall, the reviewers agree that the paper makes a novel contribution along the line of work on disentangling 'style' from 'content'. "
iclr_2018_By-7dz-AZ,The paper proposes evaluation metrics for quantifying the quality of disentangled representations. There is consensus among reviewers that the paper makes a useful contribution towards this end. Authors have addressed most of reviewers' concerns in their response.
iclr_2018_HJcSzz-CZ,"The paper extends the earlier work on Prototypical networks to semi-supervised setting. Reviewers largely agree that the paper is well-written. There are some concerns on the incremental nature of the paper wrt to the novelty aspect but in the light of reported empirical results which show clear improvement over earlier work and given the importance of the topic, I recommend acceptance. "
iclr_2018_H1q-TM-AW,"Well motivated and well written, with extensive results. The paper also received positive comments from all reviewers. The AC recommends that the paper be accepted."
iclr_2018_r1Dx7fbCW,Well motivated and well received by all of the expert reviewers. The AC recommends that the paper be accepted.
iclr_2018_ByRWCqvT-,"Pros
-- A novel formulation for cross-task and cross-domain transfer learning.
-- Extensive evaluations.

Cons
-- Presentation a bit confusing, please improve.

The paper received positive reviews from reviewers. But the reviewers pointed out some issues with presentation and flow of the paper. Even though the revised version has improved, the AC feels that it can be improved further. For example, as pointed out by reviewers, different parts of the model are trained using different losses and / or are pre-trained. It would be worth clarifying that. It might help if the authors include a pseudocode / algorithm block to the final version of the paper."
iclr_2018_H1T2hmZAb,"The paper received mostly positive comments from experts. To summarize:

Pros:
-- The paper provides complex counterparts for typical architectures / optimization strategies used by real valued networks.
Cons:
-- Although the authors include plots explaining how nonlinearities transform phase, intuition about how phase gets processed can be improved.
-- Improving evaluations: Wisdom et al. computes log magnitude; real valued networks may not be suited for computing real / complex numbers which have a large dynamic range, like the complex spectra. So please compare performance by estimating magnitude as in Wisdom et al.
-- Please add computational cost, in terms of the number of multiplies and adds, to the final version of the paper.

I am recommending that the paper be accepted based on these reviews.
"
iclr_2018_HkwBEMWCZ,"pros:
* novel explanation: skip connections <--> singualrities
* thorough analysis
* significant topic in understanding deep nets

cons:
* more rigorous theoretical analysis would be better

overall, the committee feels this paper would be interesting to have at ICLR.
"
iclr_2018_H1cWzoxA-,"The proposed Bi-BloSAN is a two-levels' block SAN, which has both parallelization efficiency and memory efficiency. The study is thoroughly conducted and well presented.  "
iclr_2018_ry8dvM-R-,The proposed routing networks using RL to automatically learn the optimal network architecture is very interesting. Solid experimental justification and comparisons. The authors also addressed reviewers' concerns on presentation clarity in revisions.
iclr_2018_rkhlb8lCZ,"The idea of using wavelet pooling is novel and will bring many interesting research work in this direction. But more thorough experimental justification such as those recommended by the reviewers would make the paper better. Overall, the committee feels this paper will bring value to the conference."
iclr_2018_SJ1Xmf-Rb,A novel dual memory system inspired by brain for the important incremental learning and very good results.
iclr_2018_BJehNfW0-,"* presents a novel way analyzing GANs using the birthday paradox and provides a theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse
* significant contribution to the discussion of whether GANs learn the target disctibution
* thorough justifications"
iclr_2018_BydjJte0-,"Novel way of analyzing neural networks to predict NN attributes such as architecture, training method, batch size etc. And the method works surprisingly good on the MNIST and ImageNet."
iclr_2018_B1J_rgWRW,Theoretical analysis and understanding of DNNs is a crucial area for ML community. This paper studies characteristics of the relu DNNs and makes several important contributions.
iclr_2018_rytNfI1AZ,"The paper presents a way of training 1bit wide resnet to reduce the model footprint while maintaining good performance. The revisions added more comparisons and discussions, which make it much better. Overall, the committee feels this work will bring value to the conference."
iclr_2018_HyzbhfWRW,"quality: interesting idea to train an end-to-end attention together with CNNs and solid experiments to justify the benefits of using such attentions.
clarity: the presentation has been updated according to review comments and improved a lot
significance: highly relevant topic, good improvements over other methods"
iclr_2018_Hko85plCW,"This clearly written paper describes a simple extension to hard monotonic attention -- the addition of a soft attention mechanism that operates over a fixed length window of inputs that ends at the point selected by the hard attention mechanism.  Experiments on speech recognition (WSJ) and on a document summarization task demonstrate that the new attention mechanism improves significantly over the hard monotonic mechanism.  About the only ""con"" the reviewers noted is that the paper is a minor extension over Raffel et al., 2017, but the authors successfully argue that the strong empirical results render this simplicity a ""pro.""
"
iclr_2018_BJ_UL-k0b,"Pros:
+ The paper introduces a non-trivial interpretation of MAML as hierarchical Bayesian learning and uses this perspective to develop a new variation of MAML that accounts for curvature information.

Cons:
- Relatively small gains over MAML on mini-Imagenet.
- No direct comparison against the state-of-the-art on mini-Imagenet.

The reviewers agree that the interpretation of MAML as a form of hierarchical Bayesian learning is novel, non-trivial, and opens up an interesting direction for future research.  The only concerns are that the empirical results on mini-Imagenet do not show a particularly large improvement over MAML, and there is no direct comparison to the state-of-the-art results on the task.  However, the value of the new perspective on meta-learning outweighs these concerns.
"
iclr_2018_B1Yy1BxCZ,"Pros:
+ Nice demonstration of the equivalence between scaling the learning rate and increasing the batch size in SGD optimization.

Cons:
- While reporting convergence as a function of number of parameter updates is consistent, the paper would be more compelling if wall-clock times were given in some cases, as that will help to illustrate the utility of the approach.
- The paper would be stronger if additional experimental results, which the authors appear to have at hand (based on their comments in the discussion) were included as supplemental material.
- The results are not all that surprising in light of other recent papers on the subject.
"
iclr_2018_HyMTkQZAb,"This clearly written paper extends the Kronecker-factored approximate curvature optimizer to recurrent networks.  Experiments on Penn Treebank language modeling and training of differentiable neural computers on a repeated copy task show that the proposed K-FAC optimizers are stronger than SGD, Adam, and Adam with layer normalization. The most negative reviewer objected to a lack of theoretical error bounds on the approximations made, but the authors successfully argue that obtaining such bounds would require making assumptions that are likely to be violated in practice, and that strong empirical performance on real tasks is sufficient justification for the approximations.

Pros:
+ ""Completes"" K-FAC training by extending it to recurrent models.
+ Experiments show effects of different K-FAC approximations.

Cons:
- The algorithm is rather complex to implement.
"
iclr_2018_ByeqORgAW,"Pros:
+ Clear, well-written paper that tackles an interesting problem.
+ Interesting potential connections to other approaches in the literature such as Carreira-Perpiñán and Wang, 2014 and Taylor et al., 2016.
+ Paper shows good understanding of the literature, has serious experiments, and does not overstate the results.

Cons:
- Theory only addresses gradient descent, not stochastic gradient descent.
- Because the optimization process is similar to BFGS, it would make sense to have an empirical comparison against some second-order method, even though the proposed algorithm is more like standard backpropagation.

This paper is a nice first step in an interesting direction, and belongs in ICLR if there is sufficient space.
"
iclr_2018_rkLyJl-0-,"Pros:
+ Clearly written paper.
+ Easily implemented algorithm that appears to have excellent scaling properties and can even improve on validation error in some cases.
+ Thorough evaluation against the state of the art.

Cons:
- No theoretical guarantees for the algorithm.

This paper belongs in ICLR if there is enough space.
"
iclr_2018_rJ33wwxRb,"This is a high quality paper, clearly written, highly original, and clearly significant. The paper gives a complete analysis of SGD in a two layer network where the second layer does not undergo training and the data are linearly separable.  Experimental results confirm the theoretical suggestion that the second layer can be trained provided the weights don't change sign and remain bounded. The authors address the major concerns of the reviewers (namely, whether these results are indicative given the assumptions). This line of work seems very promising."
iclr_2018_Skz_WfbCZ,"This is a strong paper presenting a very clean proof of a result that is similar, though now incomparable to one due to Bartlett et al. These bounds (and Bartlett's) are among the most promising norm-based bounds for NNs.

I would simply add that the citation of Dziugaite and Roy (2017) could be improved. There work also connects sharpness (or flatness) with generalization via the PAC-Bayes framework, and moreover, there bounds are nonvacuous.  Are the bounds in this paper nonvacuous, say, on MNIST for 60,000 training data, for the network learned by SGD?  If not, how close do they get to 1.0?"
iclr_2018_r1iuQjxCZ,"The paper contributes to a body of empirical work towards understanding generalization in deep learning. They do this  through a battery of experiments studying ""single directions"" or selectivity of small groups of neurons. The reviewers that have actively participated agree that the revision is of high quality, impact, originality, and significance. The issue of a lack of prescriptiveness was raised by one reviewer. I agree with the majority that this is not necessary, but nevertheless, the revision makes some suggestions.  I urge the authors to express the appropriate amount of uncertainty regarding any prescriptions that have not been as thoroughly vetted!

"
iclr_2018_r1q7n9gAb,"The paper is tackling an important open problem.

AnonReviewer3 identified some technical issues that led them to rate the manuscript 5 (i.e., just below the acceptance threshold). Many of these issues are resolved by the reviewer in their review, and the author response makes it clear that these fixes are indeed correct.  However, other issues that the reviewer raises are not provided with solutions.  The authors address these points, but in one case at least (regarding w_infinity), I find the new text somewhat hand-waivy. Regardless, I'm inclined to accept the paper because the issues seem to be straightforward. Ultimately, the authors are responsible for the correctness of the results."
iclr_2018_ByQpn1ZA-,"AnonReviewers 2 and AnonReviewer 3 rated the paper highly, with AR3 even upgrading their score.  AnonReviewer1 was less generous:

"" Overall, it is a good empirical study, raising a healthy set of questions. In this regard, the paper is worth accepting. However, I am still uncomfortable with the lack of answers and given that the revision does not include the additional discussion and experiments promised in the rebuttal, I will stay with my evaluation.""

The authors have promised to produce the discussion and new experiments. Given the nature of both (1: the discussion is already outline in the response and 2: the experiments are straightforward to run), I'm inclined to accept the paper because it represents a solid body of empirical work."
iclr_2018_S1uxsye0Z,"The reviewers agreed that the work addresses an important problem. There was disagreement as to the correctness of the arguments in the paper: one of these reviewers was eventually convinced. The other pointed out another two issue in their final post, but it seems that 1. the first is easily adopted and does not affect the correctness of the experiments and 2. the second was fixed in the second revision. Ideally these would be rechecked by the third reviewer, but ultimately the correctness of the work is the authors' responsibility.

Some related work (by McAllister) was pointed out late in the process. I encourage the authors to take this related work seriously in any revisions. It deserves more than two sentences."
iclr_2018_BJij4yg0Z,"I'm inclined to recommend accepting this paper, although it is borderline given the strong dissenting opinion. The revisions have addressed many of the concerns about quality, clarity, and significance. The paper gives an end to end explanation in Bayesian terms of generalization in neural networks using SGD.

However, it is my opinion that Bayesian statistics is not, at present, a theory that can be used to explain why a learning algorithm works. The Bayesian theory is too optimistic: you introduce a prior and model and then trust both implicitly. Relative to any particular prior and model (likelihood), the Bayesian posterior is the optimal summary of the data, but if either part is misspecified, then the Bayesian posterior carries no optimality guarantee. The prior is chosen for convenience here. And the model (a neural network feeding into cross entropy) is clearly misspecified.

However, there are ways to sidestep both these issues using a frequentist theory closely related to Bayes, which can explain generalization. Indeed, you cite a recent such paper by Dzugate and Roy who use PAC-Bayes. However, you citation is disappointingly misleading: a reader would never know that these authors are also responding to Zhang, have already proposed to explain ""broad minima"" in (PAC-)Bayesian terms, and then even get nonvacuous bounds. (The connection between PAC-bayes and marginl likelihood is explained by Germain et al. ""PAC-Bayesian Theory Meets Bayesian Inference"").  Dzugate et al don't propose to explain why SGD finds such ""good"" minima. So I would say, your work provides the missing half of their argument. This work deserves more prominent placement and shouldn't be buried on page 5. Indeed, it should appear in the introduction and a proper description of the relationship should be given. "
iclr_2018_SyELrEeAb,"The reviewers agree that the work is high quality, clear, original, and could be significant.

Despite this, the scores are borderline. The reason is due to rough agreement that the empirical evaluations are not quite there yet. In particular, two reviewers agree that, in the synthetic experiments, the method is evaluated on data that is an order of magnitude too easy and quite far from the nature of real data, which has a much lower signal to noise ratio.

However, the authors have addressed the majority of the concerns and there is little doubt that the authors are capable of carrying out this new experiment and reporting its results. Even if the results are surprising, they should shed light on what seems to be an interesting new approach."
iclr_2018_HJC2SzZCW,"Reviewers always find problems in papers like this.

AnonReviewer1 would have preferred to have seen a study of traditional architectures, rather than fully connected ones, which are now less frequently used. They thought the paper was too long, the figures too cluttered, and were not convinced by the discussion around linear v. elliptical trajectories.

I appreciate the need for a parametrizable architecture, although it may not be justified to translate these insights to other architectures, and then the fact that fully connected architectures are less common undermines the impact of the work. I don't find the length a problem, and I don't find the figures a problem.

After the back and forth, AnonReviewer3 believes that there are data compatibility issues associated with the studied transformations and that non-linear transformations would have been more informative. I find the reviewers response to be convincing.

AnonReviewer2 is strongly in favor of acceptance, finding the work exhaustive, interesting, and of high quality. I'm inclined to agree.

"
iclr_2018_SyyGPP0TZ,This paper presents a simple yet effective method for weight dropping for an LSTM that requires no modification of an RNN cell's formulation.  Experimental results shows good perplexity results on benchmarks compared to many baselines.  All reviewers agree that the paper will bring good contribution to the conference.
iclr_2018_H1meywxRW,"This is an interesting paper that provides modeling improvements over several strong baselines and presents SOTA on Squad.  One criticism of the paper is that it evaluates only on Squad, which is somewhat of an artificial task, but we think for publication purposes at ICLR, the paper has a reasonable set of components."
iclr_2018_H196sainb,"There is significant discussion on this paper and high variance between reviewers:  one reviewer gave the paper a low score.  However the committee feels that this paper should be accepted at the conference since it provides a better framework for reproducibility, performs more large scale experiments than prior work.  One small issue the lack of comparison in terms of empirical results between this work and Zhang et al's work, but the responses provided to both the reviewers and anonymous commenters seem to be satisfactory."
iclr_2018_HkuGJ3kCb,This is a good paper with strong results via a set of simple steps for post processing off the shelf words  embeddings.  Reviewers are enthusiastic about it and the author responses are satisfactory.
iclr_2018_B18WgG-CZ,"This paper presents a very cool setup for multi task learning for learning fixed length representations for sentences.  Although the authors accept the fact that fixed length representations may not be suitable for complex, long pieces of text (often, sentences), such representations may be useful for several tasks.  They use a significantly large scale setup with six interesting tasks and show that learning generic representations for sentences across tasks is useful than learning in isolation.  Two out of three reviewers presented extensive critique of the paper and there's thorough back and forth between the reviewers and the authors.  The committee believes that this paper will add positive value to the conference."
iclr_2018_r1dHXnH6-,"This paper presents a marginally interesting idea -- that of an interaction tensor that compares two sentence representations word by word, and feeds the interaction tensor into a higher level feature extraction mechanism.  It produces good results on multi-NLI and SNLI datasets.  There is some criticism about comparing with several baselines for multi-NLI where there was a restriction of not using inter-sentence comparison networks, but the authors do compare with a similar approach without that restriction and shows improvements.   However, there is no solid error analysis that shows what type of examples this interaction tensor idea captures better than other strong baselines such as ESIM. Overall, the committee feels this paper will add value to the conference."
iclr_2018_SJ1nzBeA-,"Overall, the committee finds this paper to be interesting, well written and proposes an end to end model for a very relevant task.  The comparisons are also interesting and well rounded.  Reviewer 2 is critical of the paper, but the committee finds the answers to the criticisms to be satisfactory.  The paper will bring value to the conference."
iclr_2018_HkgNdt26Z,"The committee feels that this paper presents a simple, yet effective way to adapt language models from various users in a sufficiently privacy preserving way.  Empirical results are quite strong.  Reviewer 3 says that the novelty of the paper is not great, but does not provide any references to prior work that are similar to this paper.  The meta-reviewer finds the responses to Reviewer 3 sufficient to address the concerns.

Similarly, Reviewer 2 says that the paper may not be relevant to ICLR, but the committee feels its content does belong to the conference since the topic is extremely relevant to modern language processing techniques.  In fact, the authors provide several references that show that this paper is similar in content to those submissions.

Reviewer 1's concerns are also not sufficiently strong to warrant rejection.  The responses to each criticism suffices and the meta-reviewer thinks that this paper will add value to the conference."
iclr_2018_SkT5Yg-RZ,"I fully agree with strong positive statements in the reviews.  All reviewers agree that the paper introduces a novel and elegant twist on standard RL, wherein one agent proposes a sequence of diverse tasks to a second agent so as to accelerate the second agent's learning models of the environment.  I also concur that the empirical testing of this method is quite good.  There are strong and/or promising results in five different domains (Hallway, LightKey, MountainCar, Swimmer Gather and TrainingMarines in StartCraft). This paper would make for a strong poster at ICLR."
iclr_2018_SyoDInJ0-,"The reviewers are unanimous in accepting the paper.  They generally view it as introducing an original approach to online RL using bandit-style selection from a fixed portfolio of off-policy algorithms.  Furthermore, rigorous theoretical analysis shows that the algorithm achieves near-optimal performance.

The only real knock on the paper is that they use a weak notion of regret i.e. short-sighted pseudo regret.  This is considered inevitable, given the setting."
iclr_2018_S1vuO-bCW,"This paper is an easy accept -- three reviewers have above threshold scores, while one reviewer is slightly below threshold, but based on the submitted manuscript.  It appears that the paper has substantially improved based on reviewer comments.

Pros:

All reviews had positive sentiment: ""very elegant and general idea"" (Reviewer4); ""idea is interesting and potentially very useful"" (Reviewer2); ""method is novel, the explanation is clear, and has good experimental results"" (Reviewer3); ""a good way to learn a policy for resetting while learning a policy for solving the problem.  Seems like a fairly small but well considered and executed piece of work."" (Reviewer1)

Cons:

One reviewer found that testing in only three artificial tasks was a limitation.

The initial reviews noted several issues where clarification of the text and/or figures was needed.  There were also a bunch of statements where the reviewers questioned the technical correctness / accuracy of the discussion.  Most of these points appear to have been adequately addressed in the revised manuscript."
iclr_2018_BkabRiQpb,"The reviewer reactions to the initial manuscript were generally positive.  They considered the paper to be well written and clear, providing an original contribution to learning to cooperate in multi-agent deep RL in imperfect domains.  The reviewers raised a number of specific issues to address, including improved definitions and descriptions, and proper citations of related work.  The authors have substantially  revised the manuscript to address most or all of these issues.  At this point, the only knock on this paper is that the findings seemed unsurprising from a game-theoretic or deep learning point of view.

Pros: algorithmic contribution, technical quality, clarity
Cons: no real surprises "
iclr_2018_SkZxCk-0Z,"This paper studies the problem of modeling logical structure in a neural model.  It introduces a data set for probing various existing models and proposes a new model that addresses shortcomings in existing ones.  The reviewers point out that there is a bit of a tautology in introducing a new task and a new model that solves it.  The revised version addresses some of those concerns.  Overall, it is a thought-provoking and well-written study that will be interesting to discuss at ICLR."
iclr_2018_HyRVBzap-,"This paper forms a good contribution to the active area of adversarial training.  The main issue with the original submission was presentation quality and excessive length.  The revised version is much improved.  However, it still needs some work on the writing, in large part in the transferability section but also to clean up a large number of non-native formulations like missing/extra determiners and some awkward phrasing.  It should be carefully proofread by a native English speaker if possible.  Also, the citation formatting is incorrect (frequently using \citet instead of \citep)."
iclr_2018_Sk9yuql0Z,"Paper proposes adding randomization steps during inference time to CNNs in order to defend against adversarial attacks.

Pros:

- Results demonstrate good performance, and the team achieve a high rank (2nd place) on a public benchmark.
- The benefit of the proposed approach is that it does not require any additional training or retraining.

Cons:

- The approach is very simple, common sense would tend to suggest that adding noise to images would make adversarial attempts more difficult. Though perhaps simplicity is a good thing.
- Update: Paper does not cite related and relevant work, which takes a similar approach of requiring no retraining, but rather changing the inference stage: https://arxiv.org/pdf/1709.05583.pdf
  
Grammatical Suggestions:

This paper would benefit from polishing. For example:

- Abstract: sentence 1: replace “their powerful ability” to “high accuracy”
- Abstract: sentence 3: replace “I.e., clean images…” with “For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail”
- Abstract: sentence 4: replace “utilize randomization” to “implement randomization at inference time” or something similar to make more clear that this procedure is not done during training.
- Abstract: sentence 7: replace “also enjoys” with “provides”

Main Text: Capitalize references to figures (i.e. “figure 1” to “Figure 1”).

Introduction: Paragraph 4: Again, please replace “randomization” with “randomization at inference time” or something similar to better address reviewer concerns.
"
iclr_2018_BkpiPMbA-,"Authors propose an approach to generation of adversarial examples that jointly examine the effects to classification within a local neighborhood, to yield a more robust example. This idea is taken a step further for defense, whereby the classification boundaries within a local neighborhood of a presented example are examined to determine if the data was adversarially generated or not.


Pro:
- The idea of examining local neighborhoods around data points appears new and interesting.
- Evaluation and investigation is thorough and insightful.
- Authors made reasonable attempts to address reviewer concerns.

Con
 - Generation of adversarial examples an incremental improvement over prior methods

"
iclr_2018_HJWLfGWRb,"Authors present a new multi-layered capsule network architecture, implemented an EM routing procedure, and introduced ""Coordinate Addition"".  Capsule architectures are gaining interest because of their ability to achieve equivariance of parts, and employ a new form of pooling called ""routing"" (as opposed to max pooling) which groups parts that make similar predictions of the whole to which they belong, rather than relying on spatial co-locality. New state-of-art performances are being achieved on focused datasets, for which the authors have continued the trend.

Pros:
- New significant improvement to state-of-art performance is obtained on smallNORB, both in comparison to CNN structure as well as the most recent previous implementation of capsule network.

Cons:
- Some concern arose regarding the writing of the paper and the ability to understand the material, which authors have made an effort to address.

Given the general consensus of the reviewers that this work should be accepted, the general applicability of the technology to multiple domains, and the potential impact that improvements to capsule networks may have on an early field, area chair recommends this work be accepted as a poster presentation. "
iclr_2018_BJE-4xW0W,"This paper proposes an interesting machinery around Generative Adversarial Networks to enable sampling not only from conditional observational distributions but also from interven­tional distributions. This is an important contribution as this means that we can obtain samples with desired properties that may not be present in the training set; useful in applications such as ones involving fairness and also when data collection is expensive and biased. The main component called the causal controller models the label dependencies and drives the standard conditional GAN. As reviewers point out, the causal controller assumes the knowledge of the causal graph which is a limitation as this is not known a priori in many applications. Nevertheless, this is a strong paper that convincingly demonstrates a novel approach to incorporate causal structure into generative models. This should be of great interest to the community and may lead to interesting applications that exploit causality. I recommend acceptance.
"
iclr_2018_SJyEH91A-,"The paper presents a practical approach to compute Wasserstein distance based image embeddings. The Euclidean distance in the embedded space approximates the true Wasserstein distance, thus reducing the high computation cost associated with the latter.

Pros:
- Reviewers agree that the proposed solution is novel, straightforward and well described.
- Experiment demonstrate the usefulness of such embeddings for data mining tasks such as fast computation of barycenters & geodesic analysis.

Cons:
- Though the empirical analysis is convincing, the paper lacks theoretical analysis of the approximation quality. "
iclr_2018_BJNRFNlRW,"The paper makes a good theoretical contribution by formulating the GAN training as primal-dual subgradient method for convex optimization and providing convergence proof. The authors then propose a modified objective to standard GAN training, based on this formulation, that helps address the mode collapse issue.
One weak point of the paper as pointed out by reviewers is that that the experimental results are underwhelming and the approach may not scale well to high dimensional datasets / high-resolution images. Interestingly, the proposed approach is general enough to be applied to other GAN variants that may address this issue in future. I recommend acceptance."
iclr_2018_HyyP33gAZ,"The authors investigate various class aware GANs and provide extensive analysis of their ability to address mode collapse and sample quality issues. Based on this analysis they propose an extension called Activation Maximization-GAN which tries to push each generated sample to a specific class indicated by the Discriminator. As experiments show, this leads to better sample quality & helps with mode collapse issue. The authors also analyze inception score to measure sample quality and propose a new metric better suited for this task."
iclr_2018_SkVqXOxCb,"The paper provides an interesting take on GAN training based on Coulomb dynamics. The proposed formulation is theoretically well motivated and authors provide guarantees for convergence. Reviewers agree that the theoretical analysis is interesting but are not completely impressed by the results. The method addresses mode collapse issue but still lacks in sample quality. Nevertheless, reviewers agree that this is a good step towards the understanding of GAN training. "
iclr_2018_SJx9GQb0-,The paper proposes various improvements to Wasserstein distance based GAN training. Reviewers agree that the method produces good quality samples and are impressed by the state of the art results in several semi-supervised learning benchmarks. The paper is well written and the authors have further improved the empirical analysis in the paper based on reviewer comments.
iclr_2018_BJIgi_eCZ,"State-of-the-art results on Squad (at least at time of submission) with a nice model. Authors have since applied the model to additional tasks (SNLI). Good discussion with reviewers, well written submission and all reviewers suggest acceptance. "
iclr_2018_rkgOLb-0W,Nice language modeling paper with consistently high scores. The model structure is neat and the results are solid. Good ICLR-type paper with contributions mostly on the ML side and experiments on a (simple) NLP task.
iclr_2018_rk6cfpRjZ,"The reviewers really liked this paper. This paper presents a tweak to the LSTM cell that introduces sparsity, thus reducing the number of parameters in the model.

The authors show that their sparse models match the performance of the non-sparse baselines. While the results are not state-of-the-art but vanilla implementations of standard models, this is still of interest to the community."
iclr_2018_ry018WZAZ,"The reviewers liked this paper quite a bit. The novelty seems modest and the results are limited to a fairly simple NER task, but there is nothing wrong with the paper, hence recommending acceptance."
iclr_2018_Syg-YfWCW,"Good contribution. There was a (heated) debate over this paper but the authors stayed calm and patiently addressed all comments and supplied additional evaluations, etc.
"
iclr_2018_Sk7KsfW0-,"PROS:
1. good results; the authors made it work
2. paper is largely well written

CONS:
1. some found the writing to be unclear and sloppy in places
2. the algorithm is complicated -- a chain of sub-algorithms

A few small points:

-I initially found Algorithm 1 to be confusing because it wasn't clear whether it was intended to be invoked for each task (making the training depend on all the datasets).  I finally convinced myself that this was not the intention and that the inner loop of the algorithm is what is actually executed incrementally.

"
iclr_2018_H1VjBebR-,"The reviewers were generally positive about this paper with a few caveats:

PROS:
1. Important and challenging topic to analyze and any progress on unsupervised learning is interesting.
2. the paper is clear, although more formalization would help sometimes
3. The paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know.
4. A large set of experiments

CONS:
1. Some concerns about whether the claims are sufficiently justified in the experiments
2. The paper is very long and quite dense
"
iclr_2018_BJuWrGW0Z,"PROS:

1. Interesting and clearly useful idea
2. The paper is clearly written.
3. This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know).
4. This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.

CONS:

1. The paper has some clarity issues which the authors have promised to fix.

---"
iclr_2018_S1Euwz-Rb,"PROS:
1. Good results on CLEVER datasets
2. Writing is clear
3. The MAC unit is novel and interesting.
4. Ablation experiments are helpful

CONS:
The authors overstate the degree to which they are doing ""sound"" and ""transparent"" reasoning.  In particular statements such as ""Most neural networks are essentially very large correlation engines that will hone in on any statistical, potentially spurious pattern that allows them to model the observed data more accurately. In contrast, we seek to create a model structure that requires combining sound inference steps to solve a problem instance."" I think are not supported.  As far as I can tell, the authors' do not show that the steps of these solutions are really doing inference in any sound way

I also found the interpretability section to be a bit unconvincing.  The reviewers and I discussed this and there was some attempt to assess what the operations were actually doing but it is not clear how the language and the image attention are linked.

I wonder whether the learned control activations are abstract and re-used across problems the way that the accompanying functional solution's primitives are.  Have you looked at how similar the controls are across problems which are identical except for a different choice of attributes?  To me, one of the hallmarks of a truly ""compositional"" solution is one in which the pieces are re-used across problems, not just that there is some sequence of explicit control activations used to solve each individual problem."
iclr_2018_BkXmYfbAZ,"PROS:
1. Clear, interesting idea.
2. Largely convincing evaluation
3. Good writing

CONS:
1. The model used in the evaluation is a Resnet-50 and could have been more convincing with a more SOTA model.
2. There is some concern about the whether the comparison of results (fig 6c) is really apples to apples.
"
iclr_2018_BJQRKzbA-,"PROS:
1. Overall, the paper is well-written, clear in its exposition and technically sound.
2. With some caveats, an independent team concluded that the results were ""largely reproducible""
3. The key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models.
4. The implementation seems technically sound.

CONS:
1. The results were a bit over-stated (the authors promise to correct)
2. Could benefit from more comparison with other approaches (e.g. RL)"
iclr_2018_ryTp3f-0-,"
PROS:
1. well-written and clear
2. added extra comparison to dagger which shows success
3. SOTA results on open ai benchmark problem and comparison to relevant related work (Shi 2017)
4. practical applications
5. created new dataset to test harder aspects of the problem

CONS:
1. the algorithmic novelty is somewhat limited
2. some indication of scalability to real-world tasks is provided but it is limited"
iclr_2018_Hksj2WWAW,"Learn to complete an equation by filling the blank with a missing function or numeral, and also to evaluate an expression.  Along the way learn to determine if an identity holds (e.g. sin^2(x) + cos^2(x) = 1).  They use a TreeNN with a separate node for each expression in the grammar.

PROS:
1. They've put together a new dataset of equational expressions for learning to complete an equation by filling in the blank of a missing function (or value) and function evaluation.   They've done this in a nice way with a generator and will release it.

2. They've got two interesting ideas here and they seem to work.  First, they train the network to jointly learn to manipulate symbols and to evaluate them.  This helps ground the symbolic manipulations in the validity of their evaluations.  They do this by using a common tree net for both processes with both a symbol node and a number node.  They train on identities (sin^2(x) + cos^2(x) = 1) and also on ground expressions (+(1,2) = 3).  The second idea is to help the system learn the interpretation map for the numerals like the ""2"" in ""cos^2(x) with the actual number 2.  They do this by including equations which relate decimals with their base 10 expansion.  For example 2.5 = 2*10^0 + 5*10^-1.  The ""2.5"" is (I think) treated as a number and handled by the number node in the network.  The RHS leaves are treated as symbols and handled by the symbol node of the network. This lets them learn to represent decimals using just the 10 digits in their grammar and ties the interpretation of the symbols to what is required for a correct evaluation (in terms of their model this means ""aligning"" the node for symbol with the node for number).

3. Results are good over what seem to us reasonable baselines

CONS:

1. The architecture isn't new and the idea of representing expression trees in a hierarchical network isn't new either.

2. The writing, to me, is a bit unclear in places and I think they still have some work to do follow the reviewers' advice in this area.

I really wrestled with this one, and I appreciate the arguments that say it's not novel enough but I feel that there is something interesting in here and if the authors do a clean-up before final submission it will be ok."
iclr_2018_rkZB1XbRZ,"This paper extends last year's paper on PATE to large-scale, real-world datasets.  The model works by training multiple ""teacher"" models -- one per dataset, where a dataset might be for example, one user's data -- and then distilling those models into a student model. The teachers are all trained on disjoint data. Differential privacy is guaranteed by aggregating the teacher responses with added noise.  The paper shows improved teacher consensus by adding more concentrated noise and allowing the teacher to simply not respond to a student query.  The new results beat the old results convincingly on a variety of measures.

Quality and Clarity: The reviewers and I thought the paper was well written.

Originality: In some sense this work is incremental, extending and improving the existing PATE framework.  However, the extensions and new analysis are non-trivial and the results are good.

PROS:
1. Well written though difficult in places for somebody like myself who is not involved in this area.
2. Much improved scalability to real datasets
3. Good theoretical analysis supporting the extensions.
4. Comparison to related work (with a new comparison to UCI medical datasets used in the original paper and better results)

CONS:
1. Perhaps a little dense for the non-expert


"
iclr_2018_H1aIuk-RW,"The effectiveness of active learning techniques for training modern deep learning pipelines in a label efficient manner is certainly a very well motivated topic. The reviewers unanimously found the contributions of this paper to be of interest, particularly nice empirical gains over several natural baselines."
iclr_2018_BkrSv0lA-,"While novelty is not the main strength of this paper, there is consensus that presentation is clear and the experimental results are convincing. Given the practical importance of designing and benchmarking methods to compactify deep nets, the paper deserves to be presented at ICLR-2018."
iclr_2018_BJk7Gf-CZ,"Understanding global optimality conditions for deep nets even in the restricted case of linear layers is a valuable contribution. Please add clarifications to ways in which the paper goes beyond the results of Kawaguchi'16, which was the main concern expressed by the reviewers."
iclr_2018_HJ_aoCyRZ,"The paper proposes interesting  deep learning based spectral clustering techniques. The use of functional embeddings for enabling spectral clustering to have an out-of-sample extension has of course been explored earlier (e.g., see Manifold Regularization work of Belkin et al, JMLR 2006). For polynomials or kernel-based spectral clustering, the orthogonality of the outputs can be exactly handled via a generalized eigenvector problem, while here the arguments are statistically flavored and not made very clear in the original draft. Some crucial comparisons, e.g., against large-scale versions of vanilla spectral clustering and against other methods that generalize to new samples is missing or not thorough enough. See reviews for more precise description of issues. As such the paper will benefit from a revision.
"
iclr_2018_Hk8XMWgRb,"New effective kernel learning methods are very well aligned with ICLR's focus on Representation Learning. As a reviewer pointed out, not all aspects of the paper are algorithmically ""clean"". However, the proposed approach is natural and appears to give consistent improvements over a couple of expected baselines. The paper could be strengthened with more comparisons against other kernel learning methods, but acceptance at ICLR-2018 will increase the diversity of the conversation around advances in Representation Learning."
iclr_2018_Hkn7CBaTW,The paper shows that many of the current state-of-the-art interpretability methods are inaccurate even for linear models. Then based on their analysis of linear models they propose a technique that is thus accurate for them and also empirically provides good performance for non-linear models such as DNNs.
iclr_2018_ByOfBggRZ,The paper proposes a way of detecting statistical interactions in a dataset based on the weights learned by a DNN. The idea is interesting and quite useful as is showcased in the experiments. The reviewers feel that the paper is also quite well written and easy to follow.
iclr_2018_r1ZdKJ-0W,The paper proposes a method to embed graph nodes into a gaussian distribution rather than the standard latent vector embeddings. The reviewers concur that the method is interesting and the paper is well-written especially after the opportunity to update.
iclr_2018_H1BLjgZCb,The paper proposes a method to generate adversaries close to the (training) data manifold using GANs rather than arbitrary adversaries. They show the effectiveness of their method in terms of human evaluation and success in fooling a deep network. The reviewers feel that this paper is for the most part well-written and the contribution just about makes the mark.
iclr_2018_HyydRMZC-,"All reviewers gave ""accept"" ratings.
it seems that everyone thinks this is interesting work.

The paper generated a large number of anonymous comments and these were addressed by the authors. "
iclr_2018_ryBnUWb0b,"Reviewers agree that the paper is well done and addresses an interesting problem, but uses fairly standard ML techniques.
The authors have responded to rebuttals with careful revisions, and improved results. "
iclr_2018_SJLlmG-AZ,"An interesting model, for an interesting problem but perhaps of limited applicability - doesn't achieve state of the art results on practical tasks.
Paper has other limitations, though the authors have addressed some in rebuttals."
iclr_2018_r1HhRfWRZ,"Since this seems interesting, I suggest to accept this paper at the conference. However, there are still some serious issues with the paper, including missing references. "
iclr_2018_SyzKd1bCW,"This is an interesting and well-written paper introducing two unbiased gradient estimators for optimizing expectations of black box functions. LAX can handle functions of both continuous and discrete random variables, while RELAX is specialized to functions of discrete variables and can be seen as a version of the recently introduced REBAR with its concrete-relaxation-based control variate replaced by (or augmented with) a free-form function. The experimental section of the paper is adequate but not particularly strong. If Q-prop is the most similar existing RL approach, as is state in the paper, why not include it as a baseline? It would also be good to see how RELAX performs at optimizing discrete VAEs using just the free-form control variate (instead of combining it with the REBAR control variate)."
iclr_2018_rylSzl-R-,"This is a thought-provoking paper that places GANs and VAEs in a single framework and, motivated by this perspective, proposes several novel extensions to them. The reviewers made several good suggestions for improving the paper and the authors are expected to make the revisions they promised. The current title of the paper is too general and should be changed to something more directly descriptive of the contents."
iclr_2018_HyZoi-WRb,"The authors analyze the IWAE bound as an estimator of the marginal log-likelihood and show how to reduce its bias by using the jackknife. They then evaluate the effect of using the resulting estimator (JVI) for training and evaluating VAEs on MNIST. This is an interesting and well written paper. It could be improved by including a convincing explanation of the relatively poor performance of the JVI-trained, JVI-evaluated models."
iclr_2018_rkrC3GbRW,"Viewing the problem of determining the validity of high-dimensional discrete sequences as a sequential decision problem, the authors propose learning a Q function that indicates whether the current sequence prefix can lead to a valid sequence. The paper is fairly well written and contains several interesting ideas. The experimental results appear promising but would be considerably more informative if more baselines were included. In particular, it would be good to compare the proposed approach (both conceptually and empirically) to learning a generative model of sequences. Also, given that your method is based on learning a Q function, you need to explain its exact relationship to classic Q-learning, which would also make for a good baseline."
iclr_2018_rkTS8lZAb,"Training GANs to generate discrete data is a hard problem. This paper introduces a principled approach to it that uses importance sampling to estimate the gradient of the generator. The quantitative results, though minimal, appear promising and the generated samples look fine. The writing is clear, if unnecessarily heavy on mathematical notation."
iclr_2018_Hk0wHx-RW,"Observing that in contrast to classical information bottleneck, the deep variational information bottleneck (DVIB) model is not invariant to monotonic transformations of input and output marginals, the authors show how to incorporate this invariance along with sparsity in DVIB using the copula transform. The revised version of the paper addressed some of the reviewer concerns about clarity as well as the strength of the experimental section, but the authors are encouraged to improve these aspects of the paper further."
iclr_2018_S1cZsf-RW,"The paper proposes a new approach for scalable training of deep topic models based on amortized inference for the local parameters and stochastic-gradient MCMC for the global ones.  The key aspect of the method involves using Weibull  distributions (instead of Gammas) to model the variational posteriors over the local parameters, enabling the use of the reparameterization trick. The resulting methods perform slightly worse that the Gibbs-sampling-based approaches but are much faster at test time. Amortized inference has already been applied to topic models, but the use of Weibull posteriors proposed here appears novel. However, there seems to be no clear advantage to using stochastic-gradient MCMC instead of vanilla SGD to infer the global parameters, so the value of this aspect of WHAI unclear."
iclr_2018_H1MczcgR-,"An interesting analysis of the issue of short-horizon bias in meta-optimization that highlights a real problem in a number of existing setups. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. Nonetheless, I think the paper would be a interesting addition to ICLR and recommend acceptance."
iclr_2018_rkpoTaxA-,"An interesting application of self-ensembling/temporal ensembling for visual domain adaptation that achieves state of the art on the visual domain adaptation challenge. Reviewers noted that the approach is quite engineering-heavy, but I am not sure it's really much worse than making a pixel-to-pixel approach work well for domain adaptation.

I hope the authors follow through with their promise to add experiments to the final version (notably the minimal augmentation experiments to show just how much this domain adaptation technique is tailored towards imagenet-like things).

As it stands, this paper would be a good contribution to ICLR as it shows an efficient and interesting way to solve a particular visual domain adaptation problem."
iclr_2018_SJi9WOeRb,"The paper presents the Stein gradient estimator, a kernelized direct estimate of the score function for implicitly defined models. The authors demonstrate the estimator for GANs, meta-learning for approx. inference in Bayesian NNs, and approximating gradient-free MCMC. The reviewers found the method interesting and principled.  The GAN experiments are somewhat toy-ish as far as I am concerned, so I'd encourage the authors to try out larger-scale models if possible, but otherwise this should be an interesting addition to ICLR."
iclr_2018_B1nZ1weCZ,"The paper contains an interesting way to do online multi task learning, by borrowing ideas from active learning and comparing and contrasting a number of ways on the arcade learning environment.  Like the reviewers, I have some concerns about using the target scores and I think more analysis would be needed to see just how robust this method is to the choice/distribution of target scores (the authors mention that things don't break down as long as the scores are ""reasonable"", but that's not a particularly empirical nor precise statement).

My inclination is to accept the paper, because of the earnest efforts made by the authors in understanding how DUA4C works. However, I do agree that the paper should have a larger focus on that: basically Section 6 should be expanded, and the experiments should be rerun in such a way that the setup for DUA4C is more ""favorable"" (in terms of hyper-parameter optimization).  If there's any gap between any of the proposed methods and DUA4C, then this would warrant further analysis of course (since it would mean that there's an advantage to using target scores).  "
iclr_2018_rkHywl-A-,"The AIRL is presented as a scalable inverse reinforcement learning algorithm. A key idea is to produce ""disentangled rewards"", which are invariant to changing dynamics; this is done by having the rewards depend only on the current state. There are some similarities with GAIL and the authors argue that this is effectively a concrete implementation of GAN-GCL that actually works.  The results look promising to me and the portability aspect is neat and useful!

In general, the reviewers found this paper and its results interesting and I think the rebuttal addressed many of the concerns. I am happy that the reproducibility report is positive which helped me put this otherwise potentially borderline paper into the 'accept' bucket."
iclr_2018_B1DmUzWAW,"An interesting new approach for doing meta-learning incorporating temporal convolution blocks and soft attention. Achieves impressive SOTA results on few shot learning tasks and a number of RL tasks. I appreciate the authors doing the ablation studies in the appendix as that raises my confidence in the novelty aspect of this work. I thus recommend acceptance, but do encourage the authors to perform the ablation experiments promised to Reviewer 1 (especially the one to ""show how much SNAILs performance degrades when TCs are replaced with this method [of Vaswani et al.]."")"
iclr_2018_SywXXwJAb,"This paper seemingly joins a cohort of ICLR submissions which attempt to port mature concepts from physics to machine learning, make a complex and non-trivial theoretical contribution, and fall short on the empirical front. The one aspect that sets this apart from its peers is that the reviewers agree that the theoretical contribution of this work is clear, interesting, and highly non-trivial. While the experiment sections (MNIST!) is indubitably weak, when treating this as a primarily theoretical contribution, the reviewers (in particular 6 and 3) are happy to suggest that the paper is worth reading. Taking this into account, and discounting somewhat the short (and, by their own admission, uncertain) assessment of reviewer 5, I am leaning  towards pushing for the acceptance of this paper. At very least, it would be a shame not to accept it to the workshop track, as this is by far the strongest paper of this type submitted to this conference."
iclr_2018_Skp1ESxRZ,"This paper proposes a method for training an neural network to operate stack-based mechanism in order to act as a CFG parser in order to, eventually, improve program synthesis and program induction systems. The reviewers agreed that the paper was compelling and well supported empirically, although one reviewer suggested that analysis of empirical results could stand some improvement. The reviewers were not able to achieve a clear consensus on the paper, but given that the most negative reviewer has also declared themselves the least confident in their assessment, I am happy to recommend acceptance on the basis of the median rather than mean score."
iclr_2018_S1WRibb0Z,"This paper offers a theoretical and empirical analysis of the expressivity of RNNs, in particular in comparison to TT decomposition. The reviewers argued the results was interesting and important, although there were issues with clarity of some of the explanations. More critical reviewers argued the comparison basis with CP networks was not ""fair"" in that their shallowness restricted their expressivity w.r.t. TT. The experiments could be strengthened by making the explanations surrounding the set up clearer. This paper is borderline acceptable, and would have benefited from a more active discussion between the reviewers and the author. From reading the reviews and the author responses, I am leaning towards recommending acceptance to the main conference rather than the workshop track, as it is important to have theoretical work of this nature discussed at ICLR."
iclr_2018_rJlMAAeC-,"This paper present a functional extension to NPI, allowing the learning of simpler, more expressive programs.

Although the conference does not put explicit bounds on the length of papers, the authors pushed their luck with their initial submission (a body of 14 pages). It is clear, from the discussion and the reviews, however, that the authors have sought to substantially reduce the length of their paper while improving its clarity.

Reviewers found the method and experiments interesting, and two out of three heartily recommend it for acceptance to ICLR. I am forced to discount the score of the third reviewer, which does not align with the content of their review. I had discussed the issue of length with them, and am disappointed that they chose not to adjust their score to reflect their assessment of the paper, but rather their displeasure at the length of the paper (which, as stated above, does push the boundary a little).

Overall, I recommend accepting this paper, but warn the authors that this is a generous decision, heavily motivated by my appreciation for the work, and that they should be careful not to try such stunts in future conference in order to preserve the fairness of the submission process."
iclr_2018_HJvvRoe0W,"This paper addresses an important application in genomics, i.e. the prediction of chromatin structure from nucleotide sequences.  The authors develop a novel method for converting the nucleotide sequences to a 2D structure that allows a CNN to detect interactions between distant parts of the sequence.  The reviewers found the paper innovative, interesting and convincing.  Two reviewers gave a 7 and there was one 6.  The 6, however, indicated during rather lengthy discussion that they were willing to raise their scores if their comments were addressed.  Hopefully the authors will address these comments in the camera ready version.  Overall a solid application paper with novel insights and technical innovation.

"
iclr_2018_rydeCEhs-,"This paper proposes a method for having a meta deep learning model generate the weights of a main model given a proposed architecture.  This allows the authors to search over the space of architectures efficiently.  The reviewers agreed that the paper was very well composed, presents an interesting and thought provoking idea and provides compelling empirical analysis.  An exploration of the failure modes of the approach is highly appreciated.  The lowest score was also of quite low confidence, so the overall score should probably be one point higher.

Pros:
- Very well written and composed
- ""Thought provoking""
- Some strong experimental results
- Analysis of weaker experimental results (failure modes)

Cons:
- Some weak results (also in pros, however)"
iclr_2018_ByBAl2eAZ,"This paper proposes adding noise to the parameters of a deep network when taking actions in deep reinforcement learning to encourage exploration.  The method is simple but the authors demonstrate its effectiveness through thorough empirical analysis across a variety of reinforcement learning tasks (i.e. DQN, DDPG, and TRPO).  Overall the paper is clear, well written and the reviewers enjoyed it.  However, a common trend among the reviews was that the authors overstated their claims and contributions.  The reviewers called out some statements in particular (e.g. the discussion of ES and RL) which the authors appear to have addressed when comparing their revisions (thank you).  Overall, a clear, well written paper conveying a simple but effective idea for exploration that often works across a variety of RL tasks.  The authors also released open-source code along with their paper for reproducibility (as evidenced by the reproducibility study below), which is appreciated.

Pros:
- Clear and well written
- Thorough experiments across deep RL domains
- A simple strategy for exploration that is effective empirically

Cons:
- Not a panacea for exploration (although nothing really is)
- Claims are somewhat overstated
- Lacks a strong justification for the method other than that it is empirically effective and intuitive"
iclr_2018_r1VVsebAZ,"This paper proposes a novel application of generative adversarial networks to model neural spiking activity.  Their technical contribution, SpikeGAN, generates neural spikes that accurately match the statistics of real recorded spiking behavior from a small number of neurons.

The paper is controversial among the reviewers with a 4, a 6 and an 8.  The 6 is short and finds the idea exciting but questions the utility of the proposed approach in terms of actually studying neural spiking.  The 4 and 8 are both quite thorough reviews.  4 seems to mostly question the motivation of using a GAN over a MaxEnt model and demands empirical comparison to other approaches.  8 applauds the paper as a well-executed pure application paper, applying recent innovations in machine learning to an important application with some technical innovation.  Overall the reviewers found the paper clear and easy to follow and agree that the application of GANs to neural spiking activity is novel.  In general, I find that such high variance in scores (with thorough reviews) indicate that the paper is exciting, innovative and might stir up some interesting discussion.  As such, and under the belief that ICLR is made stronger with interesting application papers, I feel inclined to accept as a poster.

Pros:
- A novel application of GANs to neural spiking data
- Addresses an important and highly studied application area (computational neuroscience)
- Clearly written and well presented
- The approach appears to model well real neural spiking activity from salamander retina

Cons:
- Known pitfalls of GANs aren't really addressed in the paper (mode collapse, etc.)
- The authors don't compare to state of the art models of neural spiking activity (although they compare to an accepted standard approach - MaxEnt)
- Limited technical innovation over existing methods for GANs"
iclr_2018_BJ8c3f-0b,"This work develops importance weighted autoencoder-like training but with sequential Monte Carlo.  The paper is interesting, well written and the methods are very timely (there are two highly related concurrent papers -  Naesseth et al. and Maddison et al.).  Initially, the reviewers shared concerns about the technical details of the paper, but the authors appear to addressed those and two reviews have been raised accordingly.   There is one outlier review (two 7s and one 3).  The 3 is the least thorough and has the lowest confidence (2) so that review is being weighted accordingly.

This appears to be a timely and interesting paper that is interesting to the community and warrants publication at ICLR.

Pros:
- Well written and clear
- An interesting approach
- Neat technical innovations
- Generative deep models are of great interest to the community (e.g. Variational Autoencoders)

Cons:
- Could include a better treatment of recent related literature
- Leaves a variety of open questions about specific details (i.e. from the reviews)"
iclr_2018_HJewuJWCZ,"The paper addresses the problem of learning a teacher model which selects the training samples for the next mini-batch used by the student model. The proposed solution is to learn the teacher model using policy gradient. It is an interesting training setting, and the evaluation demonstrates that the method outperforms the baseline. However, it remains unclear how the method would scale to larger datasets, e.g. ImageNet. I would strongly encourage the authors to extend their evaluation to larger datasets and state-of-the-art models, as well as include better baselines, e.g. from Graves et al."
iclr_2018_Syhr6pxCW,"The paper proposes a novel method for conditional image generation which is based on nearest neighbor matching for transferring high-frequency statistics. The evaluation is carried out on several image synthesis tasks, where the technique is shown to perform better than an adversarial baseline."
iclr_2018_B1l8BtlCb,"The paper proposes a novel method for training a non-autoregressive machine translation model based on a pre-trained auto-regressive model. The method is interesting and the evaluation is carried out well. It should be noted, however, that the relative complexity of the training procedure (which involves multiple stages and external supervision) might limit the practical applicability and impact of the technique."
iclr_2018_HJtEm4p6Z,"The paper describes a production-ready neural text-to-speech system. The algorithmic novelty is somewhat limited, as the fully-convolutional sequence model with attention is based on the previous work. The main contribution of the paper is the description of the complete system in full detail. I would encourage the authors to expand on the evaluation part of the paper, and add more ablation studies."
iclr_2018_r1Ddp1-Rb,"The paper presents a simple but surprisingly effective data augmentation technique which is thoroughly evaluated on a variety of classification tasks, leading to improvement over state-of-the-art baselines. The paper is somewhat lacking a theoretical justification beyond intuitions, but extensive evaluation makes up for that."
iclr_2018_HyiAuyb0b,This is an interesting piece of work that provides solid evidence on the topic of bootstrapping in deep reinforcement learning.
iclr_2018_ry1arUgCW,"This is a very interesting paper that also seems a little underdeveloped. As noted by the reviewers, it would have been nice to see the idea applied to domains requiring function approximation to confirm that it can scale -- the late addition of Freeway results is nice, but Freeway is also by far the simplest exploration problem in the Atari suite. There also seems to be a confusion between methods such as UCB, which explore/exploit, and purely exploitative methods. The case gamma_E > 0 is also less than obvious. Given the theoretical leanings of the paper, I would strongly encourage the authors to focus on deriving an RMax-style bound for their approach.
"
iclr_2018_Skw0n-W0Z,"There is a concern from one of the reviewers that the paper needs deeper analysis. On the other hand, applying finite horizon techniques to deep RL is relatively unexplored, and the paper does provide some interesting results in that direction.
"
iclr_2018_H1dh6Ax0Z,"This is a nicely written paper proposing a reasonably interesting extension to existing work (e.g.
VPN). While the Atari results are not particular convincing, they do show promise. I encourage
the authors to carefully take the reviewers' comment into consideration and incorporate them
to the final version."
iclr_2018_S19dR9x0b,The reviewers unanimously agree that this paper is worth publication at ICLR. Please address the feedback of the reviewers and discuss exactly how the potential speed up rates are computed in the appendix. I speed up rates to be different for different devices.
iclr_2018_HJNMYceCW,"The reviewers agree that the problem of learning learning credit assignment from terminal rewards is interesting, and that the presented approach is promising. There are some concerns regarding the rigor and correctness of the theoretical results, and I ask the authors to improve those aspects of the paper. I also ask the authors to the result figures easier to read. The chosen colors are not ideal and the use of log-scale x-axis is not standard. Finally, including DAgger in the same plot is confusing assuming that DAgger user more information."
iclr_2018_SyOK1Sg0W,"Given the changes to the paper, the reviewers agree that the paper meets the bar for publication at ICLR. There are some concerns regarding the practical impact on CPUs and GPUs. I ask the authors to clearly discuss the impact on different hardware. One can argue if adaptive quantization techniques are helpful, then there is a chance that future hardware will support them. All of the experiments are conducted on toy datasets. Please consider including some experiments on Imagenet as well."
iclr_2018_BkUp6GZRW,"All of the reviewers agree that the paper clearly presents promising ideas in developing a novel actor critic algorithm. The experiments do not show a significant gain against the baselines, but they support the presented ideas. I appreciated the ablation study on dual-AC.

Detailed comments:
My understanding is that the x-axis in Figures 1 & 2 shows the number of iterations each of which contains batch_size*1000 environment steps. It is more standard to show those plots in terms of the number of environment steps. Further, the optimal batch_size for different algorithms may be different, so using the same batch_size for all of the algorithms is not fair."
iclr_2018_BJk59JZ0b,"The reviewers agree that the formulation is novel and interesting, but they raised concerns regarding the motivation and the complexity of the approach. I find the authors' response mostly satisfying, and I ask them to improve the paper by incorporating the comments.

Detailed comments:
The maximum-entropy objective used in Eq. (13) reminds me of maximum-entropy RL objective in previous work including [Ziebart, 2010], [Azar, 12], [Nachum, 2017], and [Haarnoja, 2017]."
iclr_2018_ByOnmlWC-,"At least two of the reviewers found the proposed approach novel and interesting and worthy of publication at ICLR. The reviewers raised concerns regarding the paper's terminology, which may lead to some misunderstanding. I agree that upon a quick skim, a reader may think that the paper performs the crossover operation outlined at the bottom right of Figure 1. Please consider improving the figure and the caption to prevent such a misunderstanding. You can even slightly change the title to reflect the policy distillation operation rather than naive crossover. Finally, including some more complex baselines benefits the paper. I am curious whether performing policy gradient on an ensemble of 8 policies + periodic removal of the bottom half of the policies will provide similar gains."
iclr_2018_SkA-IE06W,"Dear authors,

The reviewers all appreciated your work and agree that this a very good first step in an interesting direction."
iclr_2018_BkrsAzWAb,"All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work. As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at ICLR."
iclr_2018_HyWrIgW0W,"Dear authors,

Based on the comments and your rebuttal, I am glad to accept your paper at ICLR."
iclr_2018_ByrZyglCb,The idea of universal perturbation is definitely interesting and well carried out in that paper.
iclr_2018_B1hYRMbCW,"This paper proposes an interesting analysis of the limitations of WGANs as well as a solution to these limitations. I am not too convinced by the experimental part as, as some of the reviewers have mentioned, it relies on hyperparameters which can be hard to tune.

The more theoretical part, even if it could be written with more care as pointed out by reviewer 2, is nonetheless interesting and could stir discussion. I think it would be a good addition to ICLR as a poster."
iclr_2018_Bk8ZcAxR-,"This paper on automatic option discovery connects recent research on successor representations with eigenoptions. This is a solidly presented, conceptual paper with results in tabular and atari environments. "
iclr_2018_Bk9zbyZCZ,"Biological memory systems are grounded in spatial representation and spatial memory, so neural methods for spatial memory are highly interesting. The proposed method is novel, well-designed and the empirical results are good on unseen environments, although the noise model may be too weak. Moreover, it would have been great to evaluate this method on real data rather than in simulation. "
iclr_2018_ry6-G_66b,The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The method compares well to baselines and has good accuracy in 2d and 3d envs. All three reviewers are in favor of acceptance due to the novelty and competitive performance of the approach.
iclr_2018_B1al7jg0b,"This paper is a timely application of linear algebra to propose a method for reducing catastrophic interference by training a new task in a subspace of the parameter space using conceptors. The conceptors are deployed in the backprop, making this a valuable alternative to recent continual learning methods such as EWC. The paper is clearly written and the results give a clear validation of the method. The reviewers agree as to the merits of the paper."
iclr_2018_HyfHgI6aW,"The authors have proposed an architecture that incorporates a VIN with a DNC to combine low level planning with high level memory-based optimization, resulting in a single policy for navigation and other similar problems that is trained end-to-end with sparse rewards. The reviews are mixed, but the authors did allay the concerns of the most negative reviewer by adding a comparison to traditional motion planning (A*) algorithms. "
iclr_2018_B13njo1R-,"The authors propose an architecture that uses a curriculum and multi-task distillation to gain higher performance without forgetting. The paper is largely a smart composition of known methods, and it requires keeping data from all tasks to do the distillation, so it is not truly a scalable continual learning approach. There were a lot of concerns about clarity in the manuscript, but many of these have been assuaged by an update to the paper. This is a borderline paper, but the author's rebuttal and update probably tip it towards acceptance. "
iclr_2018_B1hcZZ-AW,"This is a meta-learning approach to model compression which trains 2 policies using RL to reduce the capacity (computational cost) of a trained network while maintaining performance, such that it can be effectively transferred to a smaller student network. The approach has similarities to recently proposed methods for architecture search, but is significantly different. The paper is well written and the experiments are clear and convincing. One of the reviews was unacceptable; I am not considering it (R1)."
iclr_2018_SJJQVZW0b,"This method has a lot of strong points, but the reviewers had concerns about baselines, comparisons, and hand-engineered aspects of the method. The authors gave a strong rebuttal and made substantial updates to the paper to address the concerns. I think that this has saved the submission and tipped the balance towards acceptance. "
iclr_2018_rJwelMbR-,"This paper proposes a specific architecture for training an ensemble of separate policies on a family of easier tasks with the goal of obtaining a single policy that can perform well on a harder task. There are significant similarities to the recently published Distral algorithm, but I am convinced that this work offers a meaningful contribution beyond that work. Moreover, the authors performed a thorough comparison between their method and Distral and found that DnC performs better."
iclr_2018_B1e5ef-C-,"sadly, none of the reviewers seem to have been able to fully appreciate and check the proofs.

but in the words of even the least positive reviewer:
In general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.

i think we can all gain from fresh perspectives of LSTMs and DL for NLP :)
"
iclr_2018_BkSDMA36Z,"despite not amazing scores, this is a solid paper.
it created a lot of discussion and was found to be reproducible.
we should accept it to let the iclr community partake in the discussion and learn about this method of n-gram embeddings
"
iclr_2018_S1Dh8Tg0-,"This paper proposes an interesting new idea which creates an interesting discussion.
"
iclr_2018_HyRnez-RW,The authors did a good job addressing reviewer concerns and analyzing and  testing their model on interesting datasets with convincing results.
iclr_2018_r1SnX5xCb,"This paper is well written, addresses and interesting problem, and provides an interesting solution."
iclr_2018_HkZy-bW0-,"This paper provides an interesting synthesis of ideas. Although the results could be improved, this is a good paper."
iclr_2018_ry-TW-WAb,The paper presents a variational Bayesian approach for quantising neural network weights and makes interesting and useful steps in this increasingly popular area of deep learning.
iclr_2018_SJJySbbAZ,The reviewers thought the paper provides an interesting line of research.
iclr_2018_SJA7xfb0b,The paper provides a useful analysis of the role of gradient penalties and the performance of the proposed approach in semi-supervised cases.
iclr_2018_H1sUHgb0Z,"This paper provides an important discussion about the relationship between training efficiency and label redundancy. The updates to the paper will improve the paper further. Reviewers found the paper interesting, well written, and addresses and important problem."
iclr_2018_H1Y8hhg0b,"The results in the paper are interesting, and the modifications improve the paper further. Reviewers found teh paper interesting and potentailly applicable to many models."
iclr_2018_BkQqq0gRb,The paper addresses the problem of continual learning and solutions based on variational inference. Updates to the paper have improved it and addresses many of the concerns raised by the reviewers during the discussion period.
iclr_2018_H1-nGgWC-," A clearly written paper. While the practical relevance that came up in the review remains, the analysis and discussion is important for a deeper understanding of the deeper connections between these two important areas of machine learning."
iclr_2018_H135uzZ0-,"Mixed precision application of CNNs is being explored for e.g. hardware implementations of networks trained at full precision.  Mixed precision at training time is less common.  This submission primarily concerns itself with the practical implementation details of training with mixed precision, and focuses primarily on representation of mixed precision floating point and algorithmic issues for learning.  In the end the support for the approach is primarily empirical, with the mixed precision approach giving a factor of two speedup with half the precision, while accuracies remain effectively statistically tied on the ImageNet 1k database.  Table 1 should avoid the use of bold as there is likely no statistical significance.

The reviewers appreciated the paper. The proposed approach is sensible, and appears correct."
iclr_2018_SkFqf0lAZ,"This paper provides a comparison of different types of a memory augmented models and extends some of them to beyond their simple form. Reviewers found the paper to be clearly written, saying it ""nice introduction to the topic"" and noting that they ""enjoyed reading this paper"". In general though there was a feeling that the ""substance of the work is limited"". One reviewer complained that experiments were limited to small English datasets PTB and Wikitext-2 and asked why they didn't try ""machine translation or speech recognition"". (The author's note that they did try the Linzen dataset, and while the reviewers found the experiments impressive, the task itself felt artificial) . Another felt that the ""multipop model"" alone was not too large a contribution. The actual experiments in the work are well done, although given the fact that the models are known there was expectation of ""more ""in-depth"" analysis of the different models"". Overall this is a good empirical study, which shows the limited gains achieved by these models, a nevertheless useful piece of information for those working in this area."
iclr_2018_ry_WPG-A-,"
This submission explores recent theoretical work by Shwartz-Ziv and Tishby on explaining the generalization ability of deep networks. The paper gives counter-examples that suggest aspects of the theory might not be relevant for all neural networks.

There is some uncertainty surrounding the results where mutual information is estimated empirically. Even state-of-the-art estimation methods might lead to misleading empirical results. However, the submission appears to follow reasonable practice following previous work, making the reported results at least suggestive. They warrant reporting for further study and discussion.

The reviewers generally found the paper interesting enough for acceptance, however strong objections were posted by Tishby. A lengthy public exchange resulted between the groups of authors. Not every part of this exchange is resolved. It is not clear whether Tishby's group would be able to fix the full-connected ReLU demonstration in this paper, or whether the authors of this submission have anything to say about Tishby's ReLU+convnet demonstration. By accepting this work, we are not declaring where this debate will end. However, we felt the current submission is a constructive part of ongoing discussion in the literature on furthering our theoretical understanding of neural networks."
iclr_2018_BJJ9bz-0-,"I appreciate the experimental results, which includes a comparison against several baselines, however, I echo some of the concerns raised by the reviewers that the formulation is unclear and hard to follow. Moreover, the novelty over [Nachum, 2017] and [Haarnoja, 2017] seems small. Especially because [Nachum, 2017] also used expert trajectories to improve the performance in their experiments.

Detailed comment:
The use of log-sum-exp state values is only valid for the optimal policy, so it is not clear how an on-policy state value is replaced with the log-sum-exp state value. Also, because the equations that you derive characterize the optimal policy, I am not sure if you need importance correction at all."
iclr_2018_HJjvxl-Cb,"The reviewers agree that the results are promising and there are some interesting and novel aspect to the formulation. However, two of the reviews have raised concerns regarding the exposition and the discussion of previous work. The paper benefits from a detailed description of soft Q-learning, PCL, and off-policy actor-critic algorithms, and how SAC is different from those. Instead of differentiating against previous work by saying soft Q-learning and PCL are not actor-critic algorithms, discuss the similarities and differences and present empirical evaluation."
iclr_2018_rk6H0ZbRb,"I am somewhat of two minds from the paper. The authors show empirically that adversarial perturbation error follows power law and looks for a possible explanation. The tie in with generalization is not clear to me and makes me wonder how to evaluate the significance of the finding of the power law distribution..  On the other hand, the authors present an interesting analysis, show that the finding holds in all the cases they explored and also found that architecture search can be used to find neural networks that are more resilient to adversarial search (the last shouldn't be surprising if that was indeed the training criterion).

All in all, I think that while the paper needs a further iteration prior to publication, it already contains interesting bits that could spur very interesting discussion at the Workshop.

(Side note: There's a reference missing on page 4, first paragraph)"
iclr_2018_BJy0fcgRZ,"This paper introduces a GAN-based framework for inferring human category representations. The reviewers agree that the idea is interesting and well-motivated, and the results are promising. The technical contribution is not significant, but nevertheless the paper combines existing ideas in an interesting way. The reviewers would also like to see some more work towards the direction of investigation of the results and extraction of insights, without which the paper feels somehow incomplete."
iclr_2018_Sy4c-3xRW,"This paper proposes a general regularization algorithm which builds on the dropout idea. This is a very significant topic. The overall motivation is good, but the specific design choices are less well motivated over, for example, ad-hoc choices. Some concerns remain after the post-rebuttal discussion with the reviewers: the improvement is incremental in terms of concepts and methodology, the clarity needs to be improved and the experiments are somehow weak.
In summary, the main idea and research direction is interesting, but the attempted generality of the algorithm and the significance of the area call for a more clear and convincing presentation.
"
iclr_2018_SJUX_MWCZ,"This work is proposing an approach for ensuring classification fairness through models that encapsulate deferment criteria. On the positive side, the paper provides ideas which are conceptually interesting and novel. On the other hand, the reviewers find the technical contribution to be limited and, in some cases, challenge the practicality of the method (e.g. requirement for second set of training samples). After extensive post-rebuttal discussion, the consensus is that the above issues make the paper fall below the threshold for acceptance – even if the “out-of-scope” issue is not taken into account."
iclr_2018_r1hsJCe0Z,"To summarize the pros and cons:

Pro:
* Interesting application
* Impressive results on a difficult task
* Nice discussion of results and informative examples
* Clear presentation, easy to read.

Con:
* The method appears to be highly specialized to the four bug types. It is not clear how generalizable it will be to more complex bugs, and to the real application scenarios where we are dealing with open world classification and there is not fixed set of possible bugs.

There were additional reviewer complaints that comparison to the simple seq-to-seq baseline may not be fair, but I believe that these have been addressed appropriately by the author's response noting that all other reasonable baselines require test cases, which is an extra data requirement that is not available in many real-world applications of interest.

This paper is somewhat on the borderline, and given the competitive nature of a top conference like ICLR I feel that it does not quite make the cut. It is definitely a good candidate for presentation at the workshop however."
iclr_2018_rk3pnae0b,"The pros and cons of the paper under consideration can be summarized below:

Pros:
* Reviewers thought the underlying model is interesting and intuitive
* Main contributions are clear

Cons:
* There is confusion between keywords and topics, which is leading to a somewhat confused explanation and lack of clear comparison with previous work. Because of this, it is hard to tell whether the proposed approach is clearly better than the state of the art.
* Typos and grammatical errors are numerous

As the authors noted, the concerns about the small dataset are not necessarily warranted, but I would encourage the authors to measure the statistical significance of differences in results, which would help alleviate these concerns.

An additional comment: it might be worth noting the connections to query-based or aspect-based summarization, which also have a similar goal of performing generation based on specific aspects of the content.

Overall, the quality of the paper as-is seems to be somewhat below the standards of ICLR (although perhaps on the borderline), but the idea itself is novel and results are good. I am not recommending it for acceptance to the main conference, but it may be an appropriate contribution for the workshop track."
iclr_2018_SJDJNzWAZ,"I've summarized the pros and cons of the reviews below:

Pros:
* The method for time representation in event sequences is novel and well founded
* It shows improvements on several but not all datasets that may have real-world applications

Cons:
* Gains are somewhat small
* The task is also not of huge interest to ICLR in particular, and thus the paper might be of limited interest

As a result, because the paper is well done, but drew little excitement from any of the reviewers, I suggest that this not be accepted to the main conference, but encouraged to present at the workshop track."
iclr_2018_SkHl6MWC-,"R1 thought the proposed method was novel and the idea interesting. However, he/she raised concerns with consistency in the experimental validation, the trade-off between accuracy and running time, and the positioning/motivation, specifically the claim about interpretability. The authors responded to these concerns, and R1 upgraded their score. R2 didn’t raise major concerns or strengths. R3 questioned the novelty of the work and the experimental validations. All reviewers raised concerns with the writing. Though I think the work is interesting, issues raised about experiments and writing make me hesitant to go against the overall recommendation of the reviewers, which is just below the bar. I think this is a paper that could make a good workshop contribution.
"
iclr_2018_rJWrK9lAb,"The reviewers (all experts in this area) appreciated the novelty of the idea, though they felt that the experimental results (samples and Inception scores) did not provide convincing evidence value of this method over already established techniques. The authors responded to the concerns but were not able to address the issue of evaluation due to time constraints. The idea is likely sound but evaluation does not meet the bar, it may make a good contribution as a workshop paper."
iclr_2018_Hy1d-ebAb,"Predicting graphs is an interesting and important direction, and there exist essentially no (effective) general-purpose techniques for this problem.  The idea of predicting nodes one by one, though not entirely surprising, is interesting and the approach makes sense. Unfortunately, I (and some of reviewers) less convinced by evaluation:

-  For example, evaluation on syntactic parsing of natural language is very weak. First of all, the used metric -- perplexity and exact match are non-standard and problematic (e.g., optimizing exact match would largely correspond to ignoring longer sentences where predicting the entire tree is unrealistic).  Also the exact match scores are very low (~30% whereas 45+ were achieve by models back in 2010).

- A reviewer had, I believe, valid concerns about comparison with GrammarVAE, which were not fully addressed.

Overall, I believe that it is interesting work, which regretfully cannot be published as a conference paper in its current form.

+ important / under-explored problem
+ a reasonable (though maybe not entirely surprising / original) approach
- issues with evaluation

"
iclr_2018_r1lfpfZAb,"I (and some of the reviewers) find the general motivation quite interesting (operationalizing the Gricean maxims in order to improve language generation). However, we are not  convinced that the actual model encodes these maxims in a natural and proper way.  Without this motivation, the approach can be regarded as a set of heuristics which happen to be relatively effective on a couple of datasets.  In other words, the work seems too preliminary to be accepted at the conference.

Pros:
-- Interesting motivation (and potential impact on follow-up work)
-- Good results on a number of datasets
Cons:
-- The actual approach can be regarded as a set of heuristics, not necessarily following from the maxims
-- More serious evaluation needed (e.g., image captioning or MT) and potential better ways of encoding the maxims

It is suitable for the workshop track, as it is likely to stimulate an interesting discussion and more convincing follow-up work.

"
iclr_2018_ryZ283gAZ,"The reviewers agree that the proposed architecture is novel. However, there are issues in terms of the motivation. It would be helpful in future drafts to strengthen the argument about why the architecture is expected to be better than others. Most importantly, the gains at this stage are still incremental. A larger improvement from the new architecture would motivate more researchers to focus on this architecture."
iclr_2018_B14uJzW0b,"This submission is a continuation of a line of theoretical work that seeks to characterize optimization landscapes of neural networks by the presence or absence of spurious local minima.  As the number of critical points grows combinatorially for larger networks, it is very challenging to show such results.  The present submission extends slightly previous work by considering two hidden units and their proof technique goes beyond that of Brutzkus and Globerson, 2017, potentially leading to more interesting results if they can be extended to more complex networks.

The setting of two hidden units is quite limited - far from any practical setting.  If this were the stepping stone to proving optimality of certain optimization strategies for more complex networks, this may be of some interest, but it seems doubtful.  One indication is given in Sec. 7 / Fig. 1 in which it is shown that for even quite small numbers of hidden units, spurious local optima do occur and are reached 40% of the time for random initializations even with only 11 nodes.

"
iclr_2018_Bki4EfWCb,"Thank you for submitting you paper to ICLR. This paper provides an informative analysis of the approximation contributions from the various assumptions made in variational auto-encoders. The revision has demonstrated the robustness of the paper’s conclusions, however these conclusions are arguably unsurprising. Although the work provides a thorough and interesting piece of detective work, the significance of the findings is not quite great enough to warrant publication.

Reviewer 1 was searching for a reference for work in similar vein to section 5.4: The second problem identified in the reference below shows examples where using an approximating distribution of a particular form biases the model parameter estimates to settings that mean the true posterior is closer to that form.

R. E. Turner and M. Sahani. (2011) Two problems with variational Expectation Maximisation for time-series models. Inference and Learning in Dynamic Models. Eds. D. Barber, T. Cemgil and S. Chiappa, Cambridge University Press, 104–123, 2011."
iclr_2018_B1lMMx1CW,"Meta score: 6

This is a thorough empirical paper, demonstrating the effectiveness of a relatively simple model for recommendations:

Pros:
 - strong experiments
 - always good to see simple models pushed to perform well
 - presumably of interest to practioners in the area

Cons:
 - quite oriented to the recommendation application
 - technical novelty is in the experimental evaluation rather than any new techniques

On balance I recommend the paper is invited to the workshop."
iclr_2018_rk8wKk-R-,"meta score: 5

This paper gives a thorough experimental comparison of convolutional vs recurrent networks for a variety of sequence modelling tasks.  The experimentation is thorough, but the main point of the paper,  that convolutional networks are unjustly ignored for sequence modelling, is overstated as there are several areas where convolutional networks are well explored.
Pros:
 clear and well-written
 thorough set of experiments
Cons
 original contribution is not strong
 it is not as radical to consider convolutional networks for sequence modeling as the authors seem to suggest
"
iclr_2018_Hkfmn5n6W,"The paper analyzes neural network with hidden layer of piecewise linear units, a single output, and a quadratic loss. The reviewers find the results incremental and not ""surprising"", and also complained about comparison with previous work. I think the topic is very pertinent, and definitely more relevant compared to studying multi-layer linear networks. Hence, I recommend the paper be presented in the workshop track."
iclr_2018_ByxLBMZCb,"The paper nicely unifies previous results and develops the property of local openness. While interesting, I find the application to multi-layer linear networks extremely limiting. There appears to be a sub-field in theory now focusing on solely multi-layer linear networks which is meaningless in practice. I can appreciate that this could give rise to useful proof techniques and hence, I am recommending it to the workshop track with the hope that it can foster more discussions and help researchers move away from studying multi-layer linear networks."
iclr_2018_rJGY8GbR-,All the reviewers agree that this is an interesting paper but have concerns about readability and presentation. There is also concern that many results are speculative and not concretely tested. I recommend the authors to carefully investigate their claims with stronger experiments and submit it to another venue. I recommend presenting at ICLR workshop to obtain further feedback.
iclr_2018_BygpQlbA-,"This paper studies the control of symmetric linear dynamical systems with unknown dynamics. While the reviewers agree that this is an interesting topic, there are concerns that the assumptions are not realistic. Lack of experiments also stands out. I recommend the paper to workshop track with the hope that it will foster more discussions and lead to more realistic assumptions."
iclr_2018_HJYQLb-RW,All the reviewers agree that the paper is studying an important problem and makes a good first step towards understanding learning in GANs. But the reviewers are concerned that the setup is too simplistic and not relevant in practical settings. I recommend the authors to carefully go through reviews and to present it at the workshop track. This will hopefully foster further discussions and lead to results in more practically relevant settings.
iclr_2018_H1YynweCb,"I tend to agree with the most positive reviewer who characterizes the work with the following statements:

""Kronecker factorization was introduced for Convolutional networks (citation is in the paper). Soft unitary constraints also have been introduced in earlier work (citations are also in the paper). Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution.""

The most negative reviewer feels that the experimental work could have evaluated the different components explored here more clearly. For this reason the AC recommends an invitation to the workshop track."
iclr_2018_rk4Fz2e0b,"This paper was perceived as being well written, but the technical contribution was seen as being incremental and somewhat heuristic in nature. Some important prior work was not discussed and more extensive experimentation was recommended.
However, the proposed approach of partitioning the graph into sub graphs and a schedule alternating between intra and inter graph partitions operations has some merit.

The AC recommends inviting this paper to the Workshop Track."
iclr_2018_HymuJz-A-,"This paper studies an important problem (visual relationship detection and generalization capabilities existing networks for this task). Unfortunately, all reviewers raise concerns (e.g. limited relations studied) and are largely on the fence about this paper. While this paper does not propose solutions, it does present interesting ""negative results"" that should get some visibility in the workshop track. "
iclr_2018_SyunbfbAb,"This paper was reviewed by 3 expert reviews. While they all see value in the new task and dataset, they raise concerns (templated language, unclear what exactly are the new challenges posed by this task and dataset, etc) that this AC agrees with. To be clear, the lack of a fundamentally new model is not a problem (or a requirement for every paper introducing a new task/dataset), but make a clear compelling case for why people should work on the task is a reasonable bar. We encourage the authors to incorporate reviewer feedback and invite to the workshop track. "
iclr_2018_SylJ1D1C-,"This paper studies the approximation and integration of partial differential equations using convolutional neural networks. By constraining CNN filters to have prescribed vanishing moments, the authors interpret CNN-based temporal prediction in terms of 'pde discovery'. The method is demonstrated on simple convection-diffusion simulations.

Reviewers were mixed in assessing the quality, novelty and significance of this work. While they all acknowledged the importance of future research in this area, they raised concerns about clarity of exposition (which has been improved during the rebuttal period), as well as the novelty / motivation. The AC shares these concerns; in particular, he misses a more thorough analysis of stability (under what conditions would one use this method to estimate an actual PDE and obtain some certificate of approximation?) and discussions about pitfalls (in real situations one may not know in advance the family of differential operators involved in the physical process nor the nature of the non-linearity; does the method produce a faithful approximation? why?).

Overall, the AC thinks this is an interesting submission that is still in its preliminary stage, and therefore recommends resubmitting to the worshop track at this time."
iclr_2018_S1TgE7WR-,"This is a good contribution, with the potential to become extremely good and significant if presentation is substantially improved.
All reviewers comment on the lack of clarity of the paper, especially concerning its central contributions (Section 4 and 5), as illustrated also by the relatively low confidence scores.
Reviewers also mention the current imbalance between the generality of high-order compositional networks and the motivation and empirical evaluation of these models. Generalizations of graph neural representations based on higher order local interactions are particularly interesting in contexts such as combinatorial optimization, where heuristics typically exploit high-order interactions.

In summary, we believe this work deserves a further iteration before it can be in proceedings in order to improve the exposition and the motivation of compositional networks, that will greatly improve its exposure to the community.  That said, the idea it lays forward is of potential interest, and thus the AC recommends resubmission to the workshop track. "
iclr_2018_SyUkxxZ0b,"This paper studies the interplay between adversarial examples and generalization in the uniform setting (not specific assumptions on the architecture) in a toy high-dimensional setting. In particular, the authors show a fundamental tradeoff between generalization error and the average distance of adversarial examples.

Reviewers were skeptical about the possible significance of this work, but the paper underwent a major revision that greatly improved the quality of presentation. That said, the results are still preliminary since they only consider a toy dataset (concentric spheres). The AC recommends re-submitting this work to the workshop track."
iclr_2018_ryZ8sz-Ab,"this is an interesting approach that applies the idea of dynamically controlling the amount of information from the input fed into the classifier (some of the earlier approaches have used this idea for, e.g., parsing, real-time translation, online speech recognition, and so on...) this is also related to some of the recent work on hierarchical recurrent nets [Chung et al.]. unfortunately, two of the reviewers and other commenters found this manuscript needs more work to clarify motivation, implication and relationship to other existing works, with which i don't necessarily disagree. "
iclr_2018_B1KJJf-R-,"the reviewers all found the problem to be important, the proposed approach to be interesting, but the manuscript to be preliminary. i agree with them."
iclr_2018_Sy3XxCx0Z,"the reviewers seem to agree that this submission could be much more strengthened if more investigation is done in two directions: (1) the effect of different, available resources (e.g., in the comment, the authors mentioned WikiData didn't improve, and this raises a question of what kind of properties of external resources are necessary to help) and (2) alternatives to incorporating external knowledge (e.g., as pointed out by one of the reviewers, this is certainly not the only way to do so, and external knowledge has been used by other approaches for RTE earlier. how does this specific way fare against those or other alternatives?) addressing these two points more carefully and thoroughly would make this paper much more appreciated."
iclr_2018_Sk4w0A0Tb,"although the authors argue that their experiments were selected from the earlier work from which major comparing approaches were taken, the reviewers found the empirical result to be weak. why not some real tasks (i do not believe bAbI nor PTB could be considered real) that could clearly reveal the superiority of the proposed unit against existing ones?"
iclr_2018_Byd-EfWCb,"this submission has two results; (1) it defines what it means for the optimal representation is, although this is rather uninteresting that it simply says that if the representation from a model is going to be used based on some given metric, the cost function should directly reflect it, and (2) it shows that different choices of encoding and decoding have different implications. as with most of the reviewers, i found these to be a rather weak contribution."
iclr_2018_rkxY-sl0W,"the problem is interesting, and the approach is also interesting. however, the reviewers have found that this manuscript would benefit from more experiments, potentially involving some real data (even at least for evaluation) in addition to largely synthetic data sets used in the submission. i also agree with them and encourage authors to consider this option."
iclr_2018_BkoXnkWAb,"the reviewers were not fully convinced of the setting under which the proposed bipolar activation function was found by the authors to be preferable, and neither am i."
iclr_2018_By3v9k-RZ,"i am a big fan of this idea, but i agree with the reviewers that evaluating this idea on bAbI (which was originally created from a small set of rules and primitives) discounts quite a bit of what is being claimed here. one of the future directions mentioned by the authors (""investigating whether the proposed n-gram representation is sufficient for natural languages"") should have been included even with a negative result, which would've increased the significance significantly."
iclr_2018_HkcTe-bR-,"The paper creates a dataset for exploration of RL for molecular design and I think this makes it a strong contribution to the community at the intersection of the two. For a methods focussed conference such as ICLR however, it may not be the best fit. Hence I would recommend submitting to a workshop track or targeting a more focussed venue such as a bioinformatics conference. "
iclr_2018_SkBYYyZRZ,"The author's propose to use swish and show that it performs significantly better than Relus on sota vision models. Reviewers and anonymous ones counter that PRelus should be doing quite well too. Unfortunately, the paper falls in the category where it is hard to prove the utility of the method through one paper alone, and broader consensus relies on reproduction by the community. As a results, I'm going to recommend publishing to a workshop for now."
iclr_2018_ByQZjx-0-,"First off, this was a difficult paper to decide on. There was some vigorous discussion on the paper centering around the choices that were available to the conv-nets.  The author's strongly emphasized the improvements on the PTB task.

For my part, I think the method is very compelling -- sharing weights for all the models we are optimizing on seems like a great idea -- and that we can make it work is even more interesting. So from this point of view, I think its a novel contribution and worth accepting.

On the other hand, I'm likely to agree with some of the motivations behind the questions raised by R3. Are all the choices really necessary ? perhaps the gains came from just a couple of things like number of skip connections and channels, etc. That exploration is useful. On the flip side, I think it may be an irrelevant question -- the model is able to make the correct decisions from a big set.

The authors emphasize the language modelling part, but for me, this was actually less compelling. The authors use some of the tricks from Merity in their model training (perplexity 52.8), and as a result are already using some techniques that produces better results. Further, PTB is a regularization game -- and that's not really the claim of this paper. Although, one could argue that weight sharing between different models can produce an ensembling / regularization effect and those gains may show up on PTB. A much more compelling story would have been to show that this method works on a large dataset where the impact of the architecture cannot be conflated with controlling overfitting better.

As a result, this puts the paper on the fence for me; even though I very much like the idea. Polishing the paper and making a more convincing case for both the CNNs and RNNs will make this paper a solid contribution in the future."
iclr_2018_r1pW0WZAW,"I think the model itself is not very novel, as pointed by the reviewers and the analysis is not very insightful either. However, the results themselves are interesting and quite good (on the copy task and pMnist, but not so much the other datasets presented (timit etc) where it not clear that long term dependencies would lead to better results). Since the method itself is not very novel, the onus is upon the authors to make a strong case for the merits of the paper --  It would be worth exploring these architectures further to see if there are useful elements for real world tasks -- more so than is demonstrated in the paper --  for example showing it on tasks such as machine translation or language modelling tasks requiring long term propagation of information or even real speech recognition, not just basic TIMIT phone frame classification rate.

As a result, while I think the paper could make for an interesting contribution, in its present form, I have settled on recommending the paper for the workshop track.


As a side note, paper is related to paper 874 in that an attention model is used to look at the past. The difference is in how the past is connected to the current model. "
iclr_2018_S1lN69AT-,"The authors present a thorough exploration of large-sparse models that are pruned down to a target size and show that these models can perform better than small dense models. Results are shown on a variety of datasets with as conv models and seq2seq. The authors even go so far as to release the code. I think the authors are to be thanked for their experimental contributions.
However, in terms of accepting the paper for a premier machine learning conference the method holds little surprise or non-obviousness. I think the paper is a good experimental contribution, and would make a good workshop paper instead but it offers little contribution by way of machine learning methods.
"
iclr_2018_SkRsFSRpb,The reviewers found the paper meaningful but noted that they were not convinced by the experiments as they stand and the presentation was dense for them.
iclr_2018_ryALZdAT-," + An intriguing novel regularization method: encouraging larger norms for the feature vector input to the last softmax layer of a classifier.
 + Resonably extensive experimental validation shows that it improves test accuracy to some degree.
 - While a motivation is given, the formal analysis of what is really going on remains very superficial and limited.

Technical note: Simply scaling the softmax layer's input would not change class rankings, so any positive effect of this regularizer on classification performance is due to it changing the learning dynamic in the upper layers as well. The paper could be much stronger if it did provide an analysis regarding how the global learning dynamic is affected in all layers, by the interaction between weight decay and the last layer's feature incay.
"
iclr_2018_HkmaTz-0W,"This work proposes an improved visualisation techniques for ReLU networks that compensates for filter scale symmetries/invariances, thus allowing a more meaningful comparison of low-dimensional projected optimization landscapes between different network architectures.

- the visualisation techniques are a small variation over previous works
+ extensive experiments provide nice visualisations and yield a clearer visual picture of some properties of the optimization landscape of various architectural variants

A promising research direction, which could be further improved by providing more extensive and convincing support for the significance of its contribution in comparison to prior techniques, and to its empirically derived observations, findings and claims.
"
iclr_2018_SJTB5GZCb," + interesting novel extension of equilibrium propagation, as a biologically more plausible alternative to  backpropagation, with encouraging initial experimental validation.
 - currently lacks theoretical guarantees regarding convergence of the algorithm to a meaningful result
 - experimental study should be more extensive to support the claims"
iclr_2018_SkmiegW0b,"The paper proposes a method to disentangle style from content (two factor disentanglement) using weak labels (information about the common factor for a pair of images). It is similar to an earlier work by Mathieu et al (2016) with main novelty being in the use of the discriminator which operates with pairs of images in the proposed method. Authors also have some theoretical statements about two challenges in disentangling the factors but reviewers have complained about missing connection b/w theory and experiments, and about exposition in general.

The idea has novelty, although somewhat limited in the light of earlier work by Mathieu et al (2016)), and theoretical statements are also of interest but reviewers still feel the paper needs improvement in writing and presentation of results. I would recommend an invitation to the workshop track. "
iclr_2018_HkpYwMZRb,"The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance. Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published. There are also some concerns on networks used in the experiments not being close to practice.

I recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion. "
iclr_2018_Syr8Qc1CW,"The method proposed in the paper for latent disentanglement and attribute-conditional image generation is novel to the best of my understanding but reviewers (Anon1 and Anon3) have expressed concerns on the quality of results (CelebA images) as well as on the technical presentation and claims in the paper.

Given the novelty of the proposed method, I would *not* like to recommend a ""reject"" for this paper but the concerns raised by the reviewers on the quality of results and lack of quantitative results seem valid. Authors rule out possibility of any quantitative results in their response but I am not fully convinced -- in particular, effectiveness of attribute-conditional image generation can be captured by first training an attribute classifier on the generated images and then measuring how often the predicted attributes are flipped when conditioning signal is changed. There are also other metrics in the literature for evaluating generative models.

I would recommend inviting it to the workshop track, given that the work is novel and interesting but has scope for improvements.
"
iclr_2018_ryDNZZZAW,"Pros
-- Lays out bounds for multi-domain adaptation based on earlier work on a single source-target domain pair.
-- Shows gains over choosing the best source domain for a target domain, or naively combining domains.

Cons
-- The reviewers agree that the extensions are relatively straightforward extensions to single source-target pair.
-- Hard-max doesn’t consider the partial contribution of multiple source domains, and considers the worst-case scenario.
-- Soft-max addresses some of these issues; the authors provide reasonable justification for the algorithm but it’s not clear that the specific choice of \alphas leads to the tightest bound.

The reviewers noted that the authors significantly improved the paper during the revision process. The AC feels that the presented techniques would be of interest to the community and would help lead discussions towards theoretically optimal ways to do domain adaptation given multiple domains. The authors are therefore encouraged to submit to the workshop track.
"
iclr_2018_H1I3M7Z0b,"The paper received generally positive reviews, but the reviewers also had some concerns about the evaluations.

Pros:
-- An improvement over HashNet, the model ties weights more systematically, and gets better accuracy.
Cons:
-- Tying weights to compress models already tried before.
-- Tasks are all small and/or audio related.
-- Unclear how well the results will generalize for 2D convolutions.
-- HashNet results are preliminary; comparisons with HashNet missing for audio tasks.

Given the expert reviews, I am recommending the paper to the workshop track.
"
iclr_2018_rybAWfx0b,"Pros
-- A novel way to incorporate LM into an end-to-end model, with good adaptation results.

Cons
-- Lacks results on public corpora or results are not close to SOTA (e.g., for Librispeech).

Given the reviews, it is clear that the experimental evaluations can be improved. But the presented approach is novel and interesting. Therefore I am recommending the paper to the workshop track."
iclr_2018_S1pWFzbAW,"Pros:
-- Use of Bloomier filters for lossy compression of nets is novel and well motivated, with interesting compression performance.
Cons:
-- Does lossy compression for transmission, doesn’t address FLOPS required for runtime execution. A lot of times, client devices do not have enough cpu to run large networks (title should be udpated to mean compression and transmission)
-- Missing results for full network, larger deeper network.

Overall, the content is novel and interesting, so I would encourage the authors to submit to the workshop track.
"
iclr_2018_S1Auv-WRZ,"The paper based on cGAN developed a data augmentation GAN to deal with unseen classes of data. The paper developed new modifications to each component and designed network structure using ideas from state-of-the-art nets. As pointed out by reviewer 1 & 2, the technical contribution is not sufficient. We hence recommend it to workshop publication."
iclr_2018_rJrTwxbCb,"Pros:
+ Builds in important ways on the work of Sagun et al., 2016.

Cons:
- The reviewers were very concerned that the assumption in the paper that the second term of Equation (6) is negligible was insufficiently supported, and this concern remained after the discussion and the revision.
- The paper needs to be more precise in its language about the Hessian, particularly in distinguishing between ill conditioning and degeneracy.
- The reviewers did not find the experiment very convincing because it relied on initializing the small-batch optimization from the end point of the large-batch optimization.  Again, this concern remained following the discussion and revision.

The area chair agrees with the authors' comments in their OpenReview post of 08 Jan. 2018 ""A remark on relative evaluation,"" and has discounted the reviewers' comments about the relative novelty of the work.  It is important not to penalize authors for submitting their papers to conferences with an open review process, especially when that process is still being refined.

However, even discounting the remarks about novelty, there are key issues in the paper that need to be addressed to strengthen it (the 3 ""cons"" above), so this paper does not quite meet the threshold for ICLR Conference acceptance.

However, because it raises really interesting questions and is likely to provoke useful discussions in the community, it might be a good workshop track paper.
"
iclr_2018_B1Z3W-b0W,"This paper is intersting but has a few flaws that still need to be addressed. As one reviewer noted, ""the authors seems to have simply applied the method of Andrychowicz et al. If they added some discussion and experiments clearly showing why this is a better way to improve the existing inference methods, the paper might have more impact."".
Overall, this work builds on existing work, but does not really dig deep enough for answers to these questions raised by the reviewers. The committee still feels this paper will be of great value at ICLR and recommends it for a workshop paper.
"
iclr_2018_HkCnm-bAb,"The paper introduces an interesting family of two-player zero-sum games with tunable complexity, called Erdos-Selfridge-Spencer games, as a new domain for RL.  The authors report on extensive empirical results using a wide variety of training methods, including supervised learning and several flavors of RL (PPO, A2C, DQN) as well as single-agent vs. multi-agent training.  The reviewers also appear to agree that the method appears to be technically correct, clearly written, and easy to read.

A drawback of the paper is that it does not make a *significant* contribution to the field.  In combing through the reviewer comments, none of them identify a significant contribution.  Even in the text of the paper, the authors do not anywhere claim to have made a significant contribution. As the paper is still interesting, the committee would like to recommend this for the workshop track.

Pros:
        Interesting domain with tunable complexity
        High-quality extensive empirical results
        Writing is clear

Cons:
        Lacks a significant contribution
        Appears to overlook self-play, the dominant RL training paradigm for decades (multiagent training appears to be related but different)
        Per Reviewer3, ""I remain unconvinced that these games are good general tests for Deep RL"""
iclr_2018_Bya8fGWAZ,"This paper and reviews makes for a difficult call.  The reviewers appear to be in agreement that Value Propagation provides an interesting algorithmic advance over earlier work on Value Iteration networks.  AnonReviewer1 gives a strong rationale why the advance is both original and significant.  Their experiments also show very nice results with VProp and MVProp in 2-D grid-worlds.

However, I also fully agree with AnonReviewer2 that testing in other domains beyond 2-D grid-world is necessary.  Earlier work on VIN was also tested on a Mars Rover / continuous control domain, as well as graph-based web navigation task.  The authors' rebuttal on this point comes across as weak.  In their view, they can't tackle real-world domains until VProp has been proven effective in large, complex grid-worlds.  I don't buy this at all -- they could start initial experiments right away, which would perhaps yield some surprising results. Given this analysis, the committee recomments this paper for workshop.

Pros: significant algorithmic advance, good technical quality and writeup, nice results in 2-D grid world.

Con: Validation is only in 2-D grid-world domains. "
iclr_2018_BkfEzz-0-,"
The reviewers have significantly different views, with one strongly negative,
one strongly positive, and one borderline negative.  However, all three
reviews seem to regard the NaaA framework as a very interesting and novel approach to training neural nets.  They also concur that the major issue with the paper is very confusing technical exposition regarding the motivation, math details, and how the idea works.  The authors indicate that they have significantly revised the manuscript to improve the exposition, but none of the reviewers have changed their scores.  One reviewer states that ""technical details are still too heavy to easily follow.""  My own take regarding the current section 3 is that it is still very challenging to parse and follow. Given this analysis, the committee recommends this for workshop.

Pros:
        Interesting and novel framework for training NNs
        ""Adaptive DropConnect"" algorithm contribution
        Good empirical results in image recognition and ViZDoom domains

Cons:
        Technical exposition is very challenging to parse and follow
        Some author rebuttals do not inspire confidence.  For example,
motivation of method due to ""$100 billion market cap of Bitcoin"" and in reply to unconvincing neuroscience motivation, saying ""throw away the typical image of auction."""
iclr_2018_Hk91SGWR-,"This paper turned out to be quite difficult to call.  My take on the pros/cons is:

1. The research topic, how and why humans can massively outperform DQN, is unanimously viewed as highly interesting by all participants.

2. The authors present an original human subject study, aiming to reveal whether human outperformance is due to human knowledge priors.  The study is well conceived and well executed.  I consider the study to be a contribution by itself.

3. The study provides prima facie evidence that human priors play a role in human performance, by changing the visual display so that the priors cannot be used.

4. However, the study is not definitive, as astutely argued by AnonReviewer2.  Experiments using RL agents (with presumably no human priors) yield behavior that is similar to human behavior.  So it is possible that some factor other than human prior may account for the behavior seen in the human experiments.

5. It would indeed be better, as argued by AnonReviewer2, to use some information-theoretic measure to distinguish the normal game from the modified games.

6. The paper has been substantially improved and cleaned up from the original version.

7. AnonReviewer1 provided some thoughtful detailed discussion of how the authors may be overstating the conclusions that one can draw from the paper.

Bottom line: Given the procs and cons of the paper, the committee recommends this for workshop.
"
iclr_2018_rJk51gJRb,"The reviewers agree that the paper is below threshold for acceptance in the main track (one with very low confidence), but they favor submitting the paper to the workshop track.

The paper considers policy gradient methods for two-player zero-sum Alternating Markov games.  They propose adversarial policy gradient (fairly obviously), wherein the critic estimates min rather than mean reward.   They also report promising empirical results in the game of Hex, with varying board sizes.  I found the paper to be well-written and easy to read, possibly due to revisions in the rebuttal discussions.

The reviewers consider the contribution to be small, mainly due to the fact that the key algorithmic insights were already published decades ago.  Reintroducing them is a service to the community, but its novelty is limited.  Other critiques mentioned that results in Hex only provide limited understanding of the algorithm's behavior in general Alternating Markov games.  The lack of comparison with modern methods like AlphaGo Zero was also mentioned as a limitation.

Bottom line: The paper provides a small but useful contribution to the community, as described above, and the committee recommends it for workshop.
"
iclr_2018_BJInEZsTb,"This paper compares autoencoder and GAN-based methods for 3D point cloud representation and generation, as well as new (and welcome) metrics for quantitatively evaluating generative models.  The experiments form a good but still a bit too incomplete exploration of this topic.  More analysis is needed to calibrate the new metrics.  Qualitative analysis would be very helpful here to complement and calibrate the quantitative ones.  The writing also needs improvement for clarity and verbosity.  The author replies and revisions are very helpful, but there is still some way to go on the issues above. Overall, the committee is intersting and recommends this paper for the workshop track."
iclr_2018_BJubPWZRW,"This paper combines ideas from student-teacher training and multi-view learning in a simple but clever way.  There is not much novelty in the methods, but promising results are given across several tasks, including realistic NLP tasks.  The improvements are not huge but are consistent.  Considering the limited novelty, the paper should include some more convincing analysis and insight on why/when the approach works. Given the intersting results, the committee recommends this for workshop track."
iclr_2018_H1DkN7ZCZ,"Authors present a method for representing DNA sequence reads as one-hot encoded vectors, with genomic context (expected original human sequence), read sequence, and CIGAR string (match operation encoding) concatenated as a single input into the framework. Method is developed on 5 lung cancer patients and 4 melanoma patients.


Pros:
- The approach to feature encoding and network construction for task seems new.
- The target task is important and may carry significant benefit for healthcare and disease screening.

Cons:
- The number of patients involved in the study is exceedingly small. Though many samples were drawn from these patients, pattern discovery may not be generalizable across larger populations. Though the difficulty in acquiring this type of data is noted.
- (Significant) Reviewer asked for use of public benchmark dataset, for which authors have declined to use since the benchmark was not targeted toward task of ultra-low VAFs. However, perhaps authors could have sourced genetic data from these recommended public repositories to create synthetic scenarios, which would enable the broader research community to directly compare against the methods presented here. The use of only private datasets is concerning regarding the future impact of this work.
- (Significant) The concatenation of the rows is slightly confusing. It is unclear why these were concatenated along the column dimension, rather than being input as multiple channels. This question doesn't seem to be addressed in the paper.
Given the pros and cons, the commitee recommends this interesting paper for workshop."
iclr_2018_ByaQIGg0-,"Differentiable neural networks used as a measure of design optimality in order to improve efficiency of automated design.


Pros:
- Genetic algorithms, which are the dominant optimization routine for automated design systems, can be computationally expensive. This approach alleviates this bottleneck under certain circumstances and applications.

Cons:
- Primarily application paper, machine learning advancement is marginal.
- Multiple reviewers: Generalization capability not clear. For example, some utility systems may be stochastic (i.e. turbulence) and require multiple trials to measure fitness, which this method would not be able to model.
Overall, the committee feels this paper is interesting enough to appears as a workshop paper."
iclr_2018_HyDMX0l0Z,"The paper presents a really interesting take on the mode collapse problem and argue that the issue arises because of the current GAN models try to model distributions with disconnected support using continuous noise and generators. The authors try to fix this issue by training multiple generators with shared parameters except for the last layer.

The paper is well written and authors did a good job in addressing some of the reviewer concerns and improving the paper.

Even though arguments presented are novel and interesting, reviewers agree that the paper lacks sufficient theoretical or experimental analysis to substantiate the claims/arguments made in the paper. Limited quantitative and subjective results are not always in favor of the proposed algorithm. More controlled toy experiments and results on larger datasets are needed. The central argument about ""tunneling"" is interesting and needs deeper investigation. Overall the committee recommends this paper for workshop."
iclr_2018_rkEtzzWAb,"Pros:
 - The paper proposes interesting new ideas on evaluating generative models.
 - Paper provides hints at interesting links between structural prediction and adversarial learning.
 - Authors propose a new dataset called Thin-8 to demonstrate the new ideas and argue that it is useful in general to study generative models.
 - The paper is well written and the authors have made a good attempt to update the paper after reviewer comments.

Cons:
- The proposed ideas are high level and the paper lack deeper analysis.
- Apart from demonstrating that the parametric divergences perform better than non-parametric divergences are interesting, but the reviewers think that practical importance of the results are weak in comparison to previous works.
With this analysis, the committee recommends this paper for workshop."
iclr_2018_r1YUtYx0-,The paper proposes a new way to understand why neural networks generalize well. They introduce the concept of ensemble robustness and try to explain DNN generalization based on this concept. The reviewers feel the paper is a bit premature for publication in a top conference although this new way of explaining generalization is quite interesting.
iclr_2018_SyKoKWbC-,"All the reviewers and myself have concerns about the potentially incremental nature of this work. While I do understand that the proposed method goes beyond crafting minibatch losses, and instead parametrizes things via a neural network, ultimately it's roughly very similar to simply combining MMD and minibatch discrimination and ""learning  the kernel"". The theoretical justifications are interesting, but the results are somewhat underwhelming (as an example, DANN's are by no means the state of the art on MNIST->MNIST_M, and this task is rather contrived; the books dataset is not even clearly used by anyone else).

The interesting analysis may make it a good candidate for the workshop track, so I am recommending that."
iclr_2018_BkM3ibZRW,"In general, the reviewers and myself find this work of some interest, though potentially somewhat incremental in terms of technical novelty compared to the work for Makhzani et al. Another bothersome aspect is the question of evaluation and understanding how well the model actually does; I am not convinced that the interpolation experiments are actually giving us a lot of insights. One interesting ablation experiment (suggested privately by one of the reviewers) would be to try AAE with Wasserstein and without a learned generator -- this would disambiguate which aspects of the proposed method bring most of the benefit. As it stands, the submission is just shy of the acceptance bar, but due to its interesting results in the natural language domain, I do recommend it being presented at the workshop track."
iclr_2018_Skk3Jm96W,"Overall, the paper is missing a couple of ingredients that would put it over the bar for acceptance:

- I am mystified by statements such as ""RL2 no longer gets the best final performance."" from one revision to another, as I have lower confidence in the results now.

- More importantly, the paper is missing comparisons of the proposed methods on *already existing* benchmarks. I agree with Reviewer 1 that a paper that only compares on benchmarks introduced in the very same submission is not as strong as it could be.

In general, the idea seems interesting and compelling enough (at least on the Krazy World & maze environments) that I can recommend inviting to the workshop track."
iclr_2018_BkA7gfZAb,"All the reviewers noted that the dual formulation, as presented, only applies to the logistic family of classifiers. The kernelization is of course something that *can* be done, as argued by the authors, but is not in fact approached in the submission, only in the rebuttal. The toy-ish nature of the problems tackled in the submission limits the value of the presentation.

If the authors incorporate their domain adaptation results (SVHN-->MNIST and others) using the kernelized approach and do the stability analysis for those cases, and obtain reasonable results on domain adaptation benchmarks (70% on SVHN-->MNIST is for instance on the low side compared to the pixel-transfer-based GAN approaches out there!) then I think it'd be a great paper.

As such, I can only recommend it as an invitation to the workshop track, as the dual formulation is interesting and potentially useful."
iclr_2018_H18WqugAb,"Reviewers were somewhat lukewarm about this paper, which seeks to present an analysis of the limitations of sequence models when it comes to understanding compositionality. Somewhat synthetic experiments show that such models generalise poorly on patterns not attested during training, even if the information required to interpret such patterns is present in the training data when combined with knowledge of the compositional structure of the language. This conclusion seems as unsurprising to me as it does to some of the reviewers, so I would be inclined to agree with the moderate enthusiasm two out of three reviewers have for the paper, and suggest that it be redirected to the workshop track.

Other criticisms found in the review have to do with the lack of any discussion on the topic of how to address these limitations, or what message to take home from these empirical observations. It would be good for the authors to consider how to evaluate their claims against ""real"" data, to avoid the accusation that the conclusion is trivial from the task set up.

Therefore, while well written, it is not clear that the paper is ready for the main conference. It could potentially generate interesting discussion, so I am happy for it to be invited to the workshop track, or failing that, to suggest that further work on this topic be done before the paper is accepted somewhere."
iclr_2018_HJ3d2Ax0-,"This paper attempts a theoretical treatment of the influence of depth in RNNs on their ability to capture dependencies in the data. All reviewers found the theoretical contribution of the paper interesting, and while there were problems raised regarding formalisation, they appear to have been adequately addressed in the revisions to the paper. The main concern in all three reviews surrounds the evaluation, and weakness thereof. The overarching point of contention seems to be that the theory relates to a particular formulation of RNNs (RAC), causing doubts that the results lift to other architectural variants which are used to obtain state-of-the-art results on tasks such as language modelling. It seems that the paper could be significantly improved by the provision of stronger empirical results to support the theory, or a more convincing argument as to why the results should transfer from, say, RAC to LSTMs. The authors point to two papers on the matter in their response, but it is not clear this is a substitute for experimental validation. I find the paper a bit borderline because of this, and recommend redirection to the workshop."
iclr_2018_ryG6xZ-RZ,"This is a fascinating paper, and representative of the sort of work which is welcome in our field and in our community. It presents a compiler framework for the development of DSLs (and models) for Deep Learning and related methods. Overall, reviewers were supportive of and excited by this line of work, but questioned its suitability for the main conference. In particular, the lack of experimental demonstrations of the system, and the disconnect between domain-specific technical knowledge required to appreciate this work and that of the average ICLR attendee were some of the main causes for concern. It is clear to me that this paper is not suitable for the main conference, not due to its quality, but due to its subject matter. I would be happy, however, to tentatively recommend it for acceptance to the workshop as this topic deserves discussion at the conference, and this would provide the basis for a useful bridge between the compilers community and the deep learning community."
iclr_2018_HkGJUXb0-,"This paper proposes a new way of learning tensors representation with ring-structured decompositions rather than through Tensor Train methods. The paper investigates the mathematical properties of this decomposition and provides synthetic experiments. There was some debate, with the reviewers, about the novelty and impact of this method, where overall the feeling was this work was too preliminary to be accepted. The idea, from my understanding, is interesting and would benefit from discussion at the workshop track, but the authors are investigated to make a stronger case for the novelty of this method in any further work and, in particular, to consider showing empirical improvement on ""real"" data where TT methods are currently applied."
iclr_2018_BJypUGZ0Z,"The paper proposes to use simple regression models for predicting the accuracy of a neural network based on its initial training curve, architecture, and hyper-parameters; this can be used for speeding up architecture search. While this is an interesting direction and the presented experiments look quite encouraging, the paper would benefit from more evaluation, as suggested by reviewers, especially within state-of-the-art architecture search frameworks and/or large datasets."
iclr_2018_SkOb1Fl0Z,"The paper presents a domain-specific language for RNN architecture search, which can be used in combination with learned ranking function or RL-based search. While the approach is interesting and novel, the paper would benefit from an improved evaluation, as pointed out by reviewers. For example, the paper currently evaluated coreDSL+ranking for language modelling and extendedDSL+RL for machine translation. The authors should use the same evaluation protocol on all tasks, and also compare with the state-of-the-art MT approaches."
iclr_2018_SySaJ0xCZ,"The paper proposes a method for architecture search using network morphisms, which allows for faster search without retraining candidate models. The results on CIFAR are worse than the state of the art, but reasonably competitive, and achieved using limited computation resources. It would have been interesting to see how the method would perform on large datasets (ImageNet) and/or other tasks and search spaces. I would encourage the authors to extend the paper with further experimental evaluation."
iclr_2018_BkCV_W-AZ,"The reviewers agree this is a really interesting paper, with an interesting idea (in particular
the use of regret clipping might provide a benefit over typical policy gradient methods). However,
there are two major concerns: 1) clarity / exposition and more importantly 2) lack of a strong
empirical motivation for the new approach (why do standard methods work just as well on these
partially observable domains?)."
iclr_2018_rkEfPeZRb,"The reviewers find the gradient compression approach novel and interesting, but they find the empirical evaluation not fully satisfactory. Some aspects of the paper have improved with the feedback from the reviewers, but because of the domain of the paper, experimental evaluation is very important. I recommend improving the experiments by incorporating the reviewers' comments."
iclr_2018_SyVOjfbRb,"The reviewers think that the theoretical contribution is not significant on its own. The reviewers find the empirical aspect of the paper interesting, but more analysis of the empirical behavior is required, especially for large datasets. Even for small datasets with input augmentation (e.g. random crops in CIFAR-10) the pre-processing can become prohibitive. I recommend improving the manuscript for a re-submission to another venue and an ICLR workshop presentation."
iclr_2018_BJjquybCW,"Dear authors,

While I appreciate the result that a convolutional layer can have full rank output, this allowing a dataset to be classified perfectly under mild conditions, the fact that all reviewers expressed concern about the statement is an indication that the presentation sill needs quite a bit of work.

Thus, I recommend it as an ICLR workshop paper."
iclr_2018_SyfiiMZA-,"The chief contribution of this paper is to show that a single set of policy parameters can be optimized in an alternating fashion while the design parameters of the body are also optimized with policy gradients and sampled. The fact that this simple approach seems to work is interesting and worthy of note. However, the paper is otherwise quite limited - other methods are not considered or compared, incomplete experimental results are given, and important limitations of the method are not addressed. As it is an interesting but preliminary work, the workshop track would be appropriate."
iclr_2018_SJZ2Mf-0-,"This paper presents an interesting model which at the time of submission was still quite confusingly described to the reviewers.
A lot of improvements have been made for which I applaud the authors.
However, at this point, the original 20 babi tasks are not quite that exciting and several other models are able to fully solve them as well.
I would encourage the authors to tackle harder datasets that require reasoning or multitask settings that expand beyond babi.

"
iclr_2018_SJyfrl-0b,"The authors addressed the reviewers concerns but the scores remain somewhat low.
The method is not super novel, but it is an incremental improvement over existing approaches."
iclr_2018_S1LXVnxRb,"We encourage the authors to improve the mentioned aspects of their work in the reviews.
"
iclr_2018_rkaT3zWCZ,"The authors present an environment for semantic navigation that is based on an existing dataset, SUNCG. Datasets/environments are important for deep RL research, and the contribution of this paper is welcome. However, this paper does not offer enough novelty in terms of approach/method and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop. "
iclr_2018_HyXNCZbCZ,"Pros:
- The paper proposes to use a hierarchical structure to address reconstruction issues with ALI model.
- Obtaining multiple latent representations that individually achieve a different level of reconstructions is interesting.  
- Paper is well written and the authors made a reasonable attempt to improve the paper during the rebuttal period. 

Cons:
- Reviewers agree that the approach lacks novelty as similar hierarchical approaches have been proposed before. 
- The main goal of the paper to achieve better reconstruction in comparison to ALI without changing the latter's objective seems narrow. More analysis is needed to demonstrate that the approach out-performs other approaches that directly tackle this problem in ALI.
- The paper does not provide strong arguments as to why hierarchy works (limited to 2 levels in the empirical analysis presented in the paper). 
- Semi-supervised learning as a down-stream task is impressive but limited to MNSIT.  "
iclr_2018_B1CNpYg0-,"The pros and cons of the paper can be summarized as follows:

Pros:
* The method of combining together multiple information sources is effective
* Experimental evaluation is thorough

Cons:
* The method is a relatively minor contribution, combining together multiple existing methods to improve word embeddings. This also necessitates the model being at least as complicated as all the constituent models, which might be a barrier to practical applicability

As an auxiliary comment, the title and emphasis on computing embeddings ""on the fly"" is a bit puzzling. This is certainly not the first paper that is able to calculate word embeddings for unknown words (e.g. all the cited work on character-based or dictionary-based methods can do so as well). If the emphasis is calculating word embeddings just-in-time instead of ahead-of-time, then I would also expect an evaluation of the speed or memory requirements benefits of doing so. Perhaps a better title for the paper would be ""integrating multiple information sources in training of word embeddings"", or perhaps a more sexy paraphrase of the same.

Overall, the method seems to be solid, but the paper was pushed out by other submissions.
"
iclr_2018_SJvu-GW0b,The reviewers agree that the problem being studied is important and relevant but express serious concerns. I recommend the authors to carefully go through the reviews and significantly scale up their experiments.
iclr_2018_rJv4XWZA-,"This paper presents an interesting idea: employ GANs in a manner that guarantees the generation of differentially private data.

The reviewers liked the motivation but identified various issues. Also, the authors themselves discovered some problems in their formulation; on behalf of the community, thanks for letting the readers know.

The discovered issues will need to be reviewed in a future submission."
iclr_2018_Sk7cHb-C-,"This paper proposes a model which learns simultaneously the dynamics of sequential data, together with a static latent representation. The idea and motivation is interesting and the results are promising.

However, all reviewers agree that the presentation needs much more work to convey the messages correctly and convincingly. Moreover, the reviewers question some design choices and lack of discussion of the results. No rebuttal has been provided.

"
iclr_2018_BkS3fnl0W,"This paper presents a framework where GANs are used to improve detection of outliers (in this context, instances of the “background class”). This is a very interesting and, as demonstrated, promising idea. However, the general feeling of the reviewers is that more work is needed to make the technical and evaluations parts convincing. Suggestions for further work towards this direction include: theoretical analysis, better presentation of the manuscript and, most importantly, stronger experimental section. "
iclr_2018_HkinqfbAb,"This paper presents yet another scheme for weight tying for compressing neural networks, which looks a lot like a less Bayesian version of recent related work, and gets good empirical results on realistic problems.

This paper is well-executed and is a good contribution, but falls below the bar on
1) Discovering something new and surprising, except that this particular method (which is nice and simple and sensible) works well.  That is, it doesn't advance the conversation or open up new directions.
2) Potential impact (although it might be industrially relevant)

Also, the title is a bit overly broad given the amount of similar existing work."
iclr_2018_H15RufWAW,"This paper proposes an implicit model of graphs, trained adversarially using the Gumbel-softmax trick.  The main idea of feeding random walks to the discriminator is interesting and novel.  However,
1) The task of generating 'sibling graphs', for some sort of bootstrap analysis, isn't well-motivated.
2) The method is complicated and presumably hard to tune, with two separate early-stopping thresholds that need to be tuned
3) There is not even a mention of a large existing literature on generative models of graphs using variational autoencoders."
iclr_2018_rJiaRbk0-,"This paper proposes training binary-values LSTMs for NLP using the Gumbel-softmax reparameterization.  The motivation is that this will generalize better, and this is demonstrated in a couple of instances.

However, it's not clear how cherry-picked the examples are, since the training loss wasn't reported for most experiments.  And, if the motivation is better generalization, it's not clear why we would use this particular setup."
iclr_2018_r1h2DllAW,"This paper presents a somewhat new approach to training neural nets with ternary or low-precision weights.  However the Bayesian motivation doesn't translate into an elegant and self-tuning method, and ends up seeming kind of complicated and ad-hoc.  The results also seem somewhat toy.  The paper is fairly clearly written, however."
iclr_2018_S1Y7OOlRZ,"This paper presents a simple tweak to hyperband to allow it to be run asynchonously on a large cluster, and contains reasonably large-scale experiments.

The paper is written clearly enough, and will be of interest to anyone running large-scale ML experiments.  However, it falls below the bar by:
1) Not exploring the space of related ideas more.
2) Not providing novel insights.
3) Not attempting to compare against model-based parallel approaches."
iclr_2018_SyAbZb-0Z,"This paper presents a sensible, but somewhat incremental, generalization of neural architecture search.  However, the experiments are only done in a single artificial setting (albeit composed of real, large-scale subtasks).  It's also not clear that such an expensive meta-learning based approach is even necessary, compared to more traditional approaches.

If this paper was less about proposing a single new extension, and more about putting that extension in a larger context, (either conceptually or experimentally), it would be above the bar.
"
iclr_2018_SyrGJYlRZ,"This paper asks when SGD+M can beat adaptive methods such as Adam, and then suggests a variant of SGD+M with an adaptive controller for a single learning rate and momentum parameter.  There is are comparisons with some popular alternatives.  However, the bulk of the paper is concerned with a motivation that didn't convince any of the reviewers."
iclr_2018_H1bM1fZCW,"This paper proposes a way to automatically weight different tasks in a multi-task setting.  The problem is a bit niche, and the paper had a lot of problems with clarity, as well as the motivation for the experimental setup and evaluation."
iclr_2018_H1OQukZ0-,"This paper presents an update to the method of Franceschi 2017 to optimize regularization hyperparameters, to improve stability.  However, the theoretical story isn't so clear, and the results aren't much of an improvement.  Overall, the presentation and development of the idea needs work."
iclr_2018_SyjjD1WRb,"This method makes a connection between evolutionary and variational methods in a particular model.  This is a good contribution, but there has been little effort to position it in comparison to standard methods that do the same thing, showing relative strengths and weaknesses.

Also, please shorten the abstract."
iclr_2018_r1kP7vlRb,"The pros and cons of this paper can be summarized as follows:

Pros:
* It seems that the method has very good intuitions: consideration of partial rewards, estimation of rewards from modified sequences, etc.

Cons:
* The writing of the paper is scattered and not very well structured, which makes it difficult to follow exactly what the method is doing. If I were to give advice, I would flip the order of the sections to 4, 3, 2 (first describe the overall method, then describe the method for partial rewards, and finally describe the relationship with SeqGAN)
* It is strange that the proposed method does not consider subsequences that do not contain y_{t+1}. This seems to go contrary to the idea of using RL or similar methods to optimize the global coherence of the generated sequence.
* For some of the key elements of the paper, there are similar (widely used) methods that are not cited, and it is a bit difficult  to understand the relationship between them:
** Partial rewards: this is similar to ""reward shaping"" which is widely used in RL, for example in the actor-critic method of Bahdanau et al.
** Making modifications of the reference into a modified reference: this is done in, for example, the scheduled sampling method of Bengio et al.
** Weighting modifications by their reward: A similar idea is presented in ""Reward Augmented Maximum Likelihood for Neural Structured Prediction"" by Norouzi et al.

The approach in this paper is potentially promising, as it definitely contains a lot of promising insights, but the clarity issues and fact that many of the key insights already exist in other approaches to which no empirical analysis is provided makes the contribution of the paper at the current time feel a bit weak. I am not recommending for acceptance at this time, but would certainly encourage the authors to do clean up the exposition, perhaps add a comparison to other methods such as RL with reward shaping, scheduled sampling, and RAML, and re-submit to another venue."
iclr_2018_rkdU7tCaZ,"The pros and cons of the paper are summarized below:

Pros:
* The proposed tweaks to the dynamic evaluation of Mikolov et al. 2010 are somewhat effective, and when added on top of already-strong baseline models improve them substantially

Cons:
* Novelty is limited. This is essentially a slightly better training scheme than the method proposed by Mikolov et al. 2010.
* The fair comparison against Mikolov et al. 2010 is only shown in Table 1, where a perplexity of 78.6 turns to a perplexity of 73.5. This is a decent gain, but the great majority of this is achieved by switching the optimizer from SGD to an adaptive method, which as of 2018 is a somewhat limited contribution. The remainder of the tables in the paper do not compare with the method of Mikolov et al.
* The paper title, abstract, and introduction do not mention previous work, and may give the false impression that this is the first paper to propose dynamic evaluation for neural sequence models, significantly overclaiming the paper's contribution and potentially misleading readers.

As a result, while I think that dynamic evaluation itself is useful, given the limited novelty of the proposed method and the lack of comparison to the real baseline (the simpler strategy of Mikolov et al.) in the majority of the experiments, I think this papers till falls short of the quality bar of ICLR.

Also, independent of this decision, a final note about perplexity as an evaluation measure to elaborate on the comments of reviewer 1. In general, perplexity is an evaluation measure that is useful for comparing language models of the same model class, but tends to not correlate well with model performance (e.g. ASR accuracy) across very different types of models. For example, see ""Evaluation Metrics for Language Models"" by Chen et al. 1998. The method of dynamic evaluation is similar to the cache-based language models that existed in 1998 in that it reinforces the model to choose similar vocabulary to that it's seen before. As you can see from this paper that the quality of perplexity of an evaluation measure falls when cache-based models are thrown into the mix, and one reason for this is that cache models, while helping perplexity greatly, tend to reinforce previous errors when errors do occur."
iclr_2018_SkYibHlRb,"The pros and cons of the paper cited by the reviewers can be summarized as follows:

Pros:
- good problem, NL2SQL is an important task given how dominant SQL is
- incorporating a grammar (""sketch"") is a sensible improvement.

Cons:
- The dataset used makes very strong simplification assumptions (that every token is an SQL keyword or appears in the NL)
- The use of a grammar in the context of semantic parsing is not novel, and no empirical comparison is made against other reasonable recent baselines that do so (e.g. Rabinovich et al. 2017).

Overall, the paper seems to do some engineering for the task of generating SQL, but without an empirical comparison to other general-purpose architectures that incorporate grammars in a similar way, the results seem incomplete, and thus I cannot recommend that the paper be accepted at this time."
iclr_2018_r1nmx5l0W,"Pros and cons of the paper can be summarized as follows:

Pros:
* The underlying idea may be interesting
* Results are reasonably strong on the test set used

Cons:
* Testing on the single dataset indicates that the model may be of limited applicability
* As noted by reviewer 2, core parts of the paper are extremely difficult to understand, and the author response did little to assuage these concerns
* There is little mathematical notation, which compounds the problems of clarity

After reading the method section of the paper, I agree with reviewer 2: there are serious clarity issues here. As a result, I do cannot recommend that this paper be accepted to ICLR in its current form. I would suggest the authors define their method precisely in mathematical notation in a future submission."
iclr_2018_BkUDW_lCb,"The pros and cons of the paper can be summarized below:

Pro:
* The improvements afforded by the method are significant over baselines, although these baselines are very preliminary baselines on a new dataset.

Con
* There is already a significant amount of work in using grammars to guide semantic parsing or code generation, as rightfully noted by the authors, and thus the approach in the paper is not extremely novel.
* Because there is no empirical comparison with these methods, the relative utility of the proposed method is not clear.

As a result, I recommend that the paper not be accepted at this time."
iclr_2018_HyTrSegCb,"The pros and cons of this paper cited by the reviewers can be summarized below:

Pros:
* Empirical results demonstrate decent improvements over other reasonable models
* The method is well engineered to the task

Cons:
* The paper is difficult to read due to grammar and formatting issues
* Experiments are also lacking detail and potentially difficult to reproduce
* Some of the experimental results are suspect in that the train/test accuracy are basically the same. Usually we would expect train to be much better in highly parameterized neural models
* The content is somewhat specialized to a particular task in NLP, and perhaps of less interest to the ICLR audience as a whole (although I realize that ICLR is attempting to cast a broad net so this alone is not a reason for rejection of the paper)

In addition to the Cons cited by the reviewers above, I would also note that there is some relevant work on morphology in sequence-to-sequence models, e.g.:
* ""What do Neural Machine Translation Models Learn about Morphology?"" Belinkov et al. ACL 2017.

and that it is common in sequence-to-sequence models to use sub-word units, which allows for better handling of morphological phenomena:
* ""Neural Machine Translation of Rare Words with Subword Units"" Sennrich et al. ACL 2016.

While the paper is not without merit, given that the cons seem to significantly outweigh the pros, I don't think that it is worthy of publication at ICLR at this time, although submission to a future conference (perhaps NLP conference) seems warranted."
iclr_2018_rJ7RBNe0-,"The pros and cons of this paper cited by the reviewers (with a small amount of my personal opinion) can be summarized below:

Pros:
* The method itself seems to be tackling an interesting problem, which is feature matching between encoders within a generative model

Cons:
* The paper is sloppily written and symbols are not defined clearly
* The paper overclaims its contributions in the introduction, which are not supported by experimental results
* It mis-represents the task of decipherment and fails to cite relevant work
* The experimental setting is not well thought out in many places (see Reviewer 1's comments in particular)

As a result, I do not think this is up to the standards of ICLR at this time, although it may have potential in the future."
iclr_2018_r1HNP0eCW,"The pros and cons of this paper cited by the reviewers can be summarized below:

Pros:
* The motivation of the problem is presented well
* The architecture is simple and potentially applicable to real-world applications

Cons:
* The novel methodological contribution is limited to non-existant
* Comparison against other relevant baselines is missing, and the baseline is not strong
* The evaluation methodology does not follows standard practice in IR, and thus it is difficult to analyze and compare results
* Paper is hard to read and requires proofreading

Considering these pros and cons, my conclusion is that this paper is not up to the standards of ICLR."
iclr_2018_ryacTMZRZ,"R1 was neutral on the paper: they liked the problem, simplicity of the approach, and thought the custom pooling layer was novel, but raised issues with the motivation and design of experiments. R1 makes a reasonable point that training a CNN to classify time series, then throw away the output layer and use the internal representation in 1-NN classification is hard to justify in practice.
Results of the reproducibility report were good, though pointed out some issues around robustness to initialization and hyper-parameters. R2 gave a very strong score, though the review didn’t really expound on the paper’s merits. R3 thought the paper was well written but also sided with R1 on novelty. Overall, I side with R1 and R3. Particularly with respect to the practicality of the approach (as pointed out by both these reviewers). I would feel differently if the metric was used in another application beyond classification."
iclr_2018_ryj38zWRb,"This paper attempts to decouple two factors underlying the success of GANs: the inductive bias of deep CNNs and adversarial training. It shows that, surprisingly, the second factor is not essential. R1 thought that comparisons to Generative Moment Matching Networks and Variational Autoencoders should be provided (note: this was added to a revised version of the paper). They also pointed out that the paper lacked comparisons to newer flavors of GANs. While R1 pointed out that the use of 128x128 and 64x64 images was weak, I tend to disagree as this is still common for many GAN papers. R2 was neutral to positive about the paper and thought that most importantly, the training procedure was novel. R3 also gave a neutral to positive review, claiming the paper was easy to follow and interesting. Like R1, R3 thought that a stronger claim could be made by using different datasets. In the rebuttal, the authors argued that the main point was not in proposing a state-of-the-art generative model of images but to provide more an introspection on the success of GANs. Overall, I found the work interesting but felt that the paper could go through one more review/revision cycle. In particular, it was very long. Without a champion, this paper did not make the cut."
iclr_2018_B1ZZTfZAW,"Overall I agree with the assessment of R1 that the paper touches on many interesting issues (deep learning for time series, privacy-respecting ML, simulated-to-real-world adaptation) but does not make a strong contribution to any of these. Especially with respect to the privacy-respecting aspect, there needs to be more analysis showing that the generative procedure does not leak private information (noting R1 and R3’s comments). I appreciate the authors clarifying the focus of the work, and revising the manuscript to respond to the reviews. Overall it’s a good paper on an important topic but I think there are too many issues outstanding for accept at this point."
iclr_2018_SkfNU2e0Z,"This paper presents a toolbox for the exploration of layerwise-parallel deep neural networks. The reviewers were consistent in their analysis of this paper: it provided an interesting class of models which warranted further investigation, and that the toolbox would be useful to those who are interested in exploring further. However, there was a lack of convincing examples, and also some concern that Theano (no longer maintained) was the only supported backend. The authors responded to say that they had subsequently incorporated TensorFlow support, they were not able to provide any more examples due to several reasons: “time, pending IP concerns, open technical details, sufficient presentation quality, page restriction.” I agree with the consensus reached by the reviewers."
iclr_2018_HyIFzx-0b,"The paper proposes using a set of orthogonal bases that combine to form convolution kernels for CNNs leading to a significant reduction of memory usage. The main concerns raised by the reviewers were 1) clarity; 2) issues with writing and presentation of results; 3) some missing experiments. The authors released a revised version of the paper and a short summary of the enhancements. None of the reviewers changed scores following the author response. The reviews were detailed and came from those familiar with CNNs. I have decided to go with reviewer consensus.
"
iclr_2018_SJtfOEn6-,R1 and R3’s  main concern was that the work was not actually outperforming existing work and therefore its advantages were unclear. R2 brought up several questions on the experiments and asked for clarification with respect to previous work. R3 had several other detailed questions for the authors. The authors did not provide a response.
iclr_2018_HyFaiGbCW,"Both R1 and R2 suggested that Conceptors (Jaeger, 2014) had previously explored learning transformations in the context of reservoir computing. The authors acknowledged this in their response and added a reference. The main concern raised by the reviewers was lack of novelty and weak experiments (both the MNIST and depth maps were small and artificial). The authors acknowledged that it was mainly a proof of concept type of work. R1 and R2 also rejected the claim of biological plausibility (and this was also acknowledged by the authors). Though the authors have taken great care to respond in detail to each of the reviewers, I agree with the consensus that the paper does not meet the acceptance bar."
iclr_2018_H1vCXOe0b,"The paper proposes a new method for interpreting the hidden units of neural networks by employing an Indian Buffet Process. The reviewers felt that the approach was interesting, but at times hard to follow and more analysis was needed. In particular, it was difficult to glean any advantage of this method over others. The authors did not provide a response to the reviews."
iclr_2018_HJ1HFlZAb,"Given that the paper proposes a new evaluation scheme for generative models, I agree with the reviewers that it is essential that the paper compare with existing metrics (even if they are imperfect). The choice of datasets was very limited as well, given the nature of the paper. I acknowledge that the authors took care to respond in detail to each of the reviews."
iclr_2018_r1RQdCg0W,"There is a very nice discussion with one of the reviewers on the experiments, that I think would need to be battened down in an ideal setting. I'm also a bit surprised at the lack of discussion or comparison to two seemingly highly related papers:

1. T. G. Dietterich and G. Bakiri (1995) Solving Multiclass via Error Correcting Output Codes.
2. Hsu, Kakade, Langford and Zhang (2009) Multi-Label Prediction via Compressed Sensing.
"
iclr_2018_r1AoGNlC-,"This paper introduces a possibly useful new RL idea (though it's a incremental on Liang et al), but the evaluations don't say much about why it works (when it does), and we didn't find the target application convincing.
"
iclr_2018_r154_g-Rb,"Overall the reviewers appear to like the ideas in this paper, though this is some disagreement about novelty (I agree with the reviewer who believes that the top-level search can very easily be interpreted as an MDP, making this very similar to SMDPs). The reviewers generally felt that the experimental results need to more closely compare with some existing techniques, even if they're not exactly for the same setting."
iclr_2018_B1NOXfWR-,"Paper presents and interesting new direction, but the evaluation leaves many questions open, and situation with respect to state of the art is lacking"
iclr_2018_By3VrbbAb,This paper has some interesting ideas that have been implemented in a rather ad hoc way; the presentation focuses perhaps too much on engineering aspects.
iclr_2018_BJ4prNx0W,"This paper is novel, but relatively incremental and relatively niche; the reviewers (despite discussion) are still unsure why this approach is needed."
iclr_2018_ryZElGZ0Z,"There was substantial disagreement between reviewers on how this paper contributes to the literature; it seems (having read the paper) that the problem tackled here is clearly quite interesting, but it is hard to tease out in the current version exactly what the contribution does to extend beyond current art."
iclr_2018_SyvCD-b0W,The reviewers have pointed out that there is a substantial amount of related work that this paper should be acknowledging and building on.
iclr_2018_H1Ww66x0-,"The output kernel idea for lifelong learning is interesting, but insufficiently developed in the current draft."
iclr_2018_SkFvV0yC-,"The paper presents a variant of network morphism (Wei et al., 2016) for dynamically growing deep neural networks. There are some novel contributions (such as OptGD for finding a morphism given the parent network layer). However, in the current form, the experiments mostly focus on comparisons against fixed network structure (but this doesn't seem like a strong baseline, given Wei et al.'s work), so the paper should provide more comparisons against Wei et al. (2016) to highlight the contribution of this work. In addition, the results will be more convincing if the state-of-the-art performance can be demonstrated for large-scale problems (such as ImageNet classification). "
iclr_2018_SJQO7UJCW,"The paper presents a reasonable idea, probably an improved version of method (combination of GAN and SSL for semantic segmentation) over the existing works. Novelty is not ground-breaking (e.g., discriminator network taking only pixel-labeling predictions, application of self-training for semantic segmentation---each of this component is not highly novel by itself). It looks like a well-engineered model that manages to get a small improvement with a semi-supervised learning setting. However, given that the focus of the paper is on semi-supervised learning, the improvement from the proposed loss (L_semi) is fairly small (0.4-0.8%)."
iclr_2018_SyVVXngRW,"The paper proposes a multitask deep learning method (called Deep-AMFTL) for preventing negative transfer. Despite some positive experimental results, the contribution of the paper is not sufficient for publication at ICLR due to several issues: similarity between the proposed method and existing method (e.g., AMTL), unclear rationale/intuition of the proposed model, clarity of presentation, technical formulation, and limited empirical evaluations (see reviewer comments for details). No author rebuttal was submitted.
"
iclr_2018_Hy_o3x-0b,"The paper proposes a VAE variant by embedding spatial information with multiple layers of latent variables. Although the paper reports state-of-the-art results on multiple datasets, some results may be due to a bug. This has been discussed, and the author acknowledges the bug. We hope the problem can be fixed, and the paper reconsidered at another venue.
"
iclr_2018_HJIhGXWCZ,"The paper proposes a novel predictive model (e.g., from videos), called error encoding networks, by first learning a deterministic prediction model and then learning to minimize the residual error using latent variables. The latent variables given the sample are estimated by sampling from the prior then updating via gradient descent. The proposed method shows improved performance over the baselines. However, the qualitative results are not fully convincing, possibly because of (1) the limitation of the architecture, (2) suboptimal implementation/tuning of baselines (such as GAN and cVAE). "
iclr_2018_rJa90ceAb,"The paper proposes a method for learning convolutional networks with dynamic input-conditioned filters. There are several prior work along this idea, but there is no comparison agaist them. Overall, experimental results are not convincing enough."
iclr_2018_HkCvZXbC-,"The paper presents a layered image generation model  (e.g., foreground vs background) using GANs. The high-level idea is interesting, but novelty is somewhat limited. For example, layered generation with VAE/GAN has been explored in Yan et al. 2016 (VAEs) and Vondrick et al. 2016 (GANs). In addition, there are earlier works for unsupervised learning of foreground/background generative models (e.g., Le Roux et al., Sohn et al.). Another critical problem is that only qualitative results on relatively simple datasets (e.g., MNIST, SVHN, CelebA) are provided as experimental results. More quantitative evaluations and additional experiments on more challenging datasets will strengthen the paper.

* N. Le Roux, N. Heess, J. Shotton, J. Winn; Learning a generative model of images by factoring appearance and shape; Neural Computation 23(3): 593-650, 2011.
** Sohn, K., Zhou, G., Lee, C., & Lee, H. Learning and selecting features jointly with point-wise gated Boltzmann machines. ICML 2013.
"
iclr_2018_rkQsMCJCb,"The paper proposes a GAN model with adaptive convolution kernels. The proposed idea is reasonable, but the novelty is somewhat minor and the experimental results are limited. More comprehensive experiments (e.g., other evaluation metrics) will strengthen the future revision of paper. No rebuttal was submitted.
"
iclr_2018_HJrJpzZRZ,"The paper proposes adversarial flow-based neural network architecture with adversarial training for video prediction. Although the reported experimental results are promising, the paper seems below ICLR threshold due to limited novelty and issues in evaluation (e.g., mechanical turk experiment). No rebuttal was submitted."
iclr_2018_Hy8hkYeRb,"The paper attempts to develop a method for learning latent representations using deep predictive coding and deconvolutional networks. However, the theoretical motivation for the proposed model in relation to existing methods (such as original predictive coding, deconvolutional networks, ladder networks, etc.), as well as the empirical comparison against them is unclear. The experimental results on the CIFAR10 dataset do not provide much insight on what kind of meaningful/improved representations can be learned in comparison to existing methods, both qualitatively and quantitatively. No rebuttal was provided. "
iclr_2018_ry4S90l0b,"The paper presents self-training scheme for GANs. The proposed idea is simple but reasonable, and the experimental results show promise for MNIST and CIFAR10. However, the novelty of the proposed method seems relatively small and experimental results lack comparison against other stronger baselines (e.g., state-of-the-art semi-supervised methods). Presentation needs to be improved. More comprehensive experiments on other datasets would also strengthen the future version of the paper. "
iclr_2018_rJg4YGWRb,"A version of GCNs of Kipf and Welling is introduced with (1) no non-linearity; (2) a basic form of (softmax) attention over neighbors where the attention scores are computed as the cosine of endpoints' representations (scaled with a single learned scalar). There is a moderate improvement on Citeseer, Cora, Pubmed.

Since the use of gates with GCNs / Graph neural networks is becoming increasingly common (starting perhaps with GGSNNs of Li et al, ICLR 2016)) and using attention in graph neural networks is also not new  (see reviews and comments for references), the novelty is very limited.  In order to make the submission more convincing the authors could: (1) present results on harder datasets; (2)  carefully evaluate against other forms of attention (i.e. previous work).

As it stands, though it is interesting to see that such simple model performs well on the three datasets, I do not see it as an ICLR paper.

Pros:
-- a simple model, achieves results close / on par with state of the art

Cons:
-- limited originality
-- either results on harder datasets or / and evaluation agains other forms of attention (i.e. previous work) are needed


"
iclr_2018_HJRV1ZZAW,"The key motivation for the work is producing both an efficient (parallelizable / fast) and accurate reading comprehension model. At least two reviewers are not convinced that this goal is really achieved (e.g., no comparison to hierarchical modeling, performance is not as strong).   I also share concerns of R1 that, without proper ablation search and more careful architecture choice, the modeling decisions seem somewhat arbitrary.

+ the goal (of achieving effective reading comprehesion models) is important
- alternative parallelization techniques (e.g., hierarchical modeling) are not considered
- ablation studies / more systematic architecture search are missing
- it is not clear that the drop in accuracy can be justified by the potential efficiency gains (also see details in R3 -> no author response to them)
"
iclr_2018_BJMuY-gRW,"Though the general direction is interesting and relevant to ICLR, the novelty is limited. As reviewers point out it is very similar to Le & Zuidema (2015), with few modifications (using LSTM word representations, a different type of pooling). However, it is not clear if they are necessary  as there is no direct comparison (e.g., using a different type of pooling). Overall, though the submission is generally solid,  it does not seem appropriate for ICLR.

+ solid
+ well written
- novelty limited
- relation to Le & Zuidema is underplayed"
iclr_2018_B1kIr-WRb,"The reviewers are concerned that the evaluation quality is not sufficient to convince readers that the proposed embedding method is indeed superior to alternatives. Though the authors attempted to address these comments in a subsequent revision but still, e.g., the evaluation is only intrinsic or on contrived problems. Given the limited novelty of the approach (it is a fairly straightforward generalization of Levy and Goldberg's factorization of PPMI matrix; the factorization is not new per se as well), the quality of experiments and analysis should be improved.

+ the paper is well written
- novelty is moderate
- better evaluation and analysis are necessary
"
iclr_2018_H113pWZRb,"The authors provide an extension to GCNs of Kipf and Welling in order to incorporate information about higher order neighborhoods. The extension is well motivated (and  though I agree that it is not trivial modification of the K&W approach to the second order,  thanks to the authors for the clarification).  The improvements are relatively moderate.

Pros:
-- The approach is well motivated
-- The paper is clearly written
Cons:
-- The originality and impact (as well as motivation) are questioned by the reviewers
"
iclr_2018_rylejExC-,"The paper studies subsampling techniques necessary to handle large graphs with graph convolutional networks.  The paper introduces two ideas: (1) preprocessing for GCNs (basically replacing dropout followed by linear transformation with linear transformation followed by drop out); (2) adding control variates based on historical activations.  Both ideas seem useful (but (1) is more empirically useful than (2), Figure 4*). The paper contains a fair bit of math (analysis / justification of the method).

Overall, the ideas are interesting and can be useful in practice. However, not all reviewers are convinced that the methods constitute a significant contribution.  There is also a question whether the math has much value (strong assumptions - also, from interpretation, may be too specific to the formulation of Kipf & Welling making it a bit narrow?).  Though I share these feelings and recommend rejection, I think that the reviewers 2 and 3 were a bit too harsh, and the scores do not reflect the quality of the paper.

*Potential typo: Figure 4 -- should it be CV +PP rather than CV?

+ an important problem
+ can be useful in practical applications
+ generally solid and sufficiently well written
- significance not sufficient
- math seems not terribly useful

"
iclr_2018_SkJKHMW0Z,"The proposed relational reasoning algorithm is basically a fairly standard graph neural network, with a few modifications (e.g., the prediction loss at each layer - also not a new idea per se).

 The claim that previously reasoning has not been considered in previous applications of graph neural networks (see discussion) is questionable.  It is not even clear what is meant here by 'reasoning' as many applications of graph neural networks may be regarded as performing some kind of inference on graphs (e.g., matrix completion tasks by Berg, Kipf and Welling; statistical relational learning by  Schlichtkrull et al).

So the contribution seems a bit over-stated.  Rather than introduces a new model, the work basically proposes an application of largely known model to two (not-so-hard) tasks which have not been studied in the context of GNNs. The claim that the approach is a general framework for dealing with complex reasoning problems is not well supported as both problems are (arguably) not complex reasoning problems (see R2).

There is a general consensus between reviewers that the paper, in its current form, does not quite meet acceptance criteria.

Pros:
-- an interesting direction
-- clarity
Cons:
-- the claim of generality is not well supported
-- the approach is not so novel
-- the approach should be better grounded in previous work


"
iclr_2018_ByquB-WC-,"The contribution of this paper basically consists of using MLPs in the attention mechanism of end-2-end memory networks. Though it leads to some improvements on bAbI (which may not be so surprising - MLP attention has been shown preferable in certain scenarious), it does not seem to be a sufficient contribution. The motivation is also confusing - the work is not really that related to relation networks, which were specifically designed to deal with situations where *relations* between objects matter. The proposed architecture does not model relations.

+  improvement on bAbI over the baselines
-  limited novelty (MLP attention is fairly standard)
-  the presentation of the idea is confusing (if the claim is about relations -> other datasets need to be considered)

There is a consensus between reviewers. "
iclr_2018_rJBwoM-Cb,"The proposed neural tree transduction framework is basically a combination of tree encoding and tree decoding. The tree encoding component is simply reused from previous work (TreeLSTM) whereas the decoding components is somewhat different from the previous work. They key problems (acknowledge also by at least 2 reviewers):

Pros:
-- generating trees input under-explored direction (note that it is more general than parsing as nodes may not directly correspond to input symbols)

Cons:
-- no comparison with previous tree-decoding work
-- only artificial experiments
-- the paper is hard too read (confusing) / mathematical notation and terminology is confusing and seems sometimes inaccurate (see R3)



"
iclr_2018_S1sRrN-CW,"The reviewers are not convinced by a number of aspects: including originality and clarity. Whereas the assessment of clarity and originality may be somewhat subjective (though the connections between margin-based loss and negative sampling is indeed well known), it is pretty clear that evaluation is very questionable. This is not so much about existence of more powerful factorizations  (e.g., ConvE / HolE) but the fact that the shown baselines (e.g., DistMult) can be tuned to yield much better performance on these benchmarks.  Also, indeed the authors should report results on cleaned versions of the datasets (e.g., FB15k-237).  Overall, there is a consensus that the work is not ready for publication.

Pros:
-- In principle, new insights on standardly used methods would have been very interesting

Cons:
-- Evaluation is highly problematic
-- At least some results do not seem so novel / interesting; there are questions about the rest (e.g., assumptions)
-- The main advantage of sq loss methods is that it enables the alternating least squares algorithm, does not seem possible here (at least not shown) "
iclr_2018_SJ71VXZAZ,"The paper reports experiments where a LSTM language model is pretrained on a large corpus of reviews, and then the produced representation is used within a classifier on a number of sentiment classification datasets.  The relative success of the method is not surprising. The novelty is very questionable, the writing quality is mixed (e.g., typos, the model is not even properly described). There are many gaps in evaluation (e.g., from the intro it seems that the main focus is showing that byte level modeling is preferable to more standard set-ups -- characters / BPE / words). However, there are (almost) no experiments supporting this claim. The same is true for the 'sentiment neuron': its effectiveness is also not properly demonstrated. In general, the results are somewhat mixed.

Pros:
-- good results on some datasets
Cons:
-- limited novelty
-- some claims are not tested / issues with evaluation
-- writing quality is not sufficient / clarity issues


Overall, the reviewers are in agreement that the paper does not meet ICLR standards.

"
iclr_2018_S1XXq6lRW,"Unfortunately, it falls short of ICLR standards -- from evaluation, novelty and clarity perspectives. The method is also not discussed in all details. "
iclr_2018_BkM27IxR-,"The presented work is a good attempt to expand the work of Li and Malik to the high-dimensional, stochastic setting. Given the reviewer comments, I think the paper would benefit from highlighting the comparatively novel aspects, and in particular doing so earlier in the paper.

It is very important, given the nature of this work, to articulate how the hyperparameters of the learned optimizers, and of the hand-engineered optimizers are chosen. It is also important to ensure that the amount of time spent on each is roughly equal in order to facilitate an apples-to-apples comparison.

The chosen architectures are still quite small compared to today's standards. It would be informative to see how the learned optimizers compare on realistic architectures, at least to see the performance gap.

Please clarify the objective being optimized, and it would be useful to report test error.

The approach is interesting, but does not yet meet the threshold required for acceptance."
iclr_2018_ByuP8yZRb,"The reviewers tend to agree that the empirical results in this paper are good compared to the baselines. However, the paper in its current form is considered a bit too incremental. Some reviewers also suggested additional theory could help strengthen the paper."
iclr_2018_SJzMATlAZ,"After careful consideration, I think that this paper in its current form is just under the threshold for acceptance. Please note that I did take into account the comments, including the reviews and rebuttals, noting where arguments may be inconsistent or misleading.

The paper is a promising extension of RCC, albeit too incremental. Some suggestions that may help for the future:

1) Address the sensitivity remark of reviewer 2. If the hyperparameters were tuned on RCV1 instead of MNIST, would the results across the other datasets remain consistent?

2) Train RCC or RCC-DR in an end-to-end way to gauge the improvement of joint optimization over alternating, as this is one of the novel contributions.

3) Discuss how to automatically tune \lambda and \delta_1 and \delta_2. These may appear in the RCC paper, but it's unclear if the same derivations hold when going to the non-linear case (they may in fact transfer gracefully, it's just not obvious). It would also be helpful for researchers building on DCC."
iclr_2018_ryjw_eAaZ,"The updated draft has helped to address some of the issues that the reviewers had, however the reviewers believe there are still outstanding issues. With regard to the technical flaw, one reviewer has pointed out that the update changes the story of the paper by breaking the connection between the generative and discriminative model in terms of preserving or ignoring conditional dependencies.

In terms of the experiments, the paper has been improved by the reporting of standard deviation, and comparison to other works. However it is recommended that the authors compare to NAS by fixing the number of parameters and reporting the results to facilitate an apples-to-apples comparison. Another reviewer also recommends comparing to other architectures for a fixed number of neurons."
iclr_2018_rk9kKMZ0-,"Although paper has been improved with new quantitative results and additional clarity, the reviewers agree though that larger-scale experiments would better highlight the utility of the method. There are some concerns with computational cost, despite the fact that the two networks are trained asynchronously. A baseline against a single, asynchronously trained network (multiple GPUs) would help strengthen this point. Some reviewers expressed concerns with novelty."
iclr_2018_SkFEGHx0Z,"This paper proposes a non-parametric method for metric learning and classification. One of the reviewers points out that it can be viewed as an extension of NCA. There is in fact a non-linear version of NCA that was subsequently published, see [1]. In this sense, the approach here appears to be a version of nonlinear NCA with learnable per-example weights, approximate nearest neighbour search, and the allowance of stale exemplars. In this view, there is concern from the reviewers that there may not be sufficient novelty for acceptance.

The reviewers have concerns with scalability. It would be helpful to include clarification or even some empirical results on how this scales compared to softmax. It is particularly relevant for larger datasets like Imagenet, where it may be impossible to store all exemplars in memory.

It is also recommended to relate this approach to metric-learning approaches in few-shot learning. Particularly to address the claim that this is the first approach to combine metric learning and classification.

[1]: Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure. Ruslan Salakhutdinov and Geoffrey Hinton.  AISTATS 2007 "
iclr_2018_rJ695PxRW,"The problem of discovering ordering in an unordered dataset is quite interesting, and the authors have outlined a few potential applications. However, the reviewer consensus is that this draft is too preliminary for acceptance. The main issues were clarity, lack of quantitative results for the order discovery experiments, and missing references. The authors have not yet addressed these issues with a new draft, and therefore the reviewers have not changed their opinions."
iclr_2018_SyuWNMZ0W,"The reviewers agree that the problem being addressed is interesting, however there are concerns with novelty and with the experimental results. An experiment beyond dealing with class imbalance would help strengthen this paper, as would experiments with other kinds of GANs."
iclr_2018_HydnA1WCb,"The reviewers agree that the idea of utilizing covariance information in the few-shot setting is interesting. There are concerns with the novelty of the paper, as well as the correctness in terms of ensuring the covariance matrix is PSD in all cases. There are some concerns with the experimental evaluation as well. In this area, Omniglot is a good sanity check, but other baseline datasets like miniImagenet are necessary to determine if this approach is truly useful."
iclr_2018_ryH_bShhW,"The reviewers all outlined concerns regarding novelty and the maturity of this work. It would be helpful to clarify the relation to doubly stochastic kernel machines as opposed to random kitchen sinks, and to provide more insight into how this stochasticity helps. Finally, the approach should be tried on more difficult image datasets."
iclr_2018_rk6qdGgCZ,"This paper generated quite a bit of controversy among reviewers. The main claim of the paper is that Adam and related optimizers are broken because their ""weight decay"" regularization is not actually weight decay. It proposes to modify Adam to decay all weights the same regardless of the gradient variances.

Calling Adam's weight decay mechanism a mistake seems very far-fetched to me. Neural net optimization researchers are well aware of the connection between weight decay and L2 regularization and the fact that they don't correspond in preconditioned methods. L2 regularization is basically the only justification I have heard for weight decay, and despite rejecting this interpretation, the paper does not provide an alternative justification.

Decoupling the optimization from the cost function is a well-established principle. This abstraction barrier is not completely clean (e.g. gradient noise has well-known regularization effects), and the experiments of this paper perhaps provide evidence that the choices may be coupled in this case. This is an interesting finding, and probably worth following up on. However, the paper seems to sweep the ""decoupling optimization and cost"" issue under the carpet and take for granted that the decay rate is what should be held fixed. All three reviewers found the presentation to be misleading, and I would agree with them. While there may be an interesting contribution here, I cannot endorse the paper as-is.
"
iclr_2018_BJaU__eCZ,"The submission proposes to use GANs to learn a generative model of fMRI scans that can then be used for downstream classification tasks.  Although there was some appreciation from the reviewers of the approach, there were several important remaining concerns:

1) From Reviewer 1: ""Generating high resolution images with GANs even on faces for which there is almost infinite data is still a challenge. Here a few thousand data points are used. So it raises too concerns: First is it enough?""

and

2) R1 and R2 both raised concerns about the significance of the improvements.  Looking through the tables, there are many reported differences that are reasonably small, and no error bars or significance are given.  This should be a requirement for an empirical paper about fMRI."
iclr_2018_Sy1f0e-R-,"The problem addressed here is an important one: What is a good evaluation metric for generative models?  A good selection of popular metrics are analyzed for their appropriateness for model selection of GANs.  Two popular approaches are recommended: the kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test.  This seems reasonable, but the present work was not recommended for acceptance by 2 reviewers who raised valid concerns.

From a readability perspective, it would be nice to simply list the answer to question (1) directly in the introduction.  One must read more than a few pages to get to the answer of why the metrics that are advocated were picked.  It need not read like a mystery.

R4: ""The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification""

R2: ""First, it only considers a single task for which GANs are very popular. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions."" - the first point of which is also related to a concern of R4.

Given the overall high selectivity of ICLR, the present submission falls short."
iclr_2018_Syjha0gAZ,"The submission addresses the problem of multiset prediction, which combines predicting which labels are present, and counting the number of each object.  Experiments are shown on a somewhat artificial MNIST setting, and a more realistic problem of the COCO dataset.

There were several concerns raised by the reviewers, both in terms of the clarity of presentation (Reviewer 1), and that the proposed solution is somewhat heuristic (Reviewer 3).  On the balance, two of three reviewers did not recommend acceptance."
iclr_2018_SJme6-ZR-,"The submission proposes a Kuiper statistic based loss function for survival clustering.  This loss function is applied to train a deep network.  Results are presented on a Friendster dataset.

This submission received borderline/mixed reviews.  The primary concerns were: justification of the Kuiper loss, lack of details of the experimental setup, writing style.  In the end, these concerns remain.

Of particular importance is the justification and experimental validation of the Kuiper statistic.  Although it seems a reasonable choice, from the authors' response to R3: ""We now also report results for Kolmogorov-Smirnov loss. Although the difference in performance between the two loss functions is not significant in the Friendster dataset, Kuiper loss has higher statistical power in distinguishing distribution tails [Tygert 2010].""  If this theoretical result from [Tygert 2010] is relevant, it should be possible to demonstrate this experimentally.  If such differences are irrelevant for the data of interest, the paper should perhaps be reframed with a better discussion of available statistics and literature (cf. Reviewer 2), and a more general presentation de-emphasizing modeling choices that may have limited practical relevance.
"
iclr_2018_Bys_NzbC-,"The submission is motivated by an empirical observation of a phase transition when a sufficiently high L1 or L2 penalty on the weights is applied.  The proposed solution is to optimize for several epochs without the penalty followed by introduction of the penalty.  Although empirical results seem to moderately support this approach, there does not seem to be sufficient theoretical justification, and comparisons are missing.  Furthermore, the author response to reviewer concerns contain unclear statements e.g.
""The reason is that, to reach the level of L1 norm that is low enough, the model needs to go through the strong regularization for the first few epochs, and the neurons already lose its learning ability during this period like the baseline method.""
It is not at all clear what ""neurons already lose its learning ability"" is supposed to mean."
iclr_2018_HkOhuyA6-,"The submission proposes a strategy for creating vector representations of graphs, upon which a CNN can be applied.  Although this is a useful problem to solve, there are multiple works in the existing literature for doing so.  Given that the choice between these is essentially empirical, a through comparison is necessary.  This was pointed out in the reviews, and relevant missing comparisons were given.  The authors did not provide a response to these concerns."
iclr_2018_SyW4Gjg0W,"The reviewers were unanimous in their assessment that the paper was not ready for publication in ICLR.  Their concerns included:
 - lack of novelty over Niepert, Ahmed, Kutzkov, ICML 2016
 - The approach learns combinations of graph kernels and its expressive capacity is thus limited
 - The results are close to the state of the art and it is not clear whether any improvement is statistically significant.

The authors have not provided a response to these concerns."
iclr_2018_BkVsWbbAW,"Thank you for submitting you paper to ICLR. The big-picture idea is fairly simple, although the implementation is certainly challenging requiring a deep generative model to be trained as part of the final system. The experimental validation is not sufficient to warrant publication. A comparison to a larger number of competitors e.g. [1,2] on a greater range of tasks is required.

[1] Continual Learning Through Synaptic Intelligence Friedemann Zenke BenPoole SuryaGanguli, ICML 2017
[2] Gradient Episodic Memory for Continual Learning, David Lopez-Paz and Marc’Aurelio Ranzato, NIPS 2017"
iclr_2018_HJ5AUm-CZ,"Thank you for submitting your paper to ICLR. The reviewers agree that the idea of sharing the approximating distribution across sets of variables is an interesting one and that the Omniglot experiments are thorough. However, although the authors make the nice addition of some simple examples during the revision period and a new table of quantitative results on Omniglot, the consensus is that the experimental results are not quite persuasive enough for publication. Adding a second dataset, such as mini-imagenet or the youtube faces dataset, would make the paper very strong."
iclr_2018_Hkp3uhxCW,"Thank you for submitting you paper to ICLR. The revision improved the paper e.g. moving Appendix A3 to the main text has improved clarity, but, like reviewer 3, I still found section 4 hard to follow. As the authors suggest, shifting the terminology to ""posterior shifting” rather than “sharpening"" would help at a high level, but the design choices should be more carefully explained. The experiments are interesting and promising. The title, although altered, still seems a misnomer given that the experimental evaluation focusses on RNNs.

Summary: There is the basis of a good paper here, but the rationale for the design choices should be more carefully explained."
iclr_2018_S1fduCl0b,"Thank you for submitting you paper to ICLR. The paper studies an interesting problem and the solution, which fuses student-teacher approaches to continual learning and variational auto-encoders, is interesting. The revision of the paper has improved readability. However, although the framework is flexible, it is complex and appears rather ad hoc as currently presented. Exploration of the effect of the many hyper-parameters or some more supporting theoretical work / justification would help. The experimental comparisons were varied, but adding more baselines e.g. comparing to a parameter regularisation approach like EWC or synaptic intelligence applied to a standard VAE would have been enlightening.

Summary: There is the basis of a good paper here, but a comprehensive experimental evaluation of design choices or supporting theory would be useful for assessing what is a complex approach."
iclr_2018_BkDB51WR-,"Thank you for submitting you paper to ICLR. Two of the reviewers are concerned that the paper’s contributions are not significant enough —either in terms of the theoretical or experimental contribution -- to warrant publication. The authors have improved the experimental aspect to include a more comprehensive comparison, but this has not moved the reviewers. "
iclr_2018_B1nLkl-0Z,"Thank you for submitting you paper to ICLR. Two of the reviewers are concerned that the paper’s contributions are not significant enough —either in terms of the theoretical or experimental contribution -- to warrant publication. The authors have improved the experimental aspect to include a more comprehensive comparison, but this has not moved the reviewers.

Summary: The approach is very promising, but more experimental work is still required to demonstrate significance. "
iclr_2018_SySisz-CW,"Thank you for submitting you paper to ICLR. The paper presents an interesting analysis, but the utility of this analysis is questionable e.g. it is not clear how this might lead to improved VAEs/GANs. The authors did add an additional experimental result in their revised paper, but questions still remain. In light of this the significance of the paper is on the low side and it is therefore not ready for publication in ICLR without more work.
"
iclr_2018_rkcya1ZAW,"Thank you for submitting you paper to ICLR. The consensus from the reviewers is that there are some interesting theoretical contributions and some promising experimental support. However, although the paper is moving in the right direction, they believe that it is not quite ready for publication."
iclr_2018_rJLTTe-0W,"Thank you for submitting you paper to ICLR.  The consensus from the reviewers is that this is not quite ready for publication. There is also concern about whether ICLR, with its focus on representational learning, is the right venue for this work.

One of the reviewers initially submitted an incorrect review, but this mistake has now been rectified. Apologies that this was not done sooner in order to allow you to address their concerns."
iclr_2018_r1drp-WCZ,Thank you for submitting you paper to ICLR. The consensus from the reviewers is that this is not quite ready for publication. The work is related to (although different from) Gu et al Neural Sequential Monte Carlo NIPS2015 and it would be useful to point this out in the related work section.
iclr_2018_S1tWRJ-R-,"Thank you for submitting you paper to ICLR. ICLR. The consensus from the reviewers is that this is not quite ready for publication. In particular, the experimental results are promising, but further work is required to fully demonstrate the efficacy of the approach."
iclr_2018_HkbJTYyAb,"Thank you for submitting you paper to ICLR. ICLR. Although there revision has improved the paper, the consensus from the reviewers is that this is not quite ready for publication."
iclr_2018_rkhCSO4T-,Thank you for submitting you paper to ICLR. ICLR. The consensus from the reviewers is that this is not quite ready for publication.
iclr_2018_HJjePwx0-,"There are two parts to this paper (1) an efficient procedure for solving trust-region subproblems in second-order optimization of neural nets, and (2) evidence that the proposed trust region method leads to better generalization performance than SGD in the large-batch setting. In both cases, there are some promising leads here. But it feels like two separate papers here, and I'm not sure either individual contribution is well enough supported to merit publication in ICLR.

For (1), the contribution is novel and potentially useful, to the best of my knowledge. But as there's been a lot of work on trust region solvers and second-order optimization of neural nets more generally, claims about computational efficiency would require comparisons against existing methods. The focus on efficiency also doesn't seem to fit with the experiments section, where the proposed method optimizes less efficiently than SGD and is instead meant to provide a regularization benefit.

For (2), it's an interesting empirical finding that the method improves generalization, but the explanation for this is very hand-wavy. If second-order optimization in general turned out to help with sharp minima, this would be an interesting finding indeed, but it doesn't seem to be supported by other work in the area. The training curves in Table 1 are interesting, but don't really distinguish the claims of Section 4.5 from other possible hypotheses.
"
iclr_2018_Byk4My-RZ,"This paper presents a method for learning more flexible prior distributions for GANs by learning another distribution on top of the latent codes for training examples. It's reminiscent of layerwise training of deep generative models. This seems like a reasonable thing to do, but it's probably not a substantial enough contribution given that similar things have been done for various other generative models. Experiments show improvement in samples compared with a regular GAN, but don't compare against various other techniques that have been proposed for fixing mode dropping. For these reasons, as well as various issues pointed out by the reviewers, I don't recommend acceptance.
"
iclr_2018_H1rRWl-Cb,"This paper gives a coding theory interpretation of VAEs and uses it to motivate an additional knob for tuning and evaluating VAEs: namely, the tradeoff between the rate and the distortion. This is a useful set of dimensions to investigate, and past work on variational models has often found it advantageous to penalize the latent variable and observation coding terms differently, for broadly similar motivations. This paper includes some careful experiments analyzing this tradeoff for various VAE formulations, and provides some interesting visualizations. However, as the reviewers point out, it's difficult to point to a single clear contribution here, as the coding theory view of variational inference is well established, and the VAE case has been discussed in various other works. Therefore, I recommend rejection.
"
iclr_2018_rkMt1bWAZ,"This paper presents a bias/variance decomposition for Boltzmann machines using the generalized Pythagorean Theorem from information geometry. The main conclusion is that counterintuitively, the variance may decrease as the model is made larger. There are probably some interesting ideas here, but there isn't a clear take-away message, and it's not clear how far this goes beyond previous work on estimation of exponential families (which is a well-studied topic).

Some of the reviewers caught mathematical errors in the original draft; the revised version fixed these, but did so partly by removing a substantial part of the paper about hidden variables. The analysis, then, is limited to fully observed Boltzmann machines, which have less practical interest to the field of deep learning.
"
iclr_2018_SJOl4DlCZ,"This paper addresses the very important problem of ensuring that sensitive training data remain private. It proposes an attack whereby the attacker can reconstruct information about the training data given only the trained classifier and an auxiliary dataset. If done well, such an attack would be a useful contribution that helps make discussion of differential privacy more complete. But as the reviewers pointed out, it's not clear from the paper whether the attack has succeeded. It works only when the auxiliary data is very similar to the training data, and it's not clear if it leaks information about the training set itself, or is just summarizing the auxiliary data. This work doesn't seem quite ready for publication, but could be a strong paper if it's convincingly demonstrated that information about the training set has been leaked.

"
iclr_2018_ByuI-mW0W,"This paper proposes a method for quantitatively evaluating GANs. Better quantitative metrics for GANs are badly needed, as the field is being held back by excessive focus on generated samples. This paper proposes to estimate the Wasserstein distance to the data distribution. A paper which does this well would be a significant contribution, but unfortunately (as the reviewers point out) the experimental validation in this paper seems insufficient.

To be convincing, a paper would first need to demonstrate the ability to accurately estimate Wasserstein distance -- not an easy task, but one which receives little mention in this paper. Then it would need to validate that the method can either quantitatively confirm known results about GANs or uncover previously unknown phenomena. As it stands, I don't think this submission is ready for publication in ICLR, but I'd encourage resubmission after more careful experimental validation along the lines suggested by the reviewers.

"
iclr_2018_S1EwLkW0W,"This paper presents a theoretical justification for the Adam optimizer in terms of decoupling the signs and magnitudes of the gradients. The overall analysis seems reasonable, though there's been much back-and-forth with the reviewers about particular claims and assumptions. Overall, the contributions don't feel quite substantial enough for an ICLR publication. The interpretation in terms of signs is interesting, but it's very similar to the motivation for RMSprop, of which Adam is an extension. The performance result on diagonally dominant noisy quadratics is interesting, but it feels unsurprising that a diagonal curvature approximation would work well in this setting. I don't recommend acceptance at this point, though these ideas could potentially be developed further into a strong submission.
"
iclr_2018_HJYoqzbC-,"This paper investigates the performance of various second-order optimization methods for training neural networks. Comparing different optimizers is worthwhile, but as this is an empirical paper which doesn't present novel techniques, the bar is very high for the experimental methodology. Unfortunately, I don't think this paper clears the bar: as pointed out by the reviewers, the comparisons miss several important methods, and the experiments miss out on important aspects of the comparison (e.g. wall clock time, generalization). I don't think there is enough of a contribution here to merit publication at ICLR, though it could become a strong submission if the reviewers' points were adequately addressed.
"
iclr_2018_ByJDAIe0b,"This paper presents a memory architecture for RL based on reservoir sampling, and is meant to be an alternative to RNNs. The reviewers consider the idea to be potentially interesting and useful, but have concerns about the mathematical justification. They also point out limitations in the experiments: in particular, use of artificial toy problems, and a lack of strong baselines. I don't think the paper is ready for ICLR publication in its current form.

"
iclr_2018_H13WofbAb,"This paper introduces a method for making synchronous SGD more resistant to failed or slow workers. The idea seems plausible, but as the reviewers point out, the novelty and the experimental validation are somewhat limited. For a contribution such as this, it would be good to see some experiments on a wider range of tasks, and experiments with real rather than simulated workloads. I don't think this work is ready for publication at ICLR.
"
iclr_2018_BJLmN8xRW,"meta score: 4

This is basically an application in which some different deep learning approaches are compared on the task of automatically identifying domain names automatically generated by malware.  The experiments are well-constructed and reported.  However, the work does not have novelty beyond the application domain, and thus is not really suitable for ICLR.

Pros
 - good set of experiments carried out on an important task
 - clearly written
Cons
 - lacks technical novelty
"
iclr_2018_rkrWCJWAW,"Meta score: 5

The paper explores an interesting idea, addressing a known bias in truncated BPTT by sampling across different truncated history lengths.  Limited theoretical analysis is presented along with PTB language modelling experimentation.   The experimental part could be stronger (e.g. trying to improve over the baseline) and perhaps more than just PTB.

Pros:
 - interesting idea
Cons:
 - limited analysis
 - limited experimentation

"
iclr_2018_rJJzTyWCZ,"Meta score: 4

The paper presents a manually-constructed cloze-style fill-in-the-missing-word dataset, with baseline language modelling experiments that aim to show that  this dataset is difficult for machines relative to human performance.  The dataset is interesting but the fact that the experiments are confined to baseline language models
Pros:
 - interesting dataset
 - clear and well-written
 - attempt to move the field forward in an important area
Cons:
 - limited experimentation
 - language modelling approaches not appropriate baseline

"
iclr_2018_Bk346Ok0W,"Meta-score: 4

This paper presents an approach which uses attention across multiple speech or video channels.  After some synthetic experiments, presents experiments on chime-3, but has a rather weak baseline system

Pros:
 - addresses an interesting task

Cons:
 - does not take account of other recent papers in the area
 - experimental results are weak - very high errors in baseline system
 - limited novelty"
iclr_2018_SkNQeiRpb,"meta score: 4

The paper uses a deep autoencoder to rating prediction, with experiments on netflix.

Pros
 - Proposed dense refeeding approach appears novel
 - Good experimental results

Cons
 - limited experimentation
 - main novelty (dense refeeding) is not well linked to existing data imputation approaches
 - novel contribution is otherwise quite limited

"
iclr_2018_H1DGha1CZ,"meta score: 4

This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization.

Pros
 - good set of experiments using CIFAR, with good results
 - attempt to explain the approach using expectations
Cons
 - theoretical explanations are not so convincing
 - limited novelty
 - CIFAR is relatively limited set of experiments
 - does not compare with using bn after relu, which is now well-studied and seems to address the motivation of this paper (and thus questions the conclusions)"
iclr_2018_ryY4RhkCZ,"Meta score: 4

The paper concerns the development of a density network for estimating uncertainty in recommender systems.  The submitted paper is not very clear and it is hard to completely understand the proposed method from the way it is presented.  This makes assessing the contribution of the paper  difficult.

Pros:
 - addresses an  interesting and important problem
 - possible novel contribution

Cons:
 - poorly written, hard to understand precisely what is done
 - difficult to compare with the state-of-the-art, not helped by disorganised literature review
 - experimentation could be improved

The paper needs more work before being ready for publication."
iclr_2018_rkfbLilAb,"meta score: 4

This paper is primarily an application paper applying known RL techniques to dialogue.    Very little reference to the extensive literature in this area.

Pros:
 - interesting application (digital search)
 - revised version contains subjective evaluation of experiments

Cons:
 - limited technical novelty
 - very weak links to the state-of-the-art, missing many key aspects of the research domain
"
iclr_2018_rJVruWZRW,"meta score: 4
This paper concerns a variant to previous RNN architectures using temporal skip connections, with experimentation on the PTB language modelling task
The reviewers all recommend that the paper is not ready for publication and thus should be rejected from ICLR.  The novelty of the paper and its relation to the state-of-the-art is not clear.  The experimental validation is weak.
Pros:
 - possibly interesting idea
Cons:
 - weak experimental validation
 - weak connection to the state of the art
 - precise original contribution w.r.t state-of-the-art is not clear
"
iclr_2018_SkYXvCR6W,"meta score: 4

The paper has been extensively edited during the review process - the edits are so extensive that I think the paper requires a re-review, which is not possible for ICLR 2018

Pros:
 - potentially interesting and novel approach to prefix encoding for character level CNN text classification
 - some experimental comparisons
Cons:
 - lacks good comparison with the state-of-the-art, which makes it difficult to determine conclusions
 - writing style lacks clarity.

I would recommend that the authors continue to improve the paper and submit it to a later conference.
"
iclr_2018_Sy5OAyZC-,"This work presents a strong baseline model for several NLP-ish tasks such as document classification, sentence classification, representation learning based NLI, and text matching. In terms of originality, reviewers found that ""there is not much contribution in terms of technical novelty"" but that ""one might also conclude that we need more challenging dataset"". There was significant discussion about whether it ""sheds new lights on limitations of existing methods"" or whether the results were ""marginally surprising"". In terms of quality, reviewers found it to be an ""insightful analysis"" and noted that these ""SWEMs should be considered a strong baseline in future work"".

There was significant discussion with the AC about the signficance of the work. In the opinion of the AC reviewers did were too quick to accept the authors novelty claims, and did not push them enough to include other baselines in their tables that were not overly deep model. In particular the AC felt that important numbers were left out of the experiment tables, for document classification that muddied the results. The response of the authors was:

""Moreover, fasttext and our SWEM variants all belong to the category of simpler methods (with parameter-free compositional functions). Since our motivation is to explore the necessity of employing complicated compositional functions for various NLP tasks, we do not think it is necessary for us to make any comparisons between fasttext and SWEM.""

In addition when a reviewer pointed out the lack of inclusion of FOFE embeddings, the authors noted something similar

""Besides, we totally agree that developing sentence embeddings that are both simple and efficient is a very promising research direction (FOFE is a great work along this line).""

The reviewer correctly pointed out related work that shows a model very similar to what the author's propose. In general this seems like evidence that the techniques are known, not that they are significant and novel. "
iclr_2018_Byht0GbRZ,"This work introduces a new type of structured attention network that learn latent structured alignments between sentences in a fully differentiable manner, which allows the network to learn not only the target task, but also the latent relationships. Reviewers seem partial to the idea of the work, and it's originality, but have issues with the contributions. In particular:

- The reviewers note that the gains in performance from using this approach are quite small and do not outperform previous structured approaches."
iclr_2018_SJZsR7kCZ,"This paper presents a new pipeline for nn compression that extends that of Han et. al, but show that it reduces parameters further, maintains higher accuracy and can be applied to methods behind classification (semantic segmentation). While the authors found the paper clearly written, excepting for some typos, and potentially useful, there were questions about originality, and significance.

- Reviewers were not completely convinced the method was different enough from deep compression: ""The overall pipeline including the last two stage looks quite similar to Han[1]."", or that enough focus was paid to the differences inherent with classification focused work: ""The paper in the title and abstract refers to segmentation as the main area of focus. However, there does not seem to be much related to it except an experiment on the CityScapes dataset.""

- In terms of impact, the additional benefits from pruning seem to require a significant amount of computation, and the reviewers were not convinced these were worth a small gain in compression. Furthermore, authors felt that this approach was not being applied to the most state-of-the-art approaches to demonstrate their use."
iclr_2018_BkQCGzZ0-,"This paper presents a different method for learning autoencoders with discrete hidden states (compared to recent discrete-like VAE type models). The reviewers in general like the method being proposed and are convinced that there is worth to the underlying proposal. However there are several shared complaints about the setup and writing of the paper.

- Several reviewers complained about the use of qualitative evaluation, particularly in the ""Deciphering the latent code"" section of the paper.
- One reviewer in particular had significant issues with the experimental setup of the paper and felt that there was insignificant quantitative evaluation, particularly using standard metrics for the task (compared to the metric introduced in the paper).
- There were further critiques about the ""procedural"" nature of the writing and the lack of formal justifications for the ideas introduced. "
iclr_2018_Bkl1uWb0Z,"In this work reviewers use structured attention as a way to induce grammatical structure in NMT models. Reviewers liked th motivation of the work and found experiments mostly well done. However reviewers found the paper a bit difficult to follow, with several commenting that distinctions made between the different sub types of attention were not clear. Mainly the reviewers were not overwhelmed by the results of the work, saying that these gains, while clearly isolated to the use of structure were not significantly large. Additionally there were some concerns about the claimed novelty of the work, particularly compared to Liu and Lapata and other use of syntax in translation, and also which aspects were new or necessary. "
iclr_2018_Syx6bz-Ab,"This paper introduces a new dataset and method for a ""semantic parsing"" problem of generating logical sql queries from text.  Reviews generally seemed to be very impressed by the dataset portion of the work saying ""the creation of a large scale semantic parsing dataset is fantastic,"" but were less compelled by the modeling aspects that were introduced and by the empirical justification for the work.  In particular:

- Several reviewers pointed out that the use of RL in particularly this style felt like it was ""unjustified"", and that the authors should have used simpler baselines as a way of assessing the performance of the system, e.g. ""There are far simpler solutions that would achieve the same result, such as optimizing the marginal likelihood or even simply including all orderings as training examples""

- The reviewers were not completely convinced that the authors' backed up their claims about the role of this dataset as a novel contribution. In particular there were questions about its structure, e.g. ""dataset only covers simple queries in form of aggregate-where-select structure"" and about comparisons with other smaller but similar datasets, e.g.  ""how well does the proposed model work when evaluated on an existing dataset containing full SQL queries, such as ATIS""

There was an additional anonymous discussion about the work not citing previous semantic parsing datasets. The authors noted that this discussion inappropriately brought in previous private reviews. However it seems like the main reviewers issues were orthogonal to this point, and so it was not a major aspect of this decision. "
iclr_2018_BJInMmWC-,"This paper presents a novel model for generating images and natural language descriptions simultaneously. The aim is to distangle representations learned for image generation by connecting them to the paired text. The reviews praise the problem setup and the mathematical formulation. However they point out significant issues with the clarity of the presentation in particular the diagrams, citations, and optimization procedure in general. They also point out issues with the experimental setup in terms of datasets used and lack of natural images for the tasks in question.  Reviews are impressively thorough and should be of use for a future submission. "
iclr_2018_r1Zi2Mb0-,"This paper extends work on neural architecture search by introducing a new framework for searching and experiments on new domains of NMT and QA. The results of the work are  beneficial and show improvements using this approach. However the reviewers point out significant issues with the approach itself:

- There is skepticism about the use of NAS in general, particular compared to using the same computational power for other types of simpler hyperparameter search.
- There is general concern about the use of such large scale brute force methods in general. Several of the reviewers expressed concerns about ever possibly being able to replicate these results.
- Given the computational power required, the reviewers feel like the gains are not particularly large, for instance the Squad results not being compared to the best reported systems.

"
iclr_2018_S1347ot3b,"This work is interested in using sentence vector representations as a method for both doing extractive summarization and as a way to better understand the structure of vector representations. While the methodological aspects utilize representation learning, the reviewers felt that the main thrust of the work would be better suited for a summarization workshop or even NLP venue, as it did not target DL based contributions. Additionally they felt that the work did not significantly engage with the long literature on the problem of summarization."
iclr_2018_BJDEbngCZ,The paper studies the global convergence for policy gradient methods for linear control problems.  Multiple reviewers point out strong concerns about the novelty of the results.
iclr_2018_SJICXeWAb,The reviewers point out that most of the results are already known and are not novel. There are also issues with the presentation. Studying only depth 2 and depth 3 networks is very limiting.
iclr_2018_rJR2ylbRb,The reviewers present strong concerns about the lack of novelty in the paper. Further there are strong concerns about how the experiments are conducted. I recommend the authors to carefully go through the reviews.
iclr_2018_r1CE9GWR-,"While the reviewers agree that this is an important topic, there are numerous concerns novelty, correctness and limitations. "
iclr_2018_HyY0Ff-AZ,The reviewers point out that this is a well known result and is not novel.
iclr_2018_BJcAWaeCW,"The reviewers present strong concerns regarding presentation of the paper. The approach appears overly complex, some design choices are not clear and the experiments are not conducted properly. I recommend the authors to carefully go through the reviews."
iclr_2018_HyEi7bWR-,"The authors use the Cayley transform representation of an orthogonal matrix to provide a parameterization of an RNN with orthogonal weights.   The paper is clearly written and the formulation is simple and elegant.  However,  I share the concerns of reviewer 3 about the significance of another method for parameterizing orthogonal RNN, as there has not been a lot of evidence that these have been useful on real problems (and indeed, on most of the toys used show the value of orthogonal RNN, one can get good results just by orthogonal initialization, e.g. as in Henaff et. al. as cited in this work).    This work does not compare experimentally against many of the other methods, e.g. https://arxiv.org/pdf/1612.00188.pdf,  the two Jing et. al. works cited, simple projection methods (either full projections at each step or stochastic projections as in Henaff et. al.).  It does not cite or compare against the approach in https://arxiv.org/pdf/1607.04903.pdf.  "
iclr_2018_B1EPYJ-C-,"The authors study the problem of reducing uplink communication costs in training a ML model where the training data is distributed over many clients.   The reviewers consider the problem interesting, but have concerns about the extent of the novelty of the approach.  As the reviewers and authors agree that the paper is an empirical study, and the authors agree that the novelty is in the problem studied and the combination of approaches used, a more thorough experimental analysis would
benefit the paper."
iclr_2018_r1SuFjkRW,"The reviewers consider the paper to promising, but raise issues with the increase in the complexity of the MDP caused by the authors' parameterization of the action space, and comparisons with earlier work (Pazis and Lagoudakis).   While the authors cite this work, and say that they that they needed to make changes to PL to make it work in their setting (in addition to adding the deep networks), they do not explicitly show comparisons in the paper to any other discretization schemes.   "
iclr_2018_HyXBcYg0b,"The authors make an experimental study of the relative merits of RNN-type approaches and graph-neural-network approaches to solving node-labeling problems on graphs.   They discuss various improvements in gnn constructions, such as residual connections.

This is a borderline paper.  On one hand, the reviewers feel that there is a place for this kind of empirical study, but on the other, there is agreement amongst the reviewers that the paper is not as well written as it could be.  Furthermore, some reviewers are worried about the degree of novelty (of adding residual connections to X).

I will recommend  rejection, but urge the authors to clarify the writing and expand on the empirical  study and resubmit. "
iclr_2018_HyI5ro0pW,"The authors propose a technique for weight pruning that leaves block diagonal weights, instead of unstructured sparse weights, leading to faster inference.  However, the experiments demonstrating the quality of the pruned models are insufficient.   The authors also discuss connections to random matrix theory; but these connections are not worked out in detail."
iclr_2018_SJ1fQYlCZ,"The authors give evidence that is certain cases, the ordering of sample inclusion in a curriculum is not important.  However, the reviewers believe the experiments are inconclusive, both in the sense that as reported, they do not demonstrate the authors' hypothesis, and that they may leave out many relevant factors of variation (such as hyper-parameter tuning). "
iclr_2018_rJaE2alRW,"The reviewers feel that the novelties in the model are not significant.   Furthermore, they suggest that empirical results could be improved by
1:  analyses showing how the significance network functions and directly measuring its impact
2: More reproducible experiments.  In particular, this is really an applications paper, and the experiments on the main application are not reproducible because the data is proprietary.
3: baselines that make assumptions more in line with the authors' problem setup"
iclr_2018_B1KFAGWAZ,"The authors present a centralized neural controller for multi-agent reinforcement learning.   The reviewers are are not convinced that there is sufficient novelty, considering the authors setup as essentially a special case of other recent works, with added adjustments to the neural-networks that are standard in the literature.

I personally am more bullish about this paper than the reviewers, as I think engineering an architecture to perform well in interesting scenarios is worth reporting.  However, the reviewers are mostly in agreement, and their reviews were neither sloppy nor factually incorrect.  So I will recommend rejection, following their judgement.

Nevertheless, I encourage the authors to continue strengthening the results and the presentation and resubmit.  "
iclr_2018_S1q_Cz-Cb,"While the reviewers considered the basic idea of adding supervision intermediate to differentiable programming style architectures to be interesting and worthy of effort, they were unsure if
1: the proposed abstractions for discussing ntm and nram are well motivated/more generally applicable
2: the methods used in this work to give intermediate supervision are more generally applicable
 "
iclr_2018_HkMCybx0-,"The authors introduce a new activation function which is similar in shape to ELU, but is faster to compute.   The reviewers consider this to not be a significant innovation because the amount of time spent in computing the activation function is small compared to other neural network operations."
iclr_2018_SJxE3jlA-,"The authors show evidence that an RL agent with a new neural architecture with an external memory is superior on a version of the concentration game to a baseline.   However, other works have proposed neural architectures with episodic memories, and the reviewers feel that the proposed model was not adequately compared to these.  Furthermore, there are concerns about the novelty of the proposed model."
iclr_2018_Sk1NTfZAb,"While the reviewers feel there might be some merit to this work,  they find enough ambiguities and inaccuracies that I think this paper would be better served by a resubmission."
iclr_2018_Bk_fs6gA-,"The authors use a memory-augmented neural architecture to learn solve combinatorial optimization problems.  The reviewers consider the approach worth studying, but find the authors' experimental protocol and baselines flawed.  "
iclr_2018_r17Q6WWA-,"The experimental work in this paper leaves it just short of being suitable for acceptance.
The work needs more comparisons with prior work and other approaches.
The numerical ratings of the work by reviewers are just too low.
"
iclr_2018_SJCPLLpaW,While this paper has some very interesting ideas the majority view of the reviewers and their aggregate numerical ratings are just too low to warrant acceptance.
iclr_2018_H1srNebAZ,"While one reviewer did upgrade their Rating from 6 to 7, the most negative reviewer maintains: ""Overall, I find this work interesting and current results surprising. However, I find it to be a preliminary work and not yet ready for publication. The paper still lacks a conclusion / a leading hypothesis / an explanation for the shown results. I find this conclusion indispensable even for a small scientific study to be published."" after the rebuttal. With scores of 7-5-4 it is just not possible for the AC to recommend acceptance."
iclr_2018_r16Vyf-0-,This paper had some quality and clarity issues and the lack of motivation for the approach was pointed out by multiple reviewers.  Just too far away from the acceptance threshold.
iclr_2018_HklpCzC6-,The experimental work was seen as one of the main weaknesses.
iclr_2018_SkymMAxAb,"This is an interesting application area, but the quality of the presentation and experimental work here is not sufficient for acceptance. The numerical ratings from reviewers are just not high enough to warrant acceptance. "
iclr_2018_r1nzLmWAb,All reviewers believed that the novelty of the contribution was limited.
iclr_2018_ByZmGjkA-,"This paper resulted in significant discussion -- both between R2 and the authors, and between the AC, PCs, and other solicited experts.

The problem of language grounding (and instruction following) in virtual environments is clearly important, this work was one of the first in the recent resurgence, and the goal of understand what the agents have learned is clearly noble and important. In terms of raw recommendations, the majority reviewer recommendation is negative, but since concerns raised by R2 seemed subjective (which in principle is not a problem), out of abundance of caution, we solicited additional input. Unfortunately, we received feedback consistent with the concerns raised here:

-- The lack of generality of the behavior found. Even if we ignore the difficult question of why the agent prefers what it does, it's unclear how the conclusions here generalize much farther than the model and environment used; the manuscript does not provide any novel or transferable principals of the form ""this kind of bias in the environment leads to this kind of bias in models with these properties"".

-- We realize even providing that concrete a statement might be hard, but also missing are thorough comparisons to other kinds of models (e.g. non-deep, as asked by R1) to establish that this is a general phenomenon.

Ultimately, there is a sense that this is too narrow an analysis, too soon. If there was one architecture for learning embodied agents in 3d environments that was clearly successful and useful, then studying its properties might be interesting (even crucial).  But the dust in this space isn't settled. Our current agents are fairly poor, and so the impact of understanding the biases of a specific model trained in a specific environment seems fairly low.

Finally -- this not taken into consideration in making the decision -- it is not okay to list personal homepage domains (that may reveal author identity to ACs) as conflict domains; those are meant for institutional conflicts/domains. "
iclr_2018_SkBcLugC-,"The manuscript proposes a simple technique for adaptive ensemble prediction. Unfortunately, several significant concerns were raised (by R2 and R3) that this AC agrees with. Both R2 and R3 asked fairly specific questions and requested follow-up experiments, which have not been addressed. "
iclr_2018_rJe7FW-Cb,"This paper received borderline reviews. Initially, all reviewers raised a number of concerns (clarity, small improvements, etc). Even after some back and forth discussion, concerns remain, and it's clear that while the idea has potential, another round of reviewing is needed before a decision can be reached. This would be a major revision in a journal. Unfortunately, that is not possible in a conference setting and we must recommend rejection. We recommend the authors to use the feedback to make the manuscript stronger and submit to a future venue. "
iclr_2018_BkiIkBJ0b,"This paper received divergent ratings (7, 3, 3). While there is value in thorough evaluation papers, this manuscript has significant presentation issues. As all three reviewers point out, the way it is currently written, it misrepresents the claims made by Mirowski et al 2016 and over-reaches in its findings. Unfortunately, we cannot make a decision on what the manuscript may look like in future once these issues are fixed, and must reject. "
iclr_2018_HJPSN3gRW,"This paper was reviewed by 3 expert reviews and received largely negative reviews, with concerns about the toy-ish nature of the 2D environments and limited novelty.

Since ICLR18 received multiple papers on similar topics, we took additional measures to ensure that papers were similar papers were judged under the same criteria. Specifically, we asked reviewers of (a) this paper and (b) of a concurrent submission that also studies language grounding in 2D environments to provide opinions on (b) and (a) respectively. Unfortunately, while they may be on similar topic and both working on 2D environments, we received unanimous feedback that (b) was much higher quality (""comparison with multiple baselines, better literature review, no bold claims about visual attention, etc). We realize this may be disappointing but we encourage the authors to incorporate reviewer feedback to make their manuscript stronger. "
iclr_2018_HJNGGmZ0Z,"Paper reviewed by three experts who have provided detailed feedback. All three recommend rejection, and this AC sees no reason to overrule their recommendation. "
iclr_2018_rkWN3g-AZ,This paper was reviewed by 3 expert reviewers. All three recommend rejection citing significant concerns (e.g. missing baselines).
iclr_2018_BJDH5M-AW,"This paper studies the problem of synthesizing adversarial examples that will succeed at fooling a classification system under unknown viewpoint, lighting, etc conditions. For that purpose, the authors propose a data-augmentation technique (called ""EOT"") that makes adversarial examples robust against a predetermined family of transformations.

Reviewers were mixed in their assessment of this work, on the one hand highlighting the potential practical applications, but on the other hand warning about weak comparisons with existing literature, as well as lack of discussion about how to improve the robustness of the deep neural net against that form of attacks.
The AC thus believes this paper will greatly benefit from a further round of iteration/review, and therefore recommends rejection at this time. "
iclr_2018_S1680_1Rb,"This paper considers graph neural representations that use Cayley polynomials of the graph Laplacian as generators. These polynomials offer better frequency localization than Chebyshev polynomials. The authors illustrate the advantages of Cayleynets on several benchmarks, producing modest improvements.

Reviewers were mixed in the assessment of this work, highlighting on the one hand the good quality of the presentation and the theoretical background, but on the other hand skeptical about the experimental section significance. In particular, some concerns were centered about the analysis of complexity of Cayley versus the existing alternatives.

Overall, the AC believes this paper is perhaps more suited to an audience more savvy in signal processing than ICLR, which may fail to appreciate the contributions. "
iclr_2018_SkaPsfZ0W,"This paper proposes a multiscale variant of Graph Convolutional Networks (GCN) , obtained by combining separate GCN modules using powers of normalized adjacency as generators. The model is tested on several node classification semi-supervised tasks obtaining excellent numerical performance.

Reviewers acknowledged the good empirical performance of the model, but all raised the issue of limited novelty, relative to the growing body of literature on graph neural networks. In particular, they missed an analysis that compares random walks powers to other multiscale approaches and justifies its performance in the context of semi-supervised learning. Overall, the AC believes this is a good paper, but it can be significantly stronger with an extra iteration that addresses these limitations. "
iclr_2018_SyBBgXWAZ,"This paper exposes a simple recipe to manipulate the latent space of generative models in such a way to minimize the mismatch between the prior distribution and that of the manipulated latent space. Manipulations such as linear interpolation are commonplace in the literature, and this work will be helpful to improve assessment on that front.

Reviewers found this paper interesting, yet unpolished and incomplete. In subsequent iterations, the paper has significantly improved on those fronts, however the AC believes an extra iteration will make this work even more solid. Thus, unfortunately this paper cannot be accepted at this time. "
iclr_2018_H139Q_gAW,"This paper proposes to combine Depthwise separable convolutions developed for 2d grids with recent graph convolutional architectures. The resulting architecture can be seen as learning both node and edge features, the latter encoding node similarities with learnt weights.
Reviewers agreed that this is an interesting line of work, but that further work is needed in both the presentation and the experimental front before publication. In particular, the paper should also compare against recent models (such as the MPNN from Gilmer et al) that also propose edge feature learning. THerefore, the AC recommends rejection at this time.
"
iclr_2018_SyjsLqxR-,"This paper studies to what extent adversarial training affects the properties of adversarial examples in object classification.

Reviewers found the work going in the right direction, but agreed that it needs further evidence/focus in order to constitute a significant contribution to the ICLR community. In particular, the AC encourages authors to relate their work to the growing body of (mostly concurrent) work on robust optimization and adversarial learning. For the above reasons, the AC recommends rejection at this time. "
iclr_2018_Hki-ZlbA-,"This paper describes a method to generate provably 'optimal' adversarial examples, leveraging the so-called 'Reluplex' technique, which can evaluate properties of piece-wise linear representations.
Reviewers agreed that incorporating optimality certificates into adversarial examples is a promising direction to follow, but were also concerned about the lack of empirical justification the current paper provides and missed discussion about the relevance of choosing Lp distances. They all recommended pushing experiments to more challenging datasets before the paper can be accepted, and the AC shares the same advice.
"
iclr_2018_SyqAPeWAZ,"This paper addresses the question of how to solve image super-resolution, building on a connection between sparse regularization and neural networks.
Reviewers agreed that this paper needs to be rewritten, taking into account recent work in the area and significantly improving the grammar. The AC thus recommends rejection at this time. "
iclr_2018_HklZOfW0W,"This paper addresses the problem of learning neural graph representations, based on graph filtering techniques in the vertex domain.

Reviewers agreed on the fact that this paper has limited interest in its current form, and has serious grammatical issues. The AC thus recommends rejection at this time. "
iclr_2018_H11lAfbCW,"This paper attempts to connect the expressivity of neural networks with a measure of topological complexity. The authors present some empirical results on simplified datasets.
All reviewers agreed that this is an intriguing line of research, but that the current manuscript is still presenting preliminary results, and that further work is needed before it can be published. "
iclr_2018_Bk7wvW-C-,"here, yet another sentence representation method is proposed. i agree with R1 and R3 that this does not contribute significantly to be a full-length conference paper."
iclr_2018_ryHM_fbA-,"there are two separate ideas embedded in this submission; (1) language modelling (with the negative sampling objective by mikolov et al.) is a good objective to use for extracting document representation, and (2) CNN is a faster alternative to RNN's, both of which have been studied in similar contexts earlier (e.g., paragraph vectors, CNN classifiers and so on, most of which were pointed out by the reviewers already.) Unfortunately reading this manuscript does not reveal too clearly how these two ideas connect to each other (and are separate from each other) and are related to earlier approaches, which were again pointed out by the reviewers. in summary, i believe this manuscript requires more work to be accepted."
iclr_2018_ByUEelW0-,"the idea is interesting, but as pointed out the reviewers (and also agreed by the authors), the current manuscript lacks clear motivations, reasons underlying specific design choices and convincing empirical evaluation. "
iclr_2018_Syt0r4bRZ,"the problem is interesting, and the reviewers acknowledge it's worth an effort to tackle. unfortunately all the reviewers found the work to be too preliminary without a convincing evidence supporting the proposed approach against other alternatives (or on its own.)"
iclr_2018_Hk2MHt-3-,"The paper studies end-to-end training of a multi-branch convolutional network. This appears to lead to strong accuracies on the CIFAR and SVHN datasets, but it remains unclear whether or not this results transfers to ImageNet. The proposed approach is hardly novel, and lacks a systematic comparison with ""regular"" ensembling methods and with related mixture-of-experts approaches (for instance: S. Gross et al. Hard Mixtures of Experts for Large Scale Weakly Supervised Vision, 2017; Shazeer et al. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, 2017)."
iclr_2018_SJLy_SxC-,"The paper presents an empirical study into sparse connectivity patterns for DenseNets.

Whilst sparse connectivity is potentially interesting, the paper does not make a strong argument for such sparse connectivity patterns: in particular, the results on ImageNet suggest that sparse connectivity performs substantially worse than full connectivity (at the same FLOPS-level, Log-DenseNet obtains ~2.5% lower accuracy than baseline DenseNet models, and the best Log-DenseNet is ~4% worse than the best DenseNet). On CamVid, both network architectures appear to perform on par.

The paper motivates the model architecture by the high memory consumption of DenseNets but, frankly, that is a very weak motivation: DenseNets are actually very memory-efficient if implemented correctly (https://arxiv.org/pdf/1707.06990.pdf). The fact that such implementations are not well-supported by TensorFlow/PyTorch is a shortcoming of those deep-learning frameworks, not in DenseNets. (In fact, the memory management features that deep-learning frameworks have implemented to make residual networks memory-efficient (for instance, caching GPU memory allocation in PyTorch) are far more complex than the ""thousand lines of C++"" currently needed to implement a DenseNet correctly.) Such issues will likely be resolved relatively soon by better implementations, and are hardly a good motivation for a different network architecture."
iclr_2018_HknbyQbC-,"The paper presents AdvGAN: a GAN that is trained to generate adversarial examples against a convolutional network. The motivation for this method is unclear: the proposed attack does not outperform simpler attack methods such as Carlini-Wagner attack. In white-box settings, a clear downside for the attacker is that it needs to re-train its GAN everytime the defender changes its convolutional network.

More importantly, the work appears preliminary. In particular, the lack of extensive quantitative experiments on ImageNet makes it difficult to compare the proposed approach to alternative attacks methods such as (I-)FGSM, DeepFool, and Carlini-Wagner. The fact that AdvGAN performs well on MNIST is nice, but MNIST should be considered for what it is: a toy dataset. If AdvGANs are, as the authors state in their rebuttal, fast and good at generating high-resolution images, then it should be straightforward to perform comprehensive experiments with AdvGANs on ImageNet (rather than focusing on a small number of images on a single target, as the authors did in their revision)?"
iclr_2018_rkA1f3NpZ,"The paper empirically evaluates the effectiveness of ensembles of deep networks against adversarial examples. The paper adds little to the existing literature in this area: an detailed study on ""ensemble adversarial training"" already exists, and the experimental evaluation in this paper is limited to MNIST and CIFAR (results on those datasets do not necessarily transfer very well to much higher-dimensional datasets such as ImageNet). Moreover, the reviewers identify several shortcomings in the experimental setup of the paper. "
iclr_2018_HkeJVllRW,"The paper studies factorizations of convolutional kernels. The proposed kernels lead to theoretical and practical efficiency improvements, but these improvements are very, very limited (for instance, Figure 5). It remains unclear how they compare to popular alternative approaches such as group convolutions (used in ResNeXt) or depth-separable convolutions (used in MobileNet). The reviewers identify a variety of smaller issues with the manuscript."
iclr_2018_ByqFhGZCW,"The paper studies a adversarial attacks and defenses against convolutional networks based on a minimax formulation of the problem. Whilst this is an interesting direction of research, the present paper seems preliminary. In particular, compared to several other independent ICLR submissions, the empirical evaluation is quite weak: it does not consider the strongest known gradient-based attack (Carlini-Wagner) as baseline and does not report results on ImageNet. The reviewers identify several issues related to Lemma 1 and to the clarity of presentation."
iclr_2018_ryvxcPeAb,"The paper studies transferability of adversarial examples between model architectures, and proposes a method to improve this transferability. Whilst it covers an interesting and relevant line of research, the paper does not provide strong evidence for its main underling hypothesis: namely, that adversarial perturbations can be split in a model-specified and a data-specific part. The paper's presentation also warrants improvements. The authors have not provided a rebuttal."
iclr_2018_HJdXGy1RW,"The paper proposes a new convolutional network architecture, called CrescendoNet. Whilst achieving competitive performance on CIFAR-10 and SVHN, the accuracy of the proposed model on CIFAR-100 is substantially lower than that of state-of-the-art models with fewer parameters; the paper presents no experimental results on ImageNet. The proposed architecture does not provide clear new insights or successful new design principles. This makes it unlikely the current manuscript will have a lot of impact."
iclr_2018_Byj54-bAW,"The paper performs a theoretical analysis of the representation power of convolutional networks with inter-layer connections. Whilst the results themselves are interesting, the current presentation of the paper stands in the way of the reading grasping and appreciating the main insights from the paper.

The authors acknowledge these issues in their rebuttal, but have not yet revised the paper to resolve them. I encourage the authors to revise the paper to address the reviewer comments, and re-submit it to another venue. "
iclr_2018_B16_iGWCW,"The paper presents a boosting method and uses it to train an ensemble of convnets for image classification. The paper lacks conceptual and empirical comparisons with alternative boosting and ensembling methods. In fact, it is not even clear from the experimental results whether or not the proposed method outperforms a simple baseline model that averages the predictions of T independently trained convolutional networks."
iclr_2018_rkmtTJZCb,"The paper presents a method for forward prediction in videos. The paper insufficiently motivates the proposed method and presents very limited empirical evaluations (no ablation studies, etc.) to backup its claims. This makes it difficult for the reader to put the work into  the context of the broader research around learning from unsupervised video data; leading reviewers to complete about perceived lack of novelty and clarity."
iclr_2018_Hk-FlMbAZ,"The original paper was sloppy in its use of mathematical constructs such as manifolds, made assumptions that are poorly motivated (see review #2 for details), and presented an empirical evaluation is preliminary. Based on the reviews, the authors have substantially revised the paper to try and address those issues by adding new theory, etc.

Unfortunately, it is difficult to assess whether these revisions are sufficient to address the aforementioned issues without going through a second round of ""full"" review. I encourage the authors to use the reviewer comments to further improve the paper, and re-submit to a different venue."
iclr_2018_rJbs5gbRW,"The paper appears unfinished in many ways: the experiments are preliminary, the paper completely ignored a large body of prior work on the subject, and the presentation needs substantial improvements. The authors did not provide a rebuttal.

I encourage the authors to refrain from submitting unfinished papers such as this one in the future, as it unnecessarily increases the load on a review system that is already strained."
iclr_2018_SJCq_fZ0Z,"The authors propose to use attention over past time steps to try and solve the gradient flow problem in learning recurrent neural networks. Attention is performed over a subset of past states by a hueristic that boils to selecting best time-steps.

I agree with the authors that they offer a lot of comparisons, but like the reviewers, I am inclined to find the experiments not very convincing of the arguments they are attempting to make.  The model that they propose has similarities to seq2seq in that they use attention to pass more information in the forward pass; in a sense this is a seq2seq model with the same encoder and decoder, and there are parallels to self-attention. The model also has similarities to clockwork RNNs and other skip connection methods.. However, the experiments offered to not tease out these effects. It is unsurprising that a fixed size neural network is unable to do a long copy task perfectly, but an attention model can. What would have been more interesting would have been to explore if other RNN models could have done so. The experiments on pMNIST aren't really compelling as the baselines are far from SOTA (example: https://arxiv.org/pdf/1606.01305.pdf report 0.041 error rate (95.9% test acc) with LSTMs and regularization).  Text8 also shows worse results in full BPTT on LSTM.  If BPTT is consistently better than this method, it defeats the argument that gradient explosion and forgetting over long sequences is a problem for RNNs (one of the motivations offered for this attention model).
"
iclr_2018_HJMN-xWC-,"I am inclined to agree with R1 that there is an extensive literature on learning architectures now, and I have seen two others as part of my area chairing. This paper does not offer comparisons to existing methods for architecture learning other than very basic ones and that reduces the strength of the paper significantly. Further the broad exploration over 17 tasks is more overwhelming, than adding to an insight into the methods."
iclr_2018_Hyp3i2xRb,"The authors propose to use identity + some weights in the recurrent connections to prevent vanishing gradients. The reviewers found the experiments to have weak baselines, weakening the claims of the paper."
iclr_2018_BJ78bJZCZ,"RDA improves on RWA, but even so, the model is inferior to the other standard RNN models. As a result R1 and R3 question the motivation for the use of this model -- something the authors should motivate."
iclr_2018_HJ8W1Q-0Z,"The reviewers agree that while the presented result looks interesting, it is but one result. Further, one of the reviewer finds this to be a weak comparison as well.
The novelty of the approach over the paper by Ba et. al. also is in question -- good results on multiple tasks might have made it worth exploring, but the authors did not establish this to be the case convincingly."
iclr_2018_rJ1RPJWAW," + The paper proposes an interesting empirical measure of """"learnability"""" of a trained network: how well the predictive function it represents can be learned by another network. And shows it empirically seems to correlate with better generalization.
 - The work is purely empirical: it features no theory relating this learnability to generalization
 - Learnability measure is somewhat ad-hoc with moving parts left to be specified (learning network, data splits, ...)
 - as pointed out by a reviewer, learnability doesn't really provide any answers for now.
 - the work would be much stronger if it went beyond a mere correlation study, and if learnability considerations allowed to derive a new approach/regularization scheme that was convincingly shown to improve generalization.
"
iclr_2018_HylgYB3pZ,"The paper identifies an interesting problem in sigmoid deep nets, addressed diffferently by batchnorm, and proposes a different simple fix. It shows empirically that constraining neuron's weights to sum to zero improves training of a 100 layers sigmoid MLP.
The work is currenlty limited in its theoretical contribution, and regarding the showcased practical interest of the method compared to batchnorm (it's not appplicable to RELUs and shows positive effect on optimization but not generalization).
 "
iclr_2018_SkHkeixAW,"The paper is a well-written review of regularization approaches in deep learning.
It does not offer novel approaches or novel insight with empirically demonstrated usefulness
=> ICLR is not the appropriate venue for it."
iclr_2018_r111KtCp-," + interesting approach for a detailed analysis of the limitations of autoencoders in solving a simple toy problem
 - resulting insights somewhat trivial, not really novel, nor practically useful => lacks demonstration of a gain on non-toy task
 - regularization study too limited in scope: lacking theoretical grounding, and more exhaustive comparison of regularization schemes."
iclr_2018_rkhxwltab,"The paper proposes to use absolute value activations, in a joint supervised + unsupervised training (classification + deep autoencoder with tied encoder/decoder weights).
Pros:
 + simple model and approach on ideas worth revisiting
Cons:
- The paper initially approached these old ideas as novel, missing much related prior work
- It doesn't convincingly breathe novel insight into them.
- Empirical methodology is not up to standards (non-standard data split, lack of strong baselines for comparison)
- Empirical validation is too limited in scope (MNIST only)."
iclr_2018_Hyp-JJJRW," + Paper proposes simple joint deep autoencoder + classifier training where the hidden representation is split between (observed) class and (unobserved) style nodes.
 - Empirical evaluation is very limited, focusing on only qualitative evaluation of reconstructions and interpolations (on MNIST and EMNIST).
 - Unclear goal: if it is improving classifier robustness, then quantitative classifier robustness improvements should be experimentally demonstrated. If it is as a (conditional) generative model, then it should be compared to strong generative baselines (in the GVAE or GAN families). The paper currently has neither."
iclr_2018_By0ANxbRW,"Proposed network compression method offers limited technical novelty over existing approaches, and empirical evaluations do not clearly demonstrate an advantage over current state-of-the-art.
Paper presentation quality also needs to be improved. "
iclr_2018_SkiCjzNTZ,"The paper makes overly strong claims, too weakly supported by a hard to follow and insufficiently rigorous mathematical argument. Connections with a large body of relevant prior literature are missing."
iclr_2018_SJa1Nk10b,"The paper received mixed reviews with scores of 5 (R1), 5 (R2),  7 (R3).  All three reviewers raise concerns about the lack of comparisons to other methods. The rebuttal is not compelling on this point. There are quite a few methods that could be used for this application available (often with source code) and should be compared to, e.g. DenseNets (Huang et al.). Given that the proposed method isn't in of itself hugely novel, a thorough experimental evaluation is crucial to the justifying the approach. The AC has closely looked at the rebuttal and the paper and feels that it cannot be accepted for this reason at this time. "
iclr_2018_B1spAqUp-,"The paper received borderline-negative reviews with scores of 5,5,6. A consistent issue was the weakness of the experiments: (i) lack of comparison to appropriate baselines, (ii) differences between published/reported numbers for DeepLab-ResNet (R3) and (iii) related work, e.g. Wojna paper, as raised by R1. The AC did not find the author's responses to these issues convincing. For (ii) the gap between 73 and 79 is large and the author's explanation for the difference doesn't seem plausible. For (iii), the response promised comparisons/discussion but there were not added to the draft.

Given this, the paper cannot be accepted in it current form. The experiments should be improved before the paper is resubmitted. "
iclr_2018_ryzm6BATZ,"The paper received borderline-negative scores (6,5,5) with R1 and R2 having significant difficulty with the clarity of the paper. Although R3 was marginally positive, they pointed out that the experiments are ""extremely weak"". The AC look at the paper and agrees with R3 on this point. Therefore the paper cannot be accepted in its current form. The experiments and clarity need work before resubmission to another venue.  "
iclr_2018_BJjBnN9a-,"The paper received borderline negative scores: 5,6,4.

The authors response to R1 question about the motivations was ""...thus can achieve similar classification results with much smaller network sizes. This translates into smaller memory requirements, faster computational speeds and higher expressivity."" If this is really the case, then some experimental comparison to compression methods (e.g. Song Han's PhD work at Stanford) is needed to back up this.

R4 raises issues with the experimental evaluation and the AC agrees with them that they are disappointing. In general R4 makes some good suggestions for improving the paper.

The author's rebuttal also makes the general point that the paper should be accepted as it contains ideas, that these are sufficient alone: ""We strongly believe that with some fine-tuning it could achieve considerably better results, however we also believe that this is not the point in a first submission..."". The AC disagrees with this. Ideas are cheap. *Good ideas*, i.e. those that work, as in get good performance on standard benchmarks are valuable however. The reason for having benchmarks is to give some of objective way of seeing if an idea has any merit to it. So while the reviewers and the AC accept that the paper has some interesting ideas, this is not enough for warrant acceptance. "
iclr_2018_rkQu4Wb0Z,"The paper received scores of 5,5,5, with the reviewers agreeing the paper was marginally below the acceptance threshold. The main issue, raised by both R2 and R3 was that connection between representation learning in deep nets and coding theory was not fully justified/made.  With no reviewer advocating acceptance, it is not possible to accept the paper unfortunately. "
iclr_2018_HkMhoDITb,"All three reviewers agreed that the paper was an interesting, giving a demonstration of what quantum computer could achieve. However, they all also felt that the topic was outside the main interests of the conference and better suited to other venues, e.g. a quantum computation workshop. The AC agrees with them. Thus unfortunately, the paper cannot be accepted."
iclr_2018_SksY3deAW,"All three reviewers felt that the paper was just below the acceptance threshold, with scores of 5,4,5. R1 felt there were problems in the proofs, but the authors rebuttal satisfactorily addressed this. R3 and the authors had an extended discussion with the authors, but did not revise their score from its initial value (5). R4 had concerns about the experimental evaluation, that wasn't fully addressed in the rebuttal. With no reviewers advocating acceptance, the paper will have to rejected unfortunately. "
iclr_2018_BJgPCveAW,"The paper received weak scores: 4,4,5. R2 complained about clarity. R3's point about the lack of fully connected layers in current SOA deepnets is very valid and the authors response far from convincing. Unfortunately the major revision provided by the authors was not commented on by the reviewers, but many of the major shortcomings of the work still remain.
Generally, the paper is below the acceptance threshold, so cannot be accepted."
iclr_2018_S1fHmlbCW,"The scores were not favorable: 5,5,2. R2 felt the motivation of the paper was inadequate. R3 raised numerous technical points, some of which were addressed in the rebuttal, but not all. R3 continues to have issue with some of the results. The AC agrees with R3's concerns and feels that the paper cannot be accepted in its current form. "
iclr_2018_SkrHeXbCW,The paper received three good quality reviews which were in agreement that the paper was below the acceptance threshold. The authors are encouraged to follow the suggestions from the reviews to revise the paper and resubmit to another venue.
iclr_2018_B1mAkPxCZ,"Two reviewers recommended rejection, and one was on the edge. There was no rebuttal to address the concerns and questions posed by the reviewers."
iclr_2018_HymYLebCb,"The main idea of the paper is to transform graph classification into image representation (via adjacency matrices). Two reviewers are positive, while one is negative. The concerns are novelty (as mentioned by R2), while the last reviewer thinks the method is too simple and unprincipled (here the AC agrees with authors that simple is not necessarily bad). Overall, none of the reviewers champions this paper. Due to many excellent submissions, unfortunately this paper cannot be accepted in present form."
iclr_2018_Skvd-myR-,"Two reviewers recommended rejection, and the last reviewer votes for acceptance. The authors provided a rebuttal, including the end-to-end experiment (although the AC agrees with the authors that this experiment is not crucial to the paper). The AC read the paper and the reviews. While there are clearly interesting aspects of this work, it somewhat falls short in terms of the technical contribution. Perhaps a better writing would alleviate this issue: for example, explaining the visual features is somewhat a distraction from the main point, and could be put in the end. The 3 stage training is somewhat ad hoc (or less elegant). Since there are many excellent papers submitted to ICLR this year, this paper unfortunately did not make it above the bar."
iclr_2018_SJw03ceRW,"Two reviewers recommended rejection, and one is slightly more positive. The main concern is that the experiments are not convincing (ie, the number of base and added classes is very small). Furthermore, while the paper introduces several interesting ideas, the AC agrees with the second reviewer that each of these could be explored in more detail. This work seems preliminary. The authors are encouraged to resubmit to a future conference."
iclr_2018_BJluxbWC-,Three reviewers recommended rejection and there was no rebuttal to overturn their recommendation.
iclr_2018_HJr4QJ26W,"The reviewers agree that the idea of incorporating humans in the training of generative adversarial networks is interesting and worthwhile exploring. However, they felt that the paper fell short in providing strong support for their approach. The AC agrees. The authors are encouraged to strengthen their work and resubmit to a future venue."
iclr_2018_SJVHY9lCb,Three reviewers recommended rejection and there was no rebuttal.
iclr_2018_HJDUjKeA-,All three reviewers recommended rejection and there was no rebuttal.
iclr_2018_SkAK2jg0b,"Three reviewers recommended rejection, and there was no rebuttal."
iclr_2018_BkoCeqgR-,Three reviewers recommend rejection and there is no rebuttal.
iclr_2018_Bym0cU1CZ,"This work takes dialogue acts into account to generate responses in a human-machine conversation. However, incorporating dialogue acts into open-domain dialogue was already the focus of Zhao et al's ACL 2017 paper, Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, and using dialogue acts in a policy for human-machine conversation was also an idea that already appeared in Serban et al 2017, A Deep Reinforcement Learning Chatbot. Despite the authors' response that tries to adjust their claims and incorporate a more thorough overview, I encourage the authors to re-work their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context."
iclr_2018_Bki1Ct1AW,"This work is incremental compared to previous work, solving very specific challenges, and would probably appeal to only a very limited fraction of ICLR's audience. "
iclr_2018_S1GUgxgCW,"This paper combines existing models to detect topics and generate responses, and the resulting model is shown to be slightly preferred by human evaluators over baselines. This is quite incremental and the results are not impressive enough to stand on their own merit."
iclr_2018_SkBHr1WRW,"This paper deals with the important topic of learning better graph representations and shows promise in helping to detect critical substructures of graph that would help with the interpretability of representations. Unfortunately, this work fails to accurately portray how it relates to previous work (in particular, Niepert et al, Kipf et al, Duvenaud et al) and falls short of providing clear and convincing explanations of what it can do that these models can't, without including all of them in experimental comparisons. "
iclr_2018_BJ6anzb0Z,"This work combines words and images from Tumblr to provide more fine-grained sentiment analysis than just positive-negative. The contribution is too slight, as a straightforward combination of existing architectures applied on an emotion classification task with conclusions that aren't well motivated and are not providing any comparison to existing related work on finer emotion classification."
iclr_2018_rJ7yZ2P6-,This paper's idea is to augment pre-trained word embeddings on a large corpus with embeddings learned on the data of interest. This is shown to yield better results than the pre-trained word embeddings alone. This contribution is too limited to justify publication at iclr.
iclr_2018_By5SY2gA-,"This work attempts to incorporate affect information from additional resources into word embeddings. This is a valuable goal, but the methods used are very similar to existing ones, and the experimental results are not quite convincing enough to make a strong enough case for accepting the paper."
iclr_2018_r17lFgZ0Z,"This paper tackles a very important problem: evaluating natural language generation. The paper presents an overview of existing unsupervised metrics, and looks at how they correlate with human evaluation scores. This is important work and the empirical conclusions are useful to the community, but the datasets used are too limited and the authors agree it would be better to use newer bigger and more diverse datasets suggested by reviewers for drawing more general conclusions. This work would indeed be much stronger if it relied on better, more recent datasets; therefore publication as is seems premature.
"
iclr_2018_ByhthReRb,"This work deals with the important task of capturing named entities in a goal-directed setting. The description of the work and the experiments are not ready for publication; for example, it is unclear whether the proposed method would have an advantage over existing methods such as the match type features that are only mentioned in Table 3 for establishing the baseline on the original bAbI dialogue dataset, but not even discussed in the paper."
iclr_2018_HJXyS7bRb,"While using self-play for training a goal-oriented dialogue system makes sense, the contribution of this paper compared to previous work (that the paper itself cites) seems too minor, and the limitations of using toy synthetic data further weaken the work.
"
iclr_2018_Syl3_2JCZ,"This work extends Druckmann and Chklowskii, 2012 and demonstrates some interesting properties of the new model. This would be of interest to a neuroscience audience, but the focus is off for ICLR."
iclr_2018_HJXOfZ-AZ,"This work looks at what factors can lead to the emergence of selectivity (to certain categories) in units of a neural network. While this is an intriguing area to explore, this work uses settings that are quite toy-ish, making it a very hard to see how the observations could generalize to more realistic architectures or tasks. "
iclr_2018_SJiHOSeR-,"This paper is lacking in terms of clarity and experimentation, and would require a lot of additional work to bring it to the standards of any high quality venue."
iclr_2018_BkpXqwUTZ,This paper is nowhere near standards for publication anywhere.
iclr_2018_BJ_QxP1AZ,"The paper builds on earlier work by Wang et al (2015) on Visual Concepts (VCs) and explores the use of VCs for few-shot learning setting for novel classes.

The work, as pointed out by two reviewers is somewhat incremental in nature, with main novelty being the demonstration of utilities of VCs for few shot learning. This would not have been a big limitation if the paper had a carefully conducted empirical evaluation providing insights on the effect of various configuration settings/hyperparameters on the performance in few shot learning, which two of the reviewers (Anon3, Anon2) state are missing. The paper falls short of the acceptance threshold in its current form.

PS: The authors posted a github link to the code on Jan 12 which may potentially compromise the anonymity of the submission (though it was after all the reviews were already in) https://openreview.net/forum?id=BJ_QxP1AZ&noteId=BJaIDpBEM
"
iclr_2018_B1i7ezW0-,"The paper proposes a novel approach for DNN inversion mainly targeted towards semi-supervised learning. However the semi-supervised learning results are not competitive enough. Although the authors mention in the author-response that semi-supervised learning is not the main goal of the paper, the experiments and claims of the paper are mainly targeted towards semi-supervised learning. As the approach for inversion is novel, the paper could be motivated from a different angle with appropriate supporting experiments. In its current form it's not suitable for publication. "
iclr_2018_ByzvHagA-,"The novelty of the paper is limited and it lacks on comparisons with relevant baselines, as pointed out by the reviewers. "
iclr_2018_SySpa-Z0Z,All reviewers have acknowledged that the proposed regularization is novel and also results in some empirical improvements on the reported language modeling and image classification tasks. However there are serious concerns on writing and rigor (reviewers Anon1 and Anon3) of the paper. The authors have not uploaded any revision of the paper to address these concerns.
iclr_2018_ryykVe-0W,"The paper proposes the use of GANs to match the joint distribution of features to the product of their marginals for ICA. The approach is totally plausible but reviewers have complaints about lack of rigor and analysis in terms of (i) mixing conditions under which the proposed GAN based approach will work, given that ICA is ill-posed for general nonlinear mixing  (ii) comparison with prior work on linear and PNL ICA.

Further, in most scenarios where GANs are used, one of the distributions is fixed (say, the real distribution) and the other is dynamic (fake distribution) trying to come close to the fixed distribution during optimization. In the proposed method, the discriminator encodes the distance b/w joint and product of marginals which are both dynamic during the learning. It might be useful to comment whether or not it has any implications wrt increased instability of training, etc. "
iclr_2018_ry0WOxbRZ,"Reviewers recognize that the method proposed is somewhat novel but have strong reservations on the experimental evaluation. Discussion of some relevant papers is also missing (eg, Li et al, 2017 : ALICE). The authors have not responded to the many concerns expressed by the reviewers. "
iclr_2018_S17mtzbRb,"The paper proposes two regularizers for encouraging ""clustered feature embeddings"" (use of ""disentangled"" in the title is misleading). Reviewers have raised points about the lack of proper motivation and justification of the regularizers. There are also concerns on the experiments conducted to evaluate the method, including for hierarchical classification setting. Missing comparison with relevant baselines has also been pointed out as a weakness. I feel the work is not yet mature. "
iclr_2018_SyYYPdg0-,All reviewers acknowledge that the idea of the paper is interesting but have expressed serious concerns on empirical evaluations. The paper is not suitable for publication in its current form.
iclr_2018_rJTGkKxAZ,Reviewers recognize the proposed method of hierarchical extension to ALI to be potentially novel and interesting but have expressed strong concerns on the experiments section. The paper also needs to have comparisons with relevant hierarchical generative model baselines. Not suitable for publication in its current form.
iclr_2018_rJL6pz-CZ,"Learning identity-preserving transformations from unlabeled data is definitely an important and useful direction. However the paper does not have convincing experiments to establish the effectiveness of the proposed method on real datasets which is a crucial limitation in my view, given that the paper is largely based on an earlier published work by Culpepper and Olshausen (2009). "
iclr_2018_HJaDJZ-0W,"Pros
-- Interesting approach to induce sparsity, trains faster than alternative approaches
Cons
-- Fairly complex set of heuristics for pruning weights
-- Han et al. works well, although the authors claim it takes more time to train, which may not not hold for all training sets and doesn’t seem like a strong enough reason to choose an alternative appraoch

Given these comments, the AC recommends that the paper be rejected.
"
iclr_2018_B1NGT8xCZ,"Pros
-- Nice way to formulate domain adaptation in a Bayesian framework that explains why autoencoder and domain difference losses are useful.

Cons
-- Model closely follows the framework, but the overall strategy is similar to previous models (but with improved rationale).
-- Experimental section can be improved. It would interesting to explore and develop the relationship between the proposed technique and Tzeng et al.

Given the aforementioned cons, the AC is recommending that the paper be rejected.
"
iclr_2018_B1tC-LT6W,"Pros
-- Shows alternative strategies to train low-rank factored weight matrices for recurrent nets.

Cons
-- Minor modifications (and gains) over other forms of regularization like L2.
-- Results are only on an ASR task, so it’s not entirely clear how they’ll work on other tasks.

As pointed out by the reviewers, unless the authors show that the techniques generalize well to other tasks, and larger datasets it hard to accept it to the main conference. The AC, therefore, recommends that the paper be rejected.
"
iclr_2018_SkJd_y-Cb,"Pros
-- Extends embeddings to use a richer representation; simple yet interesting improvement on Mikolov et al. work.
Cons
-- All of the reviewers pointed out that the experimental evaluations needs improvement. The authors should find better ways to improve both quantitative (e.g., accuracy in analogies as in Mikolov et al., or by using the model for an external task if that’s the end goal) and qualitative (using functional similarity for the baseline) evaluations.

Given these comments, the AC recommends that the paper be rejected.
"
iclr_2018_Hyig0zb0Z,"Pros
-- Competitive results on LibriSpeech.
Cons
-- Limited novelty, and lacks enough comparisons.
-- Comparison with other end-to-end approaches, and on other commonly used datasets, like WSJ, missing.
-- Gated convnets have already been proposed.
-- Letter based systems have been shown to be competitive to phone based systems.
-- Optimization criterion is quite similar to lattice-free MMI proposed by Povey et al., but with a letter based LM and a slightly different HMM topology.

Given the cons pointed out by reviews, the AC is recommending that the paper be rejected.

"
iclr_2018_S1Ow_e-Rb,"The reviewers rightly point out that presented analysis is limiting and that the experimental results are not extensive enough. Moreover, several existing work that use raw waveforms have interesting analysis of what the network is trying to learn. Given these comments, the AC recommends that the paper be rejected.
"
iclr_2018_BybQ7zWCb,"The paper extends an existing work with three different frequency representations of audios and necessary network structure modifications for music style transfer.
It is an interesting study but does not provide ""sufficiently novel or justified contributions compared to the baseline approach of Ulyanov and Lebedev"". Also the revisions can not fully address reviewer 2's concerns.  "
iclr_2018_SJ60SbW0b,"The proposed LAN provides a visualization of the selectivity of networks to its inputs. It takes a trained network as golden target and estimates an LAN to predict masks that can be applied on inputs to generate the same outputs.
But the significance of the proposed method is unclear, ""what is the potential usage of the model?"". Empirical justification of that would make it stronger.  "
iclr_2018_rkmoiMbCb,"The paper presents a good analysis on the use of different linear maps instead of identity shortcuts for resnet.
It is interesting to the community but the experimental justification is insufficient.
1) As pointed out by the reviewer that this work shows ""that on small size networks Tandem Block outperforms Residual Blocks, since He at. al. (2016) in Tab 1 showed a contrary effect, does it mean that the observations do not scale to higher capacity networks?"", the paper would be much stronger if with experiments justify this claim.
2) ""extremely deep networks take much longer to train"" is not a valid reason to not conduct such exps."
iclr_2018_S1PWi_lC-,"the paper validates the benefit of multi-task learning on MNIST datasets, which is not sufficient for ICLR publication"
iclr_2018_B1p461b0W,"The paper studies the robustness of deep learning against label noise on MNIST, CIFAR-10 and ImageNet. But the generalization of the claim ""deep learning is robust to massive label noise"" is still questionable due to the limited noise types investigated.
The paper presents some tricks to improve learning with high label noise (batch size and learning rate), which is not novel enough.
"
iclr_2018_H1O0KGC6b,"* the proposed fine-tuning of only the last layer is not novel enough
* experiments are not sufficient to isolate the differences to support the benefit of post-training
"
iclr_2018_H1-oTz-Cb,The experiments are not sufficient to support the claim. The authors plan to improve it for future publication.
iclr_2018_HkwrqtlR-,"As the reviewers said, it is unclear what the main contribution of the paper is."
iclr_2018_BJQPG5lR-,"Pros:
+ Interesting perspective on training deep networks

Cons:
- Not a lot of practical significance: why would one want to use this algorithm over standard methods like ResNets or highway networks given that the proposed algorithm is more complex than established methods?
"
iclr_2018_B1D6ty-A-,"Pros:
+ Interesting alternative algorithm for training autoencoders

Cons:
- Not a lot of practical value because DANTE does not outperform SGD in terms of time or classification performance using autoencoder features.

This is an interesting and well-written paper that doesn't quite meet the threshold for ICLR acceptance. If the authors can find use cases where DANTE has demonstrable advantages over competing training algorithms, I expect the paper would be accepted.
"
iclr_2018_SyL9u-WA-,"Pros:
+ Clearly written paper.
+ Good theoretical analysis of the expressivity of the proposed model.
+ Efficient model update is appealing.
+ Reviewers appreciated the addition of results on the copy and adding tasks in Appendix C.

Cons:
- Evaluation was on less-standard RNN tasks.  A language modeling task should have been included in the empirical evaluation because language modeling is such an important application of RNNs.

This paper is close to the decision boundary, but the reviewers strongly felt that demonstration of the method on a language modeling task was necessary for acceptance.
"
iclr_2018_B1twdMCab,"Pros:
+ The paper is very clearly written.
+ The proposed re-embedding approach is easily implemented and can be integrated into fancier architectures.

Cons:
- A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors).

This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers' concerns, but in the end the reviewers were not completely convinced.
"
iclr_2018_SJ19eUg0-,"Pros:
+ Clearly written paper.

Cons:
- Limited empirical evaluation: paper should compare to first-order methods with well-tuned hyperparameters, since the block Hessian-free hyperparameters likely were well tuned, and plots of convergence as a function of time need to be included.
- Somewhat limited novelty in that block-diagonal curvature approximations have been used before, though the application to Hessian-free optimization is new.

The reviewers liked the clear description of the proposed algorithm and well-structured paper, but after discussion were not prepared to accept it primarily because (1) they wanted to see algorithmic comparisons in terms of convergence vs. time in addition to the convergence vs. updates that were provided; (2) they wanted more assurance that the baseline first-order optimizers had been carefully tuned; and (3) they wanted results on larger scale tasks.
"
iclr_2018_H1bhRHeA-,"The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations.  While the authors point out that their method is O(ND) instead of O(KND), the reviewers really wanted to see graphs demonstrating this, given that the implicit SGD method requires an iterative solver. The revised paper is otherwise much improved from the original submission, but falls a bit short of ICLR acceptance because of the lack of a measurement of convergence vs. time.

Pros:
+ Promising unbiased algorithms for optimizing the log-likelihood of a model using a softmax without having to repeatedly compute the normalizing factor.

Cons:
- The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations.
"
iclr_2018_HyKZyYlRZ,"
Pros:
+ Interesting and promising approach to multi-domain, multi-task learning.
+ Paper is clearly written.

Cons:
- Reads more like a technical report than a research paper: more space should be devoted to explaining the design decisions behind the model and the challenges involved, as this will help others tackle similar problems.

This paper had extensive discussion between the reviewers and authors, and between the reviewers.  In the end, the reviewers want more insight into the architectural choices made, either via ablation studies or via a series of experiments in which tasks or components are added one at a time.  The consensus is that this would give readers a lot more insight into the challenges involved in tackling multiple domains and multiple tasks in a single model and a lot more guidance on how to do it.
"
iclr_2018_rJ4uaX2aW,"Pros:
+ The proposed large-batch, synchronous SGD method is able to generalize at larger batch sizes than previous approaches (e.g., Goyal et al., 2017).

Cons:
- Evaluation on more than one task would make the paper more convincing.
- The addition of more hyperparameters makes the proposed algorithm less appealing.
- Some theoretical justifiction of the layer-wise rate scaling would help.
- It isn't clear that the comparison to Goyal et al., 2017 is entirely fair, because that paper also had recommendations for the implementation of batch normalization, weight decay, and a momentum correction as the learning rate is scaled up, but this submission does not address any of those.

Although the revised paper addressed many of the reviewers' concerns, they still did not feel it was quite strong enough to be accepted to ICLR.
"
iclr_2018_HJSA_e1AW,"The paper proposes a modification to Adam which is intended to ensure that the direction of weight update lies in the span of the historical gradients and to ensure that the effective learning rate does not decrease as the magnitudes of the weights increase.  The reviewers wanted a clearer justification of the changes made to Adam and a more extensive evaluation, and held to this opinion after reading the authors' rebuttal and revisions.

Pros:
+ The basic idea of treating the direction and magnitude separately in the optimization is interesting.

Cons:
- Insufficient evaluation of the new method.
- More justification and analysis needed for the modifications.  For example, are there circumstances under which they will fail?
- The modification to Adam and batch-normalized softmax idea are orthogonal to one another, making for a less coherent story.
- Proposed method does not have better generalization performance than SGD.
- Concern that constraining weight vectors to the unit sphere can harm generalization.
"
iclr_2018_SybqeKgA-,"The reviewers generally thought the proposed algorithm was a straightforward extension of Yin et al., 2017, and not enough for a new paper.  They also objected to a lack of test results (to show generalization), but the authors did provide these in their revision.

Pros:
+ Adaptive batch sizing is useful, especially if the larger batches license parallelization.

Cons:
- Small, incremental change to the algorithm from Yin et al., 2017
- Test performance did not improve over well-tuned momentum optimization, which limits the appeal of the method.
"
iclr_2018_H1A5ztj3b,"The paper reports unusally rapid convergence of the ResNet-56 model on CIFAR-10 when a single cycle of a cyclic learning rate schedule is used.  The effect is analyzed from several different perspectives. However, the reviewers were not convinced because the effect is only observed for one task, so they question the significance of the result. There was significant discussion of the paper by the reviewers and area chair before this decision was reached.

Pros:
+ Paper illustrates a ""super-convergence"" phenomenon in which training of a ResNet-56 reaches an accuracy of 92.4% on CIFAR-10 in 10,000 iterations using a single cycle of a cyclic learning rate schedule, while a more standard piecewise-constant schedule reaches 91.2% accuracy in 80,000 iterations.
+ There was partial, independent replication of the results on other tasks reported on OpenReview.

Cons:
- In the paper, the effect is shown for only one architecture and one task.
- In the paper, the effect is shown for only a single run.
- There are no error bars to indicate which differences are significant.
"
iclr_2018_ByJ7obb0b,"The reviewers thought that idea of trying to exploit low-rank structure in the loss gradients of a feedforward network to improve training was interesting; however they expressed many concerns about the clarity of the presentation, quality of the empirical evaluation, and significance of the result (since the tests were not done on an architecture anywhere near state-of-the-art). Because the authors did not participate in the discussion period, none of these concerns were addressed.

Pros:
+ Promising idea for new approaches to optimization.

Cons:
- Unclear notation for the intended machine learning audience
- Algorithm should be illustrated using pseudocode
- Limited significance if the method is only usable with purely feedforward networks.
- Limited empirical evaluation: positive results only if weights are poorly initialized.
"
iclr_2018_HJg1NTGZRZ,"Pros:
+ The idea of end-to-end training that simultaneously learns the weights and appropriate precision for those weights is very appealing.

Cons:
- Experimental results are far from the state-of-the-art, which makes the empirical evaluation unconvincing.
- More justification is needed for the update of the number of bits using the sign of the gradient.
"
iclr_2018_r1Kr3TyAb,"Two of the reviewers liked the intent of the paper -- to analyze gradient flow in residual networks and understand the tradeoffs between width and depth in such networks.  However, all reviewers flagged a number of problems in the paper, and the authors did not participate in the discussion period.

Pros:
+ Interesting analysis suggests wider, shallower ResNets should outperform narrower, deeper ResNets, and empirical results support the analysis.

Cons:
- Independence assumption on weights is not valid after any weight updates.
- The notation is not as clear as it should be.
- Empirical results would be more convincing if obtained on several tasks.
- The architecture analyzed in the paper is not standard, so it isn't clear how relevant it is for other practitioners.
- Analysis and paper should take into account other work in this area, e.g. Veit et al., 2016 and Schoenholz et al., 2017.
"
iclr_2018_rJUBryZ0W,"The author's revisions addressed clarity issues and some experimental issues (e.g., including MAML results in the comparison). The work takes an original path to an important problem (transfer learning, essentially). There is a question of significance, and this is due to the fact that the empirical comparisons are still very limited. The task is an artificial one derived from MNIST. I would call this ""toy"" as well. On this toy task, the approach isn't that much different from MAML, which is not in of itself a problem, but it would be interested to have a less superficial discussion of the differences.

The authors mention that they didn't have time for a larger empirical study. I think one is necessary in this case because the work is purposing a new learning algorithm/framework, and the question of its potential impact/significance is an empirical one."
iclr_2018_SkPoRg10b,"The concerns raised by AnonReviewer3 point out that, despite the effort of the authors to bridge the SM / ML divide, there is still some work to be done. The gulf between thermodynamic limits and finite effects is oft-cited in the author response. This seems to be a catch all. This gap needs to be addressed early. The authors might even suggest some open (empirical) questions looking for these phase transitions in finite systems in cases where they think engineering has placed us ""not too close"".  "
iclr_2018_ByJWeR1AW,"The reviewers agree that the authors have made an interesting contribution studying the effect of data augmentation, but they also agree that the claims made by the paper require a broader empirical study beyond the limited number of tasks surveyed in the current revision.  I urge the authors to follow this advice and see what they find."
iclr_2018_ByED-X-0W,"The reviewers are in agreement that while the paper is interesting, both the clarity of presentation and experimental rigor could be improved. The committee feels this paper is not ready for publication at ICLR 2018 inits current form."
iclr_2018_HJZiRkZC-,"This paper presents a method for using byte level convolutional networks for building text-based autoencoders.  They show that these models do well compared to RNN-based methods which model text in a sequence.  Evaluation is solely based on byte level prediction error.   The committee feels that the paper would have been stronger if evaluation was on some actual task (say summarization, Miao and Blunsom, for example) and show that it works as well as RNNs, the paper would have been stronger."
iclr_2018_H1a37GWCZ,"The paper presents an interesting extension of the SkipThought idea by modeling sentence embeddings using several document-structure related information.  Out of the various kinds of evaluations presented, the coreference results are interesting -- but, they fall short by a bit (as noted by Reviewer 2) because they don't compare with recent work by Kenton Lee et al.  In summary, the idea provides an interesting bit on building sentence embeddings, but the experimental results could have been stronger."
iclr_2018_Sk03Yi10Z,"This paper presents an ensemble method for conversation systems, where a retrieval-based system is ensembled with a generation-based system.  The combination is done via a reranker.  Evaluation is done on one dataset containing query reply pairs with both BLEU and human evalutations.  The experimental results are good using the ensemble model.  Although this presents some novel ideas and may be useful for chatbots (not for goal oriented systems), the committee feels that the approach and the presented material does not have enough substance for publication at ICLR:  it will be interesting to evaluate this system in a goal oriented setting; many prior papers have built generation based conversation systems (1-step) -- this paper does not present any comparison with those papers.  Addressing these issues may strengthen the paper for a future venue. "
iclr_2018_rkaqxm-0b,"This paper presents a neural compositional model for visual question answering.  The overall idea may be exciting but the committee agrees with the evaluation of Reviewer 1:  the experimental section is a bit thin and it only evaluates against an artificial dataset for visual QA that does not really need a knowledge base.  It would have been better to evaluate on more traditional question answering settings where the answer can be retrieved from a knowledge base (WebQuestions, Free917, etc.), and then compare with state of the art on those."
iclr_2018_r1kNDlbCb,"As expressed by most reviewers, the idea of the paper is interesting:  using summarization as an intermediate representation for an auto encoder.  In addition, a GAN is used on the generator output to encourage the output to look like summaries.  They just need unpaired summaries.  Even if the idea is interesting, from the committee's perspective, important baselines are missing in the experimental section:  why would one choose to use this method if it is not competitive with other baselines that have proposed work in this vein?  One reviewer brings up the point that the method is significantly worse than a supervised baseline.  Moreover, the authors mention the work of Miao and Blunsom, but could have used one of their experimental setups to show that at least in the semi-supervised scenario, this work empirically performs as well or better than that baseline."
iclr_2018_Hy3MvSlRW,"The paper presents an adversarial learning framework for reading comprehension.  Although the idea is interesting and presents an approach that ideally would make reading comprehension approaches more robust, the results are not substantially solid (see reviewer 3's comments) compared to other baselines to warrant acceptance.  Comments from reviewer 2 are also noteworthy where they mention that adversarial perturbations to a context around an answer can alter the facts in the context, thus destroying the actual information present there, and the rebuttal does not seem to satisfy the concern.  Addressing these issues will strengthen the paper for a potential future venue."
iclr_2018_r1QZ3zbAZ,"This paper presents a way to generate adversarial examples for text classification.   The method is simple -- finding semantically similar words and replacing them in sentences with high language model score.  The committee identifies weaknesses in this paper that resonate with the reviews below -- reviewer 1 suggests that the authors should closely compare with the work of Papernot et al, and the response to that suggestion is not satisfactory.  Addressing such concerns would make the paper stronger for a future venue."
iclr_2018_rybDdHe0Z,"This paper tries to establish that LSTMs are suitable for modeling neural signals from the brain.  However, the committee and most reviewers find that results are inconclusive.  Results are mixed across subjects.  We think it would have been far more interesting to compare other types of sequence models for this task other than the few simple baselines implemented here.  It is also unclear what is the LSTM learning extra in contrast with the other models presented in the paper."
iclr_2018_HJ_X8GupW,"There is overall consensus about the paper's lack of novelty and clarity.  Reviewer 1 has detailed comments that can be used to strengthen the paper.  Reviewer 3 suggests that this paper is very close to Anandkumar et al 2012, and it is not clear where the novelty lies.  Addressing these concerns of the reviewers will make the paper more acceptable to future venues."
iclr_2018_r1AMITFaW,"The reviewers of the paper are not very enthusiastic of the new model proposed, nor are they very happy with the experiments presented.   It is unclear from both the POS tagging and dependency parsing results where they stand with respect to state of the art methods that do not use RNNs.  We understand that the idea is to compare various RNN architectures, but it is surprising that the authors do not show any comparisons with other methods in the literature.  The idea of truncating sequences beyond a certain length is also a really strange choice.  Addressing the concerns of the reviewers will lead to a much stronger paper in the future."
iclr_2018_HJ39YKiTb,"None of the reviewers are enthusiastic about the paper, primarily due to lack of proper evaluation.  The response of the authors towards this criticism is also not sufficient.  The final results are mixed which does not show very clearly that the presented associative model performs better than the sole seq2seq baseline that the authors use for comparison.  We think that addressing these immediate concerns would improve the quality of this paper."
iclr_2018_HypkN9yRW,"The reviewers generally agree that the DDRprog method is both novel and interesting, while also seeing merit in outperformance of related methods in the empirical results. However, There were a lot of complaints about the writing quality, the clarity of the exposition, and unclear motivation of some of the work.  The reviewers also noted insufficient comparisons and discussions regarding relevant prior art, including recursive NNs, Tree RNNs, IEP, etc.  While the authors have made substantial revisions to the manuscript, with several additional pages of exposition, reviewers have not raised their scores or confidence in response."
iclr_2018_r1TA9ZbA-,"All reviewers agree that the contribution of this paper, a new way of training neural nets to execute Monte-Carlo Tree Search, is an appealing idea.  For the most part, the reviewers found the exposition to be fairly clear, and the proposed architecture of good technical quality.  Two of the reviewers point out flaws in implementing in a single domain, 10x10 Sokoban with four boxes and four targets.  Since their training methodology uses supervised training on approximate ground-truth trajectories derived from extensive plain MCTS trials, it seems unlikely that the trained DNN will be able to generalize to other geometries (beyond 10x10x4) that were not seen during training.  Sokoban also has a low branching ratio, so that these experiments do not provide any insight into how the methodology will scale at much higher branching ratios.

Pros: Good technical quality, interesting novel idea, exposition is mostly clear.  Good empirical results in one very limited domain.
Cons: Single 10x10x4 Sokoban domain is too limited to derive any general conclusions.

Point for improvement: The paper compares performance of MCTSnet trials vs. plain MCTS trials based on the number of trials performed.  This is not an appropriate comparison, because the NN trials will be much more heavyweight in terms of CPU time, and there is usually a time limit to cut off MCTS trials and execute an action.  It will be much better to plot performance of MCTSnet and plain MCTS vs. CPU time used."
iclr_2018_BkPrDFgR-,"All three reviewers are in agreement that this paper is not ready for ICLR in its current state. Given the pros/cons, the committee feels the paper is not ready for acceptance in its current form."
iclr_2018_B1nxTzbRZ,"The reviewer scores are fairly close, and the comments in their reviews are likewise similar.  All reviewers indicate that they find this to be an interesting learning domain.  However, they also agree in assessing the proposed method as having limited novelty and significance.  They also critiqued the empirical evaluation as being too specific to Starcraft and not comprehensive, without providing evidence that the defogger contributes to winning at StarCraft.  The authors wrote a substantial rebuttal to the reviews, but it did not convince anyone to increase their scores."
iclr_2018_H1LAqMbRW,"There was certainly some interest in this paper which investigates learning latent models of the environment for model-based planning, particularly articulated by Reviewer3.  However, the bulk of reviewer remarks focused on negatives, such as:

--The model-based approach is disappointing compared to the model-free approach.
--The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling.
--I feel the paper overstates the results in saying that the learned forward model is usable in MCTS.
-- the paper in it’s current form is not written well and does not contain strong enough empirical results


"
iclr_2018_r15kjpHa-,"All reviewers are unanimous that the paper is below threshold for acceptance.  The authors have not provided rebuttals, but merely perfunctory generic responses.

I think the most important criticism is that the approach is ""very ad-hoc.""  I would encourage the authors to consider more principled ways of automatically designing reward functions, like for example, Inverse Reinforcement Learning, in which you start with a good agent behavior policy, and then estimate a reward function for which the behavior policy maximizes the reward function."
iclr_2018_SJvrXqvaZ,"Reviewers are unanimous in scoring this paper below threshold for acceptance.  The authors did not submit any rebuttals of the reviews.

Pros:
Paper is generally clear.
Hardware results are valuable.

Cons:
Limited simulation results.
Proposed method is not really novel.
Insufficient empirical validation of the approach."
iclr_2018_rJIN_4lA-,"The reviewers found numerous issues in the paper, including unclear problem definitions, lack of motivation, no support for desiderata, clarity issues, points in discussion appearing to be technically incorrect, restrictive setting, sloppy definitions, and uninteresting experiments.  Unfortunately, little note of positive aspects was mentioned.  The authors wrote substantial rebuttals, including an extended exchange with Reviewer2, but this had no effect in terms of score changes. Given the current state of the paper, the committee feels the paper falls short of acceptance in its current form."
iclr_2018_B1EGg7ZCb,"The reviewers agree that the manuscript is below the acceptance threshold at ICLR.  Many points of criticism were evident in the reviewer comments, including small artificial test domain, no new methods introduced, poor writing in some places, and dubious need for DeepRL in this domain.  The reviews mentioned a number of constructive comments to improve the paper, and we hope this will provide useful guidance for the authors to rewrite and resubmit to a future venue."
iclr_2018_rye7IMbAZ,"This paper addresses the question of how to regularize when starting from a pre-trained convolutional network in the context of transfer learning.  The authors propose to regularize toward the parameters of the pre-trained model and study multiple regularizers of this type.  The experiments are thorough and convincing enough.  This regularizer has been used quite a bit for shallow models (e.g. SVMs as the authors mention, but also e.g. more general MaxEnt models).  There is at least some work on regularization toward a pre-trained model also in the context of domain adaptation with deep neural networks (e.g. for speaker adaptation in speech recognition).  The only remaining novelty is the transfer learning context.  This is not a sufficiently different setting to merit a new paper on the topic."
iclr_2018_rkZzY-lCb,"The paper presents an approach for learning continuous-valued vector representations combining multiple input feature sets of different types, in both unsupervised and supervised settings.  The revised paper is a merger of the original submission and another ICLR submission.  This meta-review takes into account all of the comments on both submissions and revisions.

The merged paper is an improvement over the two separate ones.  However, the contribution over previous work is still a bit unclear.  It still does not sufficiently compare to/discuss in the context of other recent work on combining multiple feature groups.

The experiments are also quite limited.  The idea is introduced as extremely general, but the experiments focus on a small number of specific tasks, some of them non-standard."
iclr_2018_H18uzzWAZ,"This is a nice but very narrow study of domain invariance in a microscopic imaging application.  Since the problem is very general, the paper should include much more substantial context, e.g. discussion of various alternative methods (e.g. the ones cited in Sun et al. 2017).  In order to contribute to the broader ICLR community, ideally the paper would also include application to more than just the one task."
iclr_2018_BJvVbCJCb,"The paper proposes an approach to jointly learning a data clustering and latent representation.  The main selling point is that the number of clusters need not be pre-specified.  However, there are other hyperparameters and it is not clear why trading # clusters for other hyperparameters is a win.  The empirical results are not strong enough to overcome these concerns."
iclr_2018_S191YzbRZ,"This paper proposes an approach for predicting transcription factor (TF) binding sites and TF-TF interaction.  The approach is interesting and may ultimately be valuable for the intended application.   But in its current state, the paper has insufficient technical novelty (e.g. relative to matching networks of Vinyals 2016), insufficient comparisons with prior work, and unclear benefit of the approach.  The reviewers also had some concerns about clarity.  "
iclr_2018_SJzmJEq6W,"This paper proposes an approach for learning a sparsifying transform via a set of nonlinear transforms at learning time.  The presentation needs a lot of work.  The original paper was 17 pages long and very difficult to understand.  The revised paper is 12 pages long, which is still too long for the content.  The paper needs to better distinguish between the major and minor points.  It is still too difficult to judge the contribution."
iclr_2018_r1tJKuyRZ,"The paper proposes an autoencoder for sets, an interesting and timely problem.  The encoder here is based on prior related work (Vinyals et al. 2016) while the decoder uses a loss based on finding a matching between the input and output set elements.  Experiments on multiple data sets are given, but none are realistic.   The reviewers have also pointed out a number of experimental comparisons that would improve the contribution of the paper, such as considering multiple matching algorithms and more baselines.  In the end the idea is reasonable and results are encouraging, but too preliminary at this point."
iclr_2018_B1EVwkqTW,"This paper proposes to pre-train a feature embedding, using Siamese networks, for use with few-shot learning for SVMs.  The idea is not very novel since there is a fairly large body of work in the general setting of pre-trained features + simple predictor.  In addition, the experimental results could be stronger -- there are stronger results in the literature (not cited), and better data sets for testing few-shot learning."
iclr_2018_BkVf1AeAZ,"This paper proposes an approach for jointly learning a label embedding and prediction network, as a way of taking advantage of relationships between labels.  This general idea is well-motivated, but the specifics of the proposed approach are not motivated or described well.  More discussion of relationship with prior work (e.g. other ways of ""softening"" the softmax) is needed.  The authors claim to have state-of-the-art results, but reviewers point out that much better results exist."
iclr_2018_HJsk5-Z0W,This paper has been withdrawn by the authors.
iclr_2018_SyGT_6yCZ,"The paper addresses the training time of CNNs, in the common setting where a CNN is trained on one domain and then used to extract features for another domain.  The paper proposes to speed up the CNN training step via a particular proposed training schedule with a reduced number of epochs.  Training time of the pre-trained CNN is not a huge concern, since this is only done once, but optimizing training schedules is a valid and interesting topic of study.   However, the approach here does not seem novel; it is typical to adjust training schedules according to the desired tradeoff between training time and performance.  The experimental validation is also thin, and the writing needs improvement."
iclr_2018_r1cLblgCZ,"This paper applies a form of recurrent autoencoder for a specific type of industrial sensor signal analysis.  The application is very narrow and the data set is proprietary.  The approach is not clearly described, but seems very straightforward and is not placed in context of prior work.  It is therefore not clear how to evaluate the contribution of the method.  The authors have revised the paper to include more details and prior work, but it still needs a lot more work on all of the above dimensions before it can make a significant contribution to the ICLR community."
iclr_2018_H1uP7ebAW,"Authors apply dense nets and LSTM to model dependencies among labels and demonstrate new state-of-art performance on an X-Ray dataset.

Pros:
- Well written.
- New improvement to state-of-art

Cons:
- Novelties are not strong. One combination of existing approaches are used to achieve state-of-art on what is still a relatively new dataset. (All Reviewers)

- Using LSTM to model dependencies would be affected by the selected order of the disease states. In this sense, LSTM seems like the wrong architecture to use to model dependencies among labels. This may be a drawback in comparison to other methods of modeling dependencies, but this is not thoroughly discussed or evaluated. (Reviewer 1 & 3)

- There is a large body of work on multi-task learning with shared information, which have not been evaluated for comparison. Because of this, the contribution of the LSTM to model dependencies between labels in comparison to other available approaches cannot be verified. (Reviewer 1 & 3)

- Top AUC performance on this dataset does not carry much significance on its own, as the dataset is new (CVPR 2017), and few approaches have been tested against it.

- Medical literature not cited to justify with evidence the discovered dependencies among disease states. (Reviewer 1)

"
iclr_2018_ryserbZR-,"Authors present a method for disease classification and localization in histopathology images. Standard image processing techniques are used to extract and normalize tiles of tissue, after which features are extracted from pertained networks. A 1-D convolutional filter is applied to the bag of features from the tiles (along the tile dimension, kernel filter size equal to dimensionality of feature vector). The max R and min R values are kept as input into a neural network for classification, and thresholding of these values provides localization for disease / non-disease.

Pro:
 - Potential to reduce annotation complexity of datasets while producing predictions and localization

Con:
- Results are not great. If anything, results re-affirm why strong annotations are necessary.
- Several reviewer concerns regarding novelty of proposed method. While authors have made clear the distinctions from prior art, the significance of those changes are debated.

Given the current pros/cons, the committee feels the paper is not ready for acceptance in its current form."
iclr_2018_rk1FQA0pW,"Authors present an evaluation of end-to-end training connecting reconstruction network with detection network for lung nodules.

Pros:
- Optimizing a mapping jointly with the task may preserve more information that is relevant to the task.

Cons:
- Reconstruction network is not ""needed"" to generate an image -- other algorithms exist for reconstructing images from raw data. Therefore, adding the reconstruction network serves to essentially add more parameters to the neural network. As a baseline, authors should compare to a detection-only framework with a comparable number of parameters to the end-to-end system. Since this is not provided, the true benefit of end-to-end training cannot be assessed.

- Performance improvement presented is negligible

- Novelty is not clear / significant"
iclr_2018_HkJ1rgbCb,"
Pro:
 - Interesting approach to tie together reinforcement Q-learning with CNN for prediction and reward function learning in predicting downstream effects of chemical structures, while providing relevant areas for decision-making.

Con:
- Datasets are small, generalizability not clear.
- Performance is not high (although performance wasn't the goal necessarily)
- Sometimes test performance is higher than training performance, making results questionable.
- Should include comparison to other wrapper-based combinatorial approaches.
- Too targeted an appeal/audience (better for chemical journal)"
iclr_2018_HytSvlWRZ,"Authors present a method for modeling neurodegenerative diseases using a multitask learning framework that considers ""censored regression"" problems (to model where the outputs have discrete values and ranges). Given the pros/cons, the committee feels this paper is not ready for acceptance in its current state.


Pro:
- This approach to modeling discrete regression problems is interesting and may hold potential, but the evaluation is not in a state where strong meaningful conclusions can be made.

Con:
- Reviewers raise multiple concerns regarding evaluation and comparison standards for tasks. While authors have added some model comparisons in response, in other areas comparisons don't appear complete. For example, when using MRI data, networks compared all use features derived from images, rather than systems that may learn from images themselves. Authors claim dataset is too small to learn directly from pixels in this data (in comments), but transfer learning and data augmentation have been successfully applied to learn from datasets of this size. In addition, new multitask techniques in the imaging domain have also been presented that dynamically learn the network structure, rather than relying on a hand-crafted neural network design. How this approach would compare is not addressed.


"
iclr_2018_HkanP0lRW,"Area chair is in agreement with reviewers: this is a good experiment that successfully applies specific machine learning techniques to the particular task. However, the authors have not discussed or studied the breadth of other possible methods that could also solve the given task ... besides those mentioned by the reviewers, U-Nets, and variants thereof, come to mind. Without these comparisons, the novelty and significance cannot be assessed.

Authors are encouraged to study similar works, and perform a comparison among multiple possible approaches, before submission to another venue.

"
iclr_2018_H1K6Tb-AZ,"General consensus among reviewers that paper does not meet criteria for publication.

Pro:
- Improvement over the original IDP proposal.
- Some promising preliminary results.

Con:
- Insufficient comparison to other methods of network compression,
- Insufficient comparison to other datasets (such as ImageNet)
- Insufficient evaluation on variety of other models
- Writing could be more clear"
iclr_2018_HkjL6MiTb,"Reviewers unanimous in assessment that manuscript has merits, but does not satisfy criteria for publication.

Pros:
- Potentially novel application of neural networks to survival analysis with competing risks, where only one terminal event from one risk category may be observed.

Cons:
- Incomplete coverage of other literature.
- Architecture novelty may not be significant.
- Small performance gains (though statistically significant)"
iclr_2018_ByJbJwxCW,"This paper presents a MIL method for medical time series data. General consensus among reviewers that work does not meet criteria for being accepted.

Specifically:

Pros:
- A variety of meta-learning parameters are evaluated for the task at hand.
- Minor novelty of the proposed method

Cons:
- Minor novelty of the proposed method
- Rationale behind architectural design
- Thoroughness of experimentation
- Suboptimal choice of baseline methods
- Lack of broad evaluation across applications for new design
- Small dataset size
- Significance of improvement
"
iclr_2018_SJFM0ZWCb,"Joint optimization of dimensionality reduction and temporal clusters. Results suggest performance improvement in a variety of scenarios versus a baseline of a recent state-of-art clustering method.

Pro:
- Joint optimization may be new and results suggest performance improvement when done on NASA Magnetospheric Multiscale (MMS) Mission.

Con:
- Small datasets evaluated, impact unclear
- Breadth of possible applications unclear
- Similarities exist to prior works. Significance of novelty not clear.
- Unanimous consensus among reviewers that work is not in a state to be accepted."
iclr_2018_rJr4kfWCb,"Pros:
- Addresses an important medical imaging application
- Uses an open dataset

Con:
- Authors do not cite original article describing challenge from which they use their data: https://arxiv.org/pdf/1612.08012.pdf , or the website for the corresponding challenge: https://luna16.grand-challenge.org/results/
- Authors either 1) do not follow the evaluation protocol set forth by the challenge, making it impossible to compare to other methods published on this dataset, or 2) incorrectly describe their use of that public dataset.
- Compares only to AlexNet architecture, and not to any of the other multiple methods published on this dataset (see: https://arxiv.org/pdf/1612.08012.pdf).
- Too much space is spent explaining well-understood evaluation functions.
- As reviewers point out, no motivation for new architecture is given.
"
iclr_2018_HJqUtdOaZ,"The presented method essentially builds a model that remaps features into a new space that optimizes nearest-neighbor classification. The model is a neural network, and the optimization is carried out through a genetic algorithm.

Pros:
 - One major issue with neural network classification is that of a lack of explainability. Many networks are currently ""black box"" approaches. By moving to the optimization problem to that of building a feature space for nearest neighbor classification, one can, to a degree, alleviate the ""black box"" issue by providing the discovered nearest neighbor instances as ""evidence"" of the decision.
- Authors use established datasets.

Cons:
- Authors do not properly cite previous work, as brought up by reviewers. There is much literature on optimization of feature spaces (such as the entire field of metric learning), as well as prior approaches using genetic optimization. The originality and significance here is therefore not clear. "
iclr_2018_S1m6h21Cb,"Pros:
- The authors propose a new algorithm to train GAN based on Cramer distance arguing that this eases optimization compared to Wasserstein GAN.
-  Reviewers agree that the paper reads well and provides a good overview of the properties of divergence measures used for GAN training.

Cons:
- It is not clear how much the central arguments about scale sensitivity, sum invariance, and unbiased sample gradients of the distances hold true in practice and generalize.
- The reviewers do not agree the benefits of the new algorithm is clear from the experiments shown.
Given the pros/cons ,the committee feels the paper falls short of acceptance in its current form."
iclr_2018_SJahqJZAW,"The paper proposes to use multiple discriminators to stabilize the GAN training process. Additionally, the discriminators only see randomly projected real and generated samples.

Some valid concerns raised by the reviewers which makes the paper weak:
 - Multiple discriminators have been tried before and the authors do not clearly show experimentally / theoretically if the random projection is adding any value.
- Authors compare only with DCGAN and the results are mostly subjective. How much improvement the proposed approach provides when compared to other GAN models that are developed with stability as the main goal is hence not clear."
iclr_2018_Hy7EPh10W,"Pros:
The paper aims to unify classification and novelty detection which is interesting and challenging.

Cons:
- The reviewers find that the work is incremental and contains heuristics. Reviewers find the repurposing of the fake logit in semi-supervised GAN discriminator for assigning novelty strange.
- The experiments presented are weak and authors do not compare with traditional/stronger approaches for novelty detection such as ""learning with abstention"" models and density models.
GIven the pros and cons, the committe finds the paper to fall short of acceptance in its current form.
"
iclr_2018_S1EfylZ0Z,"The authors propose to detect anomaly based on its representation quality in the latent space of the GAN trained on valid samples.

Reviewers agree that:
- The proposed solution lacks novelty and similar approaches have been tried before.
- The baselines presented in the paper are primitive and hence do not demonstrate the clear benefits over traditional approaches.
"
iclr_2018_rJHcpW-CW,"The paper aims to address the mode collapse issue in GANs by training multiple generators and forcing them to be diverse.

Reviewers agree that the proposed solution is not novel and has disadvantages such as increased parameters due to multiple generator models. The authors do not provide convincing arguments as to why the proposed approach should work well. The experiments presented also fail to demonstrate this. The results are limited to smaller MNIST and CIFAR10 datasets. Comparisons with approaches that directly address the mode collapse problem are missing."
iclr_2018_S1FQEfZA-,The paper proposes a new metric to measure GAN performance by training a classifier on the true labeled dataset and then comparing the distribution of the labels of the generated samples to the true label distribution. Reviewers find that the paper is well written but lacks novelty and is quite experimental does not present any new insights. The paper investigates well-known model collapse and diversity issues. Reviewers are not convinced that this is a good metric to measure sample quality or diversity as the generator can drop examples far away from the boundary and still achieve a good score on this metric.
iclr_2018_B1tExikAW,"The paper proposes to launch adversarial attacks in the latent space of VAE such that the minimal change in the latent representation leads to the decoder producing an image with class predictions altered.  Given the pros/cons the paper in its current form falls short of acceptance.

Pros:
Reviewers agree that the paper is well written and easy to follow

Cons:
- The paper lacks novelty and uses standard attacks and defense methodology.
- Reviewers find the attack scenario presented is unrealistic and hence may not useful.
- Experiments lack rigorous comparisons with baselines and it is not clear if the attack in the latent space will be stronger than the attack in the input space. "
iclr_2018_ryepFJbA-,"Pros:
The proposed regularization for GAN training is interesting and simple to implement.

Cons:
- Reviewers agree that the methodology is incremental over the WGAN with gradient penalty and the modification is not well motivated.
- Experimental results do not clearly demonstrate the benefits of the proposed algorithm and the paper also lacks comparisons with related works.
GIven the pros/cons, the committee feels the paper is not ready for acceptance in its current state."
iclr_2018_ry4SNTe0-,"The paper aims to combine Wasserstein GAN with Improved GAN framework for semi-supervised learning.

The reviewers unanimously agree that:
 - the paper lacks novelty and such approaches have been tried before.
 - the approach does not make sufficient gains over the baselines and stronger baselines are missing.
 - the paper is not well written and experimental results are not satisfactory."
iclr_2018_By9iRkWA-,"Generally solid engineering work but a bit lacking in terms of novelty and some issues with clarity. At the end of the day the empirical gains are not sufficient for acceptance - the results are state-of-the-art relative to published work, but not in the top 10 based on the official leaderboard (not even at time of submission). Since the technical contributions are small and the engineering contributions have been made obsolete by concurrent work, I suggest rejection."
iclr_2018_S1Q79heRW,"Two knowledgeable and confident reviewers suggest rejection, while one not confident reviewer suggests acceptance. I agree with the confident reviewers. All reviewers also point out that the paper is confusingly written and difficult to understand."
iclr_2018_ryOG3fWCW,"This paper does not meet the bar for ICLR - neither in terms of the quality of the write-up, nor in experimental design. The two confident reviewers agree to reject the paper, the weak accept comes from a less confident reviewer who did not write a good review at all. The rebuttal does not change this assessment."
iclr_2018_rJ8rHkWRb,The paper presents yet another approach for modeling words based on their characters. Unfortunately the authors do not compare properly to previous approaches and the idea is very incremental.
iclr_2018_rkYgAJWCZ,"The paper is looking at an interesting problem, but it seems too early. The approach requires training a new language model  from scratch for each new word, rendering it completely impractical for real use. The main evaluation therefore only considers four words - ""bonuses"", ""explained"", ""marketers"", ""strategist"" (expanded to 20 during the rebuttal). This is not sufficient for ICLR."
iclr_2018_HJw8fAgA-,"There was quite a bit of discussion about this paper but in the end the majority felt that, though the paper is interesting, the results are too limited and more needs to be done for publication.

PROS:
1. Good comparison of state space model variations
2. Good writing (perhaps a bit dense in places)
3. Promising results, especially concerning speedup

CONS:
1. The evaluation is quite limited

"
iclr_2018_SJky6Ry0W,"PROS:
1. All the reviewers thought that the work was interesting and showed promise
2. The paper is relatively well written

CONS:
1. Limited experimental evaluation (just MNIST)

The reviewers were all really on the fence about this but in the end felt that while the idea was a good one and the authors were responsive in their rebuttal, the experimental evaluation needed more work. "
iclr_2018_HkepKG-Rb,"This one was really on the fence.  After some additional rounds of discussion post-rebuttal with the reviewers I think the general consensus is that it's a good paper and almost there but not quite ready for acceptance at this time.  A detailed list of issues and concerns below.

PROS:
1. good idea: an additional loss term that enforces semantic constraints on the network output (like exactly 1 output element must be 1).
2. well written generally
3. a nice variety of different experiments

CONS:
1. paper organization.  The authors start with the axioms they would like a semantic loss function to obey, then provide a general definition, then show it does obey the axioms.  The general definition is intractable in a naive implementation.  The authors use boolean circuits to tractably solve the problem but this isn't discussed enough and it's unreasonable to expect readers to just give a pass on it without some more background.  I personally would prefer an organization that presented the motivation (in english) for the loss definition; then the  definition with a description of its pieces and why they are there; then a short discussion of how to implement such a loss in practice using boolean circuits (or if this is too much put it in the appendix); and a pointer to the axiomatization in an appendix.

2. related to 1, I didn't see anything which discussed the training time of this approach.  Given that the semantic loss has to be computed in a more involved way than usual, it's not clear whether it is practical."
iclr_2018_HJnQJXbC-,"The authors propose a system for asynchronous, model-parallel training, suitable for dynamic neural networks.  To summarize the reviewers:

PROS:
1. Paper contrasts well with existing work.
2. Positive results on dynamic neural network problems.
3. Well written and clear

CONS:
1. Some concern about extrapolations/estimates to hardware other than that on CPU.
2. Comparisons with Dynet seem to suggest auto-batching results in a dynamic mode aren't very positive.

For 1) the AC notes the author's objections to reviewer 1's views on the value of estimation/extrapolation to non-CPU hardware.  However, reviewer 3 voiced  a similar concern and  both still feel that there is more to be done to be convincing in the experiments."
iclr_2018_B1CQGfZ0b,"The reviewers were largely agreed that the paper presented an interesting idea and has potential but needs a better empirical evaluation.  It seems that the authors largely agree and are working to improve it.

PROS:
1. Improving the speed of program synthesis is a useful problem
2. Good treatment of related work, e.g. CEGIS

CONS:
1. The approach likely does not scale
2. The architecture is underspecified making it hard to reproduce
3. Only 1 domain for evaluation"
iclr_2018_r1kjEuHpZ,"Each of the reviewers had a slightly different set of issues with this paper but here is an attempt at a summary:

PROS:
1. Paper is mostly clear and well structured.

CONS:
1. Lack of novelty
2. Unsupported claims
3. Questionable methodology (using dropout confounds the goal of the experiment)

The authors did not submit a rebuttal."
iclr_2018_SJdCUMZAW,"The reviewers were quite unanimous in their assessment of this paper.

PROS:
1. The paper is relatively clear and the approach makes sense
2. The paper presents and evaluates a collection of approaches to speed learning of policies for manipulation tasks.
3. Improving the data efficiency of learning algorithms and enabling learning across multiple robots is important for practical use in robot manipulation.
4. The multi-stage structure of manipulation is nicely exploited in reward shaping and distribution of starting states for training.

CONS
1. Lack of novelty e.g. wrt to Finn et al. in ""Deep Spatial Autoencoders for Visuomotor Learning""
2. The techniques of asynchronous update and multiple replay steps may have limited novelty, building closely on previous work and applying it to this new problem.
3. The contribution on reward shaping would benefit from a more detailed description and investigation.
4. There is concern that results may be specific to the chosen task.
5. Experiments using real robots are needed for practical evaluation."
iclr_2018_H1kMMmb0-,"The consensus among the reviewers is that this paper is not quite ready for publication for reasons I will summarize in more detail below. However, I think there are some things that are really nice about this approach, and worth calling out:

PROS:

1.  the idea of tackling tasks broadly all the way from perception through symbolic reasoning is an important direction.

2. It certainly would be useful to have a ""plug and play"" framework in which various knowledge sources or skills can be assembled behind a simple interface designed by the ML practioner to solve a given problem or class of problems.

3. Clearly finding ways to increase sample efficiency -- especially in a deep net approach -- is of great importance practically.

4. The writing  is good.

CONS:

1. The comparison to feedforward networks needs to be made fair in order to disentangle the benefit of the architecture from the benefit of pre-training the modules.

2. Using the very limited 2x2 grid was too low a bar for the reviewers.  The authors aim at a  more general, efficient architecture useful for a variety of tasks, and perhaps you didn't want to devote too much time to this particular task, but I think having a slam-dunk example of the power of the approach is really necessary to be convincing.

3. Given the similarity, I think more has to be done to show the intellectual contribution over Zaremba et al, the difference in motivation notwithstanding.  One way to do this is to really prove out the increased sample efficiency claim."
iclr_2018_BkIkkseAZ,"Understanding the quality of the solutions found by gradient descent for optimizing deep nets is certainly an important area of research. The reviewers found several intermediate results to be interesting.  At the same time, the reviewers unanimously have pointed out various technical aspects of the paper that are unclear, particularly new contributions relative to recent prior work. As such, at this time, the paper is not ready for ICLR-2018 acceptance."
iclr_2018_ryCM8zWRb,"While the use of RNNs for building session-based recommender systems is certainly an important class of applications, the main strength of the paper is to propose and benchmark practical modifications to prior RNN-based systems that lead to performance improvements. The reviewers have pointed out that the writing in the paper needs improvement,  modifications are somewhat straightforward and some expected baselines such as comparisons against state of the art matrix-factorization based methods is missing. As such the paper could benefit from a revision and resubmission elsewhere."
iclr_2018_r1saNM-RW,"While the paper shows some encouraging results for scaling up SVMs using coreset methods, it has fallen short of making a fully convincing case, particularly given the amount of intense interest in this topic back in the heydey of kernel methods. When it comes to scalability, it has become the norm now to benchmark results on far larger datasets using parallelism, specialized hardware in conjunction with algorithmic speedups (e.g., using random feature methods, low-rank approximations such as Nystrom and other approaches). As such the paper is unlikely to generate much interest in the ICLR community in its current form."
iclr_2018_H1U_af-0-,"This an interesting new contribution to construction of random features for approximating kernel functions. While the empirical results look promising, the reviewers have raised concerns about not having insights into why the approach is more effective;  the exposition of the quadrature method is difficult to follow; and the connection between the quadrature rules and the random feature map is never explicitly stated. Some comparisons are missing (e.g., QMC methods). As such the paper will benefit from a revision and is not ready for ICLR-2018 acceptance."
iclr_2018_HJBhEMbRb,"Understanding the generalization behavior of deep networks is certainly an open problem. While this paper appears to develop some interesting new Fourier-based methods in this direction, the analysis in its current form is currently too restrictive, with somewhat limited empirical support, to broadly appeal to the ICLR community. Please see the reviews for more details.  "
iclr_2018_SJ8M9yup-,"- The paper is overall difficult to read and would benefit from a revised presentation.
- The practical relevance of the recovery conditions and algorithmic consequences of the work is not sufficiently clear or  convincing."
iclr_2018_SJu63o10b,"The paper is well written overall. However, the algorithmic framework has limited novelty and the reviewers unanimously are unconvinced by experimental results showing marginal improvements on smallish UCI datasets."
iclr_2018_SJDYgPgCZ,The reviewers are unanimous in their opinion that the theoretical results in this paper are of limited novelty and significance. Several parts of the paper are not presented clearly enough. As such the paper is not ready for ICLR-2018 acceptance.
iclr_2018_BJgd7m0xRZ,"The reviewers have unanimously expressed concerns about clarity, novelty, sound theoretical justification and intuitive motivation of the proposed approach. "
iclr_2018_HyiRazbRb,"The reviewers have unanimously expressed strong concerns about the technical correctness of the theoretical results in the paper. The paper should be carefully revised and checked for technical errors. In its current form, the paper is not suitable for acceptance at ICLR 2018.
"
iclr_2018_rJSr0GZR-,"The paper proposes learning the prior for AAEs by training a code-generator that is seeded by the standard Gaussian distribution and whose output is taken as the prior. The code generator is trained by minimizing the GAN loss b/w the distribution coming out of the decoder and the real image distribution. The paper also modifies the AAE by replacing the L2 loss in pixel domain with ""learned similarity metric"" loss inspired by the earlier work (Larsen et al., 2015).

The contribution of the paper is specific to AAE which makes the scope narrow. Even there, the benefits of learning the prior using the proposed method are not clear. Experiments make two claims: (i) improved image generation over AAE, (ii) improved ""disentanglement"".

Towards (i), the paper compares images generated by AAE with those generated by their model. However, it is not clear if the improved generation quality is due to the use of decoder loss on the learned similarity metric (Larsen at al, 2015), or due to the use of GAN loss in the image space (ie, just having GAN loss over decoder's output w/o having a code generator), or due to learning the prior which is the main contribution of the paper. This has also been hinted at by AnonReviewer1. Hence, it's not clear if the sharper generated images are really due to the learned prior.

Towards (ii), the paper uses InfoGAN inspired objective to generate class conditional images. It shows the class-conditional generated images for AAE and the proposed method. Here AAE is also trained on ""learned similarity metric"" and augmented with similar InfoGAN type objective so the only difference is in the prior. Authors say the performance of both models is similar on MNIST and SVHN but on CIFAR their model with ""learned prior"" generates images that match the conditioned-upon labels better. However this claim is also subjective/qualitative and even if true, it is not clear if this is due to learned prior or due to the extra GAN discriminator loss in the image-space -- in other words, how do the results look for AAE + a discriminator in the image space, just like in the proposed model but without a code generator?

The t-SNE plots for the learned prior are also shown but they are only shown when InfoGAN loss is added. The same plots are not shown for AAE with added InfoGAN loss so it is difficult to know the benefits of learning the code-generator as proposed.

Overall, I feel the scope of the paper is narrow and the benefits of learning the prior using the method proposed in the paper are not clearly established by the reported experiments. I am hesitant to recommend acceptance to the main conference in its current form.

"
iclr_2018_HyI6s40a-,"The paper proposes a method to detect and correct adversarial examples at the input stage (using a sparse coding based model) and/or at a hidden layer (using a GMM). These detector/corrector models are trained using only the natural examples. While the proposed method is interesting and has some novelty wrt to the specific models used for detection/correction (ie sparse coding and GMMs), there are crucial gaps in the empirical studies:

- It does not compare with a highly relevant prior work MagNet (Meng and Chen, 2017) which also detects and corrects adversarial examples by modeling the distribution of the natural examples

- The attacks used in the evaluations do not consider the setting where the existence (and architecture) of the defender models is known to the attacker

- It does not evaluate the method on a stronger PGD attack (also known as iterative FGSM)"
iclr_2018_ryZERzWCZ,The paper provides a constrained mutual information objective function whose Lagrangian dual covers several existing generative models. However reviewers are not convinced of the significance or usefulness of the proposed unifying framework (at least from the way results are presented currently in the paper). Authors have not taken any steps towards revising the paper to address these concerns. Improving the presentation to bring out the significance/utility of the proposed unifying framework is needed.
iclr_2018_H1wt9x-RW,"The paper proposes iterative training strategies for learning teacher and student models. They show how iterative training can lead to interpretable strategies over joint training on multiple datasets. All the reviewers felt the idea was interesting, although, one of the reviewers had concerns about the experimentation.

However, there is a BIG problem with this submission. The author names appear in the manuscript thus disregarding anonymity."
iclr_2018_B13EC5u6W,"The paper proposes a semi-supervised method to make deep learning more interpretable and at the same time be accurate on small datasets. The main idea is to learn dense representations from unlabelled data and then use those for building classifiers on small datasets as well as generate visual explanations. The idea is interesting, however, as one reviewer points out the presentation is poor. For instance, Table 2 is not understandable. Given the high standards of ICLR this cannot be ignored especially given the fact that the authors had the benefit of updating the paper which is a luxury for conference submissions."
iclr_2018_ByYPLJA6W,"The paper proposes a method to map input probability distributions to output probability distributions with few parameters. They show the efficacy of their method on synthetic and real stock data. After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data. More rigorous experimentation needs to be done to justify the method."
iclr_2018_ry9tUX_6-,"The paper proposes a new analysis of the optimization method called entropy-sgd which seemingly leads to more robust neural network classifiers. This is a very important problem if successful. The reviewers are on the fence with this paper. On the one hand they appreciate the direction and theoretical contribution, while on the other they feel the assumptions are not clearly elucidated or justified. This is important for such a paper. The author responses have not helped in alleviating these concerns. As one of the reviewers points out, the writing needs a massive overhaul. I would suggest the authors clearly state their assumptions and corresponding justifications in future submissions of this work."
iclr_2018_HJUOHGWRb,"The paper proposes a method to learn and explain simultaneously. The explanations are generated as part of the learning and in some sense come for free. It also goes the other way in that the explanations also help performance in simpler settings. Reviewers found the paper easy to follow and the idea has some value, however, the related work is sparse and consequently comparison to existing state-of-the-art explanation methods is also sparse. These are nontrivial concerns which should have been addressed in the main article not hidden away in the supplement."
iclr_2018_HyPpD0g0Z,"The paper proposes a method to robustify neural networks which is an important problem. They uses ideas from causality and create a model that would only depend on ""stable"" features ignoring the easy to manipulate ones. The paper has some interesting ideas, however, the main concern is regarding insufficient comparison to existing literature. One of the reviewers also has concerns regarding novelty of the approach."
iclr_2018_H1xJjlbAZ,The paper tries to show that many of the state-of-the-art interpretability methods are brittle and do not provide consistent stable explanations. The authors show this by perturbing (even randomly) the inputs so that the differences are imperceptible to a human observer but the interpretability methods provide completely different explanations. Although the output class is maintained before and after the perturbation it is not clear to me or the reviewers why one shouldn't have different explanations. The difference in explanations can be attributed to the fragility of the learned models (highly non-smooth decision boundaries) rather than the explanation methods. This is a critical point and has to come out more clearly in the paper.
iclr_2018_S1EzRgb0W,"The paper proposes a way to find why a classifier misclassified a certain instance. It tries to find pertubations in the input space to identify the appropriate reasons for the misclassification. The reviewers feel that the idea is interesting, however, it is insufficiently evaluated.  Even for the datasets they do evaluate not enough examples of success are provided. In fact, for CelebA the results are far from flattering."
iclr_2018_r1Oen--RW,"This paper showcases how saliency methods are brittle and cannot be trusted to obtain robust explanations. They define a property called input invariance that they claim all reliable explanation methods must possess. The reviewers have concerns regarding the motivation of this property in terms of why is it needed. This is not clear from the exposition. Moreover, even after having the opportunity to update the manuscript they seem to have not touched upon this issue other than providing a generic response."
iclr_2018_SJPpHzW0-,"The paper defines a new measure of influence and uses it to highlight important features. The definition is novel however, the reviewers have concerns regarding its significance, novelty and a thorough empirical comparison to existing literature is missing."
iclr_2018_rJhR_pxCZ,The paper proposes a new model called differential decision tree which captures the benefits of decision trees and VAEs. They evaluate the method only on the MNIST dataset. The reviewers thus rightly complain that the evaluation is thus insufficient and one also questions its technical novelty.
iclr_2018_B1ydPgTpW,"Reviewers concur that the paper and the application area are interesting but that the approaches are not sufficiently novel to justify presentation at ICLR.
"
iclr_2018_HJcjQTJ0W,"Reviews are marginal.
I concur with the two less-favorable reviews that the metrics  for privacy protection are not sufficiently strong for preserving privacy. "
iclr_2018_H1DJFybC-,"The paper addresses an interesting problem, is novel and works.
While the paper improved through reviews + rebuttal, the reviewers still find the presentation lacking. "
iclr_2018_HJWGdbbCW,"While the reviewers agree that this paper does provide a contribution, it is small and does overlap with several concurrent works. it is a bit hand-engineered.
The authors have provided a lengthy rebuttal, but the final reviews are not strong enough. "
iclr_2018_S1FFLWWCZ,"This paper describes active vision for object recognition learned in an RL framework.
Reviewers think the paper is not of sufficient quality: Insufficient detail, and insufficient evaluation.
While the authors have provided a lengthy rebuttal, the shortcomings have not yet been addressed in the paper. "
iclr_2018_SkmM6M_pW,"Authors do not respond to significant criticism - e.g. lack of a critical reference
Reviewers unanimously reject. "
iclr_2018_rJqfKPJ0Z,"The reviewers have various reservations.
While the paper has interesting suggestions, it is slightly incremental and the results are not sufficiently compared to other techniques.
We not that one reviewer revised his opinion "
iclr_2018_ByCPHrgCW,"While the reviewers all seem to think this is interesting and basically good work, the Reviewers are consistent and unanimous in rejecting the paper.
While the authors did provide a thorough rebuttal, the original paper did not meet the criteria and the reviewers have not changed their scores."
iclr_2018_H1u8fMW0b,"All 3 reviewers consider the paper insufficiently good, including a post-rebuttal updated score.
All reviewers + anonymous comment find that the paper isn't well-enough situated with the appropriate literature.
Two reviewers cite poor presentation - spelling /grammar errors making hte paper hard to read.
Authors have revised the paper and promise further revisions for final version.
"
iclr_2018_r1ayG7WRZ,"The reviewers highlight a lack of technical content and poor writing.
They all agree on rejection.
There was no author rebuttal or pointer to a new version. "
iclr_2018_H1BHbmWCZ,"Reviewers unanimous on rejection.
Authors don't maintain anonymity.
No rebuttal from authors.
Poorly written"
iclr_2018_SyhcXjy0Z,"Reviewers are unanimous that this is a reject.
A ""class project"" level presentation.
Errors in methodology and presentation.
No author rebuttal or revision"
iclr_2018_HkGcX--0-,"To ensure that a VAE with a powerful autoregressive decoder does not ignore its latent variables, the authors propose adding an extra term to the ELBO, corresponding to a reconstruction with an auxiliary non-autoregressive decoder. This does indeed produce models that use latent variables and (with some tuning of the weight on the KL term) perform as well as the underlying autoregressive model alone. However, as the reviewers pointed out, the paper does not demonstrate the value of the resulting models. If the goal is learning meaningful latent representations, then the quality of the representations should be evaluated empirically. Currently it is not clear whether that the proposed approach would yield better representations than a VAE with a non-autoregressive decoder or a VAE with an autoregressive decoder trained using the ""free bits"" trick of Kingma et al. (2016). This is certainly an interesting idea, but without a proper evaluation it is impossible to judge its value."
iclr_2018_SkxqZngC-,"The paper proposes a BNP topic model that uses a stick-breaking prior over document topics and performs VAE-style inference over them. Unfortunately, the novelty of this work is limited, as VAE-like inference for LDA-like models, inference with stick-breaking priors for VAEs, and placing a prior on the concentration parameter in a non-parametric topic model have all been done before (see e.g. Srivastava & Sutton (2017), Nalisnick & Smyth (2017), and Teh, Kurihara & Welling (2007) respectively). There are also concerns about the correctness of treating topics as parameters (as opposed to random variables) in the proposed model. The authors' clarification regarding this point was helpful but not sufficient to show the validity of the approach."
iclr_2018_SJSVuReCZ,"The proposed conditional variance regularizer looks interesting and the results show some promise. However, as the reviewers pointed out, the connection between the information-theoretic argument provided and the final form of the regularizer is too tenuous in its current form. Since this argument is central to the paper, the authors are urged to either provide a more rigorous derivation or motivate the regularizer more directly and place more emphasis on its empirical evaluation."
iclr_2018_SJ3dBGZ0Z,"The authors propose an efficient LSH-based method for computing unbiased gradients for softmax layers, building on (Mussmann et al. 2017). Given the somewhat incremental nature of the method, a thorough experimental evaluation is essential to demonstrating its value. The reviewers however found the experimental section weak and expressed concerns about the choice of baselines and their surprisingly poor performance."
iclr_2018_BJRxfZbAW,"The paper proposes augmenting Neural Statistician with a meta-context variable that specifies the partitioning of the latent context into the per-dataset and per-datapoint dimensions. This idea makes a lot of sense but the reviewers found the experimental section clearly insufficient to demonstrate its effectiveness convincingly. Also introducing only the unsupervised version of the model, which looks challenging to train, but performing all the experiments with the less interesting semi-supervised version makes the paper both less compelling and harder to follow."
iclr_2018_HkPCrEZ0Z,The paper has some potentially interesting ideas but it feels very preliminary. The experimental section in particular needs a lot more work.
iclr_2018_SkZ-BnyCW,"The reviewers agreed that while this is a well-written paper, it is low on novelty and does not make a substantial enough contribution. They also pointed out that although the reported MNIST results are highly competitive, possibly due to the use of a powerful ResNet decoder, the CIFAR10/ImageNet results are underwhelming."
iclr_2018_HkbmWqxCZ,"This is a well-written paper that aims to address an important problem. However, all the reviewers agreed that the experimental section is currently too weak for publication. They also made several good suggestions about improving the paper and the authors are encouraged to incorporate them before resubmitting."
iclr_2018_ryb83alCZ,"The authors propose a hierarchical VAE model with a discrete latent variable in the top-most layer for unsupervised learning of discriminative representations.  While the reported results on the two flow cytometry datasets are encouraging, they are insufficient to draw strong conclusions about the general effectiveness of the proposed architecture. Also, as two of the reviewers stated the proposed model is very similar to several VAE models in the literature. This paper seems better suited for a more applied venue than ICLR."
iclr_2018_SkERSm-0-,"The reviewers agreed that the paper was too long (more than twice the recommended page limit not counting the appendix) and difficult to follow. They also pointed out that its central idea of learning the noise distribution in a VAE was not novel. While the shortened version uploaded by the authors looks like a step in the right direction, it was not sufficient to convince the reviewers."
iclr_2018_r1kj4ACp-,"The reviewers are in agreement, that the paper is a big hard to follow and incorrect in places, including some claims not supported by experiments. "
iclr_2018_B1X4DWWRb,"The submission provides an interesting way to tackle the so-called distributional shift problem in machine learning. One familiar example is unsupervised domain adaptation. The main contribution of this work is deriving a bound on the generalization error/risk for a target domain as a combo of re-weighted empirical risk on the source domain and some discrepancy between the re-weighted source domain and the target domain. The authors then use this to formulate an objective function.

The reviewers generally liked the paper for its theoretical results, but found the empirical evaluation somewhat lacking, as do I. Especially the unsupervised domain adaptation results are very toy-ish in nature (synthetic data), whereas the literature in this field, cited by the authors, does significantly larger scale experiments. I am unsure as to how much I value I can place in the IHDP results since I am not familiar with the benchmark (and hence my lower confidence in the recommendation).

Finally, I am not very convinced that this is the appropriate venue for this work, despite containing some interesting results."
iclr_2018_HJ4IhxZAb,"In general, this seems like a sensible idea, but in my opinion the empirical results do not show a very compelling margin between using *entropy* as an active learning selection criterion vs the proposed methods. The difference is small enough that in practice it is very hard for me to believe that many researchers would choose to use the meta-learning via deep RL method (given that they'd need to train on multiple datasets and tune REINFORCE which is not going to be obviously easy). For that reason I am inclined to reject the paper.

In a follow-up version, I would heed the advice of Reviewer 1 and do more ablation analyses to understand the value of myopic vs non-myopic, cross-dataset vs. not, bandits vs RL, on the fly vs not (these are all intermingled issues). The relative lack of such analyses in the paper does not help in terms of it passing the bar."
iclr_2018_SktLlGbRZ,I concur with two of the reviewers: the work is somewhat incremental in terms of technical novelty (it's effectively CycleGANs for domain adaptation with a couple of effective tricks) and the need/advantage of the cycle consistency loss is not demonstrated sufficiently. The only solid ablation evidence seems to the the SVHN-->MNIST experiment from post-submission; I would personally like to see this kind of empirical proof extended much further (the fact that Shrivastava et al.'s method doesn't work well on GTA-->Cityscapes is not itself proof that cycle consistency is needed). With more empirical evidence I can see this paper being a good candidate for a computer vision conference like CVPR or ICCV.
iclr_2018_SyhRVm-Rb,"In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable ""goals"" (subsets of the state space) and use that instead of uniform sampling for goals. Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high-dimensional state space, which seems to be be whole raison d'etre of GANs in this proposed method. The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments  (a la Figure 2) showing how this method performs on complicated tasks.

I encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases)."
iclr_2018_ryj0790hb,"This work tackles an important problem of incremental learning and does so with extensive experimentation. As pointed out by two reviewers, the idea does seem novel and interesting, but the submission would require some rewriting before being potentially accepted at a venue like ICLR. I suggest focusing the paper more on the task-incremental learning aspects, doing the ablation studies (and other changes) as requested by the reviewers, and having a rich appendix with details (with more discussion in the paper itself)."
iclr_2018_r1DPFCyA-,"This submission presents intriguingly good results on k-shot learning and I agree with the authors that the results are better than the presented previous work, and that the method is simple, so I took a deeper look into the paper despite the overall negative reviews. However, I think in its current form, the paper is not suitable for publication:

- The previous work, that the authors compare to, were not really using comparable architectures: in fact, likely much worse base models with fewer parameters etc. I think any future version of this work would need to control for architecture capacity, otherwise how can we be sure where the gains come from? To me, this is a major unknown in terms of the credit assignment for the great results.
- The authors should be comparing with MAML (and follow-up work) by Finn et al. (2017)
- I don't really understand why the authors claim to have no need for validation sets. That's a very strong claim: are ALL the hyper-parameters (model architectures etc) just chosen in another, principled way? This issue would definitely need to be addressed in a follow-up work."
iclr_2018_rkeZRGbRW,"The reviewers found a number of short-comings in this work that would prevent it from being accepted at ICLR in its current form, both in terms of writing (not specifying the loss function),  experiments that are too limited, and inconclusive comparisons with existing regularization techniques. I recommend the authors take into account the feedback from reviewers in any follow-up submissions."
iclr_2018_H1BO9M-0Z,"While the problem of learning word embeddings for a new domain is important, the proposed method was found to be unclearly presented and missing a number of important baselines. The reviewers found the technical contribution to be of only limited value."
iclr_2018_BJB7fkWR-,"The reviewers have found that while the task of visual domain adaptation is meaningful to explore and improve, the proposed method is not sufficiently well-motivated, explained or empirically tested. "
iclr_2018_B1suU-bAW,"The reviewers agree that this paper provides a sensible mechanism for producing word embeddings that exploit correlating features in the data (e.g. texts written by the same author), but point to other work doing the same thing. The lack of direct comparison in the experimental section is troublesome, although it is entirely possible the authors' were not aware of related work. Unfortunately, the lack of an author response to the reviews makes it hard to see the argument in defense of this paper, and I must recommend rejection."
iclr_2018_S16FPMgRZ,"This paper proposes methods for replacing parts of neural networks with tensors, the values of which are efficiently estimated through factorisation methods. The paper is well written and clear, but the two main objections from reviewers surround the novelty and evaluation of the method proposed. I am conscious that the authors have responded to reviewers on the topic of novelty, but the case could be made more strongly in the paper, perhaps by showing significant improvements over alternatives. The evaluation was considered weak by reviewers, in particular due to the lack of comparable baselines.

Interesting work, but I'm afraid on the basis of the reviews, I must recommend rejection."
iclr_2018_rkGZuJb0b,"This paper proposes a tree-structured tensor factorisation method for parameter reduction. The reviewers felt the paper was somewhat interesting, but agreed that more detail was needed in the method description, and that the experiments were on the whole uninformative. This seems like a promising research direction which needs more empirical work, but is not ready for publication as is."
iclr_2018_B1bgpzZAZ,"This paper provides a method for eliminating options in multiple-answer reading comprehension tasks, based on the contents of the text, in order to reduce the ""answer space"" a machine reading model must consider. While there's nothing wrong with this, conceptually, reviewers have questioned whether or not this is a particularly useful process to include in a machine reading pipeline, versus having agents that understand the text well enough to select the correct answer (which is, after all, the primary goal of machine reading). Some reviewers were uncomfortable with the choice of dataset, suggesting SQuAD might be a better alternative), and why I am not sure I agree with that recommendation, it would be good to see stronger positive results on more than one dataset. At the end of the day, it is the lack of convincing experimental results showing that this method yields substantial improvements over comparable baselines which does the most harm to this well written paper, and I must recommend rejection."
iclr_2018_ryF-cQ6T-,"This paper seeks to integrate tensor-based models from physics into machine learning architectures. The two main objections to this paper are first that, despite honest (I assume) efforts from the authors, it remains somewhat hard to understand without substantial background knowledge of physics. Second, that the experiments focus on MNIST and CIFAR image classification tasks, two datasets where linear models perform with high accuracy, and as such are unsuitable for properly evaluating the claims made about the models in this paper. Unfortunately, it does not seem there is sufficient enthusiasm for this paper amongst the reviewers to justify its inclusion in the conference."
iclr_2018_HyHmGyZCZ,"This paper proposes a method for refining distributional semantic representation at the lexical level. The reviews are fairly unanimous in that they found both the initial version of the paper, which was deemed quite rushed, and the substantial revision unworthy of publication in their current state. The weakness of both the motivation and the experimental results, as well as the lack of a clear hypothesis being tested, or of an explanation as to why the proposed method should work, indicates that this work needs revision and further evaluation beyond what is possible for this conference. I unfortunately must recommend rejection."
iclr_2018_SJlhPMWAW,"The authors present GraphVAE, a method for fitting a generative deep model, a variational autoencoder, to small graphs.  Fitting deep learning models to graphs remains challenging (although there is relevant literature as brought up by the reviewers and anonymous comments) and this paper is a strong start.

In weighing the various reviews, AnonReviewer3 is weighed more highly than AnonReviewer1 and AnonReviewer2 since that review is far more thorough and the reviewer is more expert on this subject.  Unfortunately, the review from AnonReviewer1 is extremely short and of very low confidence.  As such, this paper sits just below the borderline for acceptance.  In general, the main criticisms of the paper are that some claims are too strong (e.g. non-differentiability of discrete structures), treatment of related work (missing references, etc.) and weak experiments and baselines.  The consensus among the reviews (even AnonReviewer2) is that the paper is preliminary.  The paper is close, however, and addressing these concerns will make the paper much stronger.

Pros:
- Proposes a method to build a generative deep model of graphs
- Addresses a timely and interesting topic in deep learning
- Exposition is clear

Cons:
- Treatment of related literature should be improved
- Experiments and baselines are somewhat weak
- ""Preliminary""
- Only works on rather small graphs (i.e. O(k^4) for graphs with k nodes)"
iclr_2018_S1fcY-Z0-,"This paper presents a new method for approximate Bayesian inference in neural networks.  The reviewers all found the proposed idea interesting but originally had questions about its novelty (with regard to normalizing flows) and questioned the technical rigor of the approach.  The authors did a good job of addressing the technical concerns, causing two of the reviewers to raise their scores.  However, the paper remains just borderline and none of the reviewers are willing to champion the paper as their questions about novelty and empirical evaluation remain.  The reviewers all questioned fundamental technical aspects of the paper (which were clarified in the discussion), indicating that the paper requires more careful exposition of the technical contributions.  Taking the reviewers feedback and discussion into account, running some more compelling experiments and rewriting the paper to make the technical aspects more clear would make this a much stronger submission.

Pros:
- Provides an interesting idea for approximate Bayesian inference in deep networks
- The paper appears correct
- The approach is scalable and tractable

Cons:
- The technical writing is not rigorous
- The reviewers don't seem convinced by the empirical analysis
- Incremental over existing (but recent) work (Luizos and Welling)"
iclr_2018_SkqV-XZRZ,"This paper proposes a method for performing stochastic variational inference for bidirectional LSTMs through introducing an additional latent variable that induces a dependence between the forward and backward directions.  The authors demonstrate that their method achieves very strong empirical performance (log-likelihood on test data) on the benchmark TIMIT and BLIZZARD datasets.

The paper is borderline in terms of scores with a 7, 6 and 4.  Unfortunately the highest rating also corresponds to the least thorough review and that review seems to indicate that the reviewer found the technical exposition confusing.  AnonReviewer2 also found the writing confusing and discovered mistakes in the technical aspects of the paper (e.g. in Eq 1).  Unfortunately, the reviewer who seemed to find the paper most easy to understand also gave the lowest score.  A trend among the reviewers and anonymous comments was that the paper didn't do a good enough job of placing itself in the context of related work (Goyal et. al, ""Z-forcing"") in particular.  The authors seem to have addressed this (curiously in an anonymous link and not in an updated manuscript) but the manuscript itself has not been updated.

In general, this paper presents an interesting idea with strong empirical results.   The paper itself is not well composed, however, and can be improved upon significantly.  Taking the reviews into account and including a better treatment of related work in writing and empirically will make this a much stronger paper.

Pros:
- Strong empirical performance (log-likelihood on test data)
- A neat idea
- Deep generative models are of great interest to the community

Cons:
- Incremental in relation to Goyal et al., 2017
- Needs better treatment of related work
- The writing is confusing and the technical exposition is not clear enough"
iclr_2018_Bk6qQGWRb,"This work develops a methodology for exploration in deep Q-learning through Thompson sampling to learn to play Atari games.  The major innovation is to perform a Bayesian linear regression on the last layer of the deep neural network mapping from frames to Q-values.  This Bayesian linear regression allows for efficiently drawing (approximate) samples from the network.  A careful methodology is presented that achieves impressive results on a subset of Atari games.

The initial reviews all indicated that the results were impressive but questioned the rigor of the empirical analysis and the implementation of the baselines.  The authors have since improved the baselines and demonstrated impressive results across more games but questions over the empirical analysis remain (by AnonReviewer3 for instance) and the results still span only a small subset of the Atari suite.  The reviewers took issue with the treatment of related work, placing the contributions of this paper in relation to previous literature.

In general, this paper shows tremendous promise, but is just below borderline.  It is very close to a strong and impressive paper, but requires more careful empirical work and a better treatment of related work.  Hopefully the reviews and the discussion process will help make the paper much stronger for a future submission.

Pros:
- Very impressive results on a subset of Atari games
- A simple and elegant solution to achieving approximate samples from the Q-network
- The paper is well written and the methodology is clearly explained

Cons:
- Questions remain about the rigor of the empirical analysis (comparison to baselines)
- Requires more thoughtful comparison in the manuscript to related literature
- The theoretical justification for the proposed methods is not strong"
iclr_2018_By-IifZRW,"The authors propose the use of Gaussian processes as the prior over activation functions in deep neural networks.  This is a purely mathematical paper in which the authors derive an efficient and scalable approach to their problem.  The idea of having flexible distributions over activation functions is interesting and possibly impactful.  One reviewer recommended acceptance with low confidence.  The other two found the idea interesting and compelling but confidently recommended rejection.  These reviewers are concerned that the paper is unnecessarily complex in terms of the mathematical exposition and that it repeats existing derivations without citation.  It is very important that the authors acknowledge existing literature for mathematical derivations.  Furthermore, the reviewers question the correctness of some of the statements (e.g. is the variational bound preserved?).  These reviewers agreed that the paper is incomplete without any empirical validation.

Pros:
- A compelling and promising idea
- The approach seems to be scalable and highly plausible

Cons:
- No experiments
- Significant issues with citing of related work
- Significant questions about the novelty of the mathematical work"
iclr_2018_BJlrSmbAZ,"This paper shows that batch normalization can be cast as approximate inference in deep neural networks.  This is an appealing result as batch normalization is used in practice in a wide variety of models.   The reviewers found the paper well written and easy to understand and were motivated by underlying idea.  However, they found the empirical analysis lacking and found that there was not enough detail in the main text to verify whether the claims were true.

The authors empirically compared to a recent method showing that dropout can be cast as approximate inference with the claim that by transitivity they were comparing to a variety of recent methods.  AnonReviewer1 casts significant doubt on the results of that work.  This is very unfortunate and not the fault of the authors of this paper.  The authors have since gone to great length to compare to Louizos and Welling, 2017.  Unfortunately, that comparison doesn't appear to be complete in the manuscript.

The main text was also lacking specific detail relating to fundamental parts of the proposed method (noted by all reviewers).

Overall, this paper seems to be tremendously promising and the underlying idea potentially very impactful.  However, given the reviews, it doesn't seem that this paper would achieve its potential impact.  The response from the authors is appreciated and goes a long way to improving the paper.  Taking the reviews into account, adding specific detail about the methodology and model (e.g. the prior) and completing careful empirical analysis will make this a strong paper that should be much more impactful."
iclr_2018_SknC0bW0-,"This paper combines multiple existing ideas in Bayesian optimization (continuous-fidelity, use of gradient information and knowledge gradient) to develop their proposed cfKG method.  While the methodology seems neat and effective, the reviewers (and AC) found that the presented approach was not quite novel enough in light of existing work to justify acceptance to ICLR.  Continuous fidelity Bayesian optimization is well studied and knowledge gradient + derivative information was presented at NIPS.  The combination of these things seems quite sensible but not sufficiently novel (unless the empirical results were *really* compelling).

Pros:
- The paper is clear and writing is of high quality
- Bayesian optimization is interesting to the community and compelling methods are potentially practically impactful
- Outperforms existing methods on the chosen benchmarks

Cons:
- Is an incremental combination of existing methods
- The paper claims too much"
iclr_2018_rk8R_JWRW,"The reviewers agreed that the paper was somewhat preliminary in terms of the exposition and empirical work.  They all find the underlying problem quite interesting and challenging (i.e. spiking recurrent networks).  However, the manuscript failed to motivate the approach.  In particular, everyone agrees that spiking networks are very interesting, but it's unclear what problem the presented work is solving.  The authors need to be more clear about their motivation and then close the loop with empirical validation that their approach is solving the motivating problem (i.e. do we learn something about biological plausibility, are spiking networks better than traditional LSTMs at modeling a particular kind of data, or are they more efficiently implemented on hardware?).  Motivating the work with one of these followed by convincing experiments would make this a much stronger paper.

Pros:
- Tackles an interesting and challenging problem at the intersection of neuroscience and ML
- A novel method for creating a spiking LSTM

Cons:
- The motivation is not entirely clear
- The empirical analysis is too simple and does not demonstrate the advantages of this approach
- The paper seems unfocused and could use rewriting

"
iclr_2018_SyxCqGbRZ,"This paper brings recent innovations in reinforcement learning to bear on a tremendously important application, treating sepsis.  The reviewers were all compelled by the application domain but thought that the technical innovation in the work was low.  While ICLR welcomes application papers, in this instance the reviewers felt that the technical contribution was not justified well enough.  Two of the reviewers asked for a more clear discussion of the underlying assumptions of the approach (i.e. offline policy evaluation and not missing at random).  Unfortunately, lack of significant revisions to the manuscript over the discussion period seem to have precluded changes to the reviewer scores.  Overall, this could be a strong submission to a conference that is more closely tied to the application domain.

Pros:
- Very compelling application that is well motivated
- Impressive (possibly impactful) results
- Thorough empirical comparison

Cons:
- Lack of technical innovation
- Questions about the underlying assumptions and choice of methodology"
iclr_2018_rkTBjG-AZ,"This paper introduces a framework for specifying the model search space for exploring over the space of architectures and hyperparameters in deep learning models (often referred to as architecture search).  Optimizing over complex architectures is a challenging problem that has received significant attention as deep learning models become more exotic and complex.  This work helps to develop a methodology for describing and exploring the complex space of architectures, which is a challenging problem.  The authors demonstrate that their method helps to structure the search over hyperparameters using sequential model based optimization and Monte Carlo tree search.

The paper is well written and easy to follow.  However, the level of technical innovation is low and the experiments don't really demonstrate the merits of the method over existing strategies.  One reviewer took issue with the treatment of related work.  The underlying idea is compelling and addresses an open question that is of great interest currently.  However, without experiments demonstrating that this works better than, e.g., the specification in the hyperopt package, it is difficult to assess the contribution.  The authors must do a better job of placing this contributing in the context of existing literature and empirically demonstrate its advantages.  The presented experiments show that the method works in a limited setting and don't explore optimization over complex spaces (i.e. over architectures - e.g. number of layers, regularization for each layer, type of each layer, etc.).  There's nothing presented empirically that hasn't been possible with standard Bayesian optimization techniques.

This is a great start, but it needs more justification empirically (or theoretically).

Pros:
- Addresses an important and pertinent problem - architecture search for deep learning
- Provides an intuitive and interesting solution to specifying the architecture search problem
- Well written and clear

Cons:
- The empirical analysis does not demonstrate the advantages of this approach over existing literature
- Needs to place itself better in the context of existing literature"
iclr_2018_HyBbjW-RW,"The idea of using the determinant of the covariance matrix over inputs to select experiments to run is a foundational concept of experimental design.  Thus it is natural to think about extending such a strategy to sequential model based optimization for the hyperparameters of machine learning models, using recent advances in determinantal point processes.  The idea of sampling from k-DPPs to do parallel hyperparameter search, balancing quality and diversity of expected outcomes, seems neat.  While the reviewers found the idea interesting, they saw weaknesses in the approach and most importantly were not convinced by the empirical results.  All reviewers thought that the baselines were inappropriate given recent work in hyperparameter optimization (and classic work in statistics).

Pros:
- Useful to a large portion of the community (if it works)
- An interesting idea that seems timely

Cons:
- Only slightly outperforms baselines that are too weak
- Not empirically compared to recent literature
- Some of the design and methodology require more justification
- Experiments are limited to small scale problems"
iclr_2018_H1Nyf7W0Z,"The reviewers agreed that this paper is not quite ready for publication at ICLR.  One of the reviewers thought the paper was well written and easy to follow while the two others said the opposite.  One of the main criticisms was issues with the composition.  The paper seems to lack a clear formal explanation of the problem and the proposed methodology.  The reviewers in general weren't convinced by the experiments, complaining about the lack of a required baseline and that the proposed method doesn't seem to significantly help in the experiment presented.

Pros:
- The proposed idea is interesting
- The problem is timely and of interest to the community
- Addresses multiple important problems at the intersection of ML and RL in sequence generation

Cons:
- Novel but somewhat incremental
- The experiments are not compelling (i.e. the results are not strong)
- A necessary baseline is missing
- Significant issues with the writing - both in terms of clarity and correctness."
iclr_2018_ryk77mbRZ,"This paper proposes a regularizer for recurrent neural networks, based on injecting random noise into the hidden unit activations.  In general the reviewers thought that the paper was well written and easy to understand.  However, the major concern among the reviewers was a lack of empirical evidence that the method works consistently.  Essentially, the reviewers were not compelled by the presented experiments and demanded more rigorous empirical validation of the approach.

Pros:
- Well written and easy to follow
- An interesting idea
- Regularizing RNNs is an interesting and active area of research in the community

Cons:
- The experiments are not compelling and are questioned by all the reviewers
- The writing does not cite relevant related work
- The work seems underexplored (empirically and methodologically)"
iclr_2018_r1vccClCb,"The paper proposes a form of autoencoder that learns to predict the neighbors of a given input vector rather than the input itself.  The idea is nice but there are some reviewer concerns about insufficient evaluation and the effect of the curse of dimensionality.  The revised paper does address some questions and includes additional helpful experiments with different types of autoencoders.  However, the work is still a bit preliminary.  The area of auto-encoder variants, and corresponding experiments on CIFAR-10 and the like, is crowded.  In order to convince the reader that a new approach makes a real contribution, it should have very thorough experiments.  Suggestions:  try to improve the CIFAR-10 numbers (they need not be state-of-the-art but should be more credible), adding more data sets (especially high-dimensional ones), and analyzing the effects of factors that are likely to be important (e.g. dimensionality, choice of distance function for neighbor search)."
iclr_2018_SkYMnLxRW,"The paper proposes a modification to the Transformer network, which mostly consists in changing how the attention heads are combined. The contribution is incremental, and its novelty is limited. The results demonstrate an improvement over the baseline at the cost of a more complicated training procedure with more hyper-parameters, and it is possible that with similar tuning the baseline performance could be improved in a similar way."
iclr_2018_rJBiunlAW,"The paper presents Simple Recurrent Unit, which is characterised by the lack of state-to-gates connections as used in conventional recurrent architectures. This allows for efficient implementation, and leads to results competitive with the recurrent baselines, as shown on several benchmarks.

The submission lacks novelty, as the proposed method is essentially a special case of Quasi-RNN [Bradbury et al.], published at ICLR 2017. The comparison in Appendix A confirms that, as well as similar results of SRU and Quasi-RNN in Figures 4 and 5. Quasi-RNN has already been demonstrated to be amenable to efficient implementation and perform on a par with the recurrent baselines, so this submission doesn’t add much to that."
iclr_2018_HJOQ7MgAW,"The paper performs an ablation analysis on LSTM, showing that the gating component is the most important. There is little novelty in the analysis, and in its current form, its impact is rather limited."
iclr_2018_SkffVjUaW,"Regarding clarity, while the paper definitely needs work if it is to be resubmitted to an ML venue, different revisions would be appropriate for a physics audience. And given the above comment, any suggested changes are likely to be superfluous."
iclr_2018_SJmAXkgCb,"The paper presents a technique for feature map compression at inference time. As noted by reviewers, the main concern is that the method is applied to one NN architecture (SqueezeNet), which severely limits its impact and applicability to better performing state-of-the-art models."
iclr_2018_SJn0sLgRb,"The paper proposes a data augmentation technique for image classification which consists in averaging two input images and using the label of one of them. The method is shown to outperform the baseline on the image classification task, the but evaluation doesn’t extend beyond that (to other tasks or alternative augmentation mechanisms); theoretical justification is also lacking."
iclr_2018_Sy3fJXbA-,"The paper proposes a method for learning connectivity in neural networks, evaluated on the ResNeXt architecture. The novelty of the method is rather limited, and even though the method has been shown to improve on the ResNeXt baselines on CIFAR-100 and ImageNet classification tasks (which is encouraging), it should have been evaluated on more architectures and datasets to confirm its generality."
iclr_2018_rJoXrxZAZ,"The paper presents a hybrid architecture which combines WaveNet and LSTM for speeding-up raw audio generation. The novelty of the method is limited, as it’s a simple combination of existing techniques. The practical impact of the approach is rather questionable since the generated audio has significantly lower MOS scores than the state-of-the-art WaveNet model."
iclr_2018_S1NHaMW0b,"The paper proposes a regularisation technique based on Shake-Shake which leads to the state of the art performance on the CIFAR-10 and CIFAR-100 dataset. Despite good results on CIFAR, the novelty of the method is low, justification for the method is not provided, and the impact of the method on tasks beyond CIFAR classification is unclear."
iclr_2018_rJ6iJmWCW,"The paper proposes a method for accented speech generation using GANs.
The reviewers have pointed out the problems in the justification of the method (e.g. the need for using policy gradients with a differentiable objective) as well as its evaluation."
iclr_2018_H1cKvl-Rb,"The idea studied here is interesting, if incremental. The empirical results are not particularly stellar, but it's clear that the authors have done their best to provide reproducible and defensible results. A few sticking points: a) The use of the term 'UCB', as mentioned in an anonymous comment, is somewhat misleading. ""Approximate Confidence Interval"" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O'Donoghue et al.) and worth comparing to, and c) the theoretical results are not always justified or useful (e.g. Equation 9: the bound is trivial, posterior >= 0 or 1). "
iclr_2018_B16yEqkCZ,"This paper presents an interesting idea that is related to imitation learning, safe exploration,
and intrinsic motivation. However, in its current state the paper needs improvement in clarity. There are also some concerns about the number of hyperparameters involved. Finally, the experimental results are not completely convincing and should reflect existing baselines in one of the areas described above.
"
iclr_2018_BJ7d0fW0b,"This paper proposes a simple idea for using expert data to improve a deep RL agent's performance. Its main flaw is the lack of justification for the specific techniques used. The empirical evaluation is also fairly limited.
"
iclr_2018_HkpRBFxRb,"This is an interesting paper, but was quite difficult to follow. As they stand, the empirical results are not altogether convincing nor warrant acceptance."
iclr_2018_HyunpgbR-,"The reviewers feel there are two issues that make this paper fall short of acceptance: first, the
lack of a clear emphasis and focus (evidenced by the significant revisions) and second, a lack of
comparison to similar, existing methods for multi-agent reinforcement learning."
iclr_2018_BJvWjcgAZ,"The reviewers agree the proposed idea is relatively incremental, and the paper itself does not do an exemplary job in other areas to make up for this."
iclr_2018_Sy_MK3lAZ,The idea studied here is fairly incremental and the empirical evaluation could be improved.
iclr_2018_S1GDXzb0b,"The paper is hard to follow at times. The heuristic reward has little justification -- not clear how
this would extend to other domains. Lack of empirical comparisons (see e.g. Hester et al., Deep Q-Learning from Demonstrations, 2017). "
iclr_2018_HyDAQl-AW,The reviewers agree that this paper suffers from a lack of novelty and does not make sufficient contributions to warrant acceptance.
iclr_2018_rkc_hGb0Z,The reviewers are unanimous that the paper is not sufficiently clear and could be improved with better empirical results.
iclr_2018_rkvDssyRb,"The reviewers agree this is an interesting paper with interesting ideas, but is not ready for publication in its current shape. In particular, there is a need for strong empirical results."
iclr_2018_rJIgf7bAZ,"The reviewers are unanimous that this is an interesting paper, but that ultimately the empirical results are not sufficiently promising to warrant the added complexity."
iclr_2018_Bk-ofQZRb,The reviewers agree this paper is not yet ready for publication.
iclr_2018_SyF7Erp6W,This paper does not seem completely appropriate for ICLR.
iclr_2018_rJ3fy0k0Z,"All of the reviewers found some aspects of the formulation and experiments interesting, but they found the paper hard to read and understand. Some of the components of the technique such as the state screening function (SSF) seem ad-hoc and heuristic without much justification. Please improve the exposition and remove the unnecessary component of the technique, or come up with better justifications."
iclr_2018_B1mSWUxR-,"There are some interesting ideas discussed in the paper, but the reviewers expressed difficulty understanding the motivation and the theoretical results. The experiments do not seem convincing in showing that SQDML achieves significant gains. Overall, the the paper needs either stronger and clearer theoretical results, or more convincing experiments for publication at ICLR."
iclr_2018_rk3b2qxCW,"The paper has some interesting ideas around auto-regressive policies and estimating their entropy for exploration. The use of autoregressive policies in RL is not particularly novel, and the estimate of entropy for such models is straightforward. Finally, the experiments focus on very simple tasks."
iclr_2018_SyPMT6gAb,"The reviewers agree that the paper studies and interesting problem with an interesting approach. The reviewers raised some concerns regarding the theoretical and empirical results. The authors have made changes to the paper, but given the theoretical nature of the paper and the extent of changes, another review is needed before publication."
iclr_2018_By5ugjyCb,"All of the reviewers agree that the experimental results are promising and the proposed activation function enables a decent degree of quantization. However, the main concern with the approach is its limited novelty compared to previous work on clipped activation functions.

minor comments:
- Even though PACT is very similar to Relu, the names are very different.
- Please include a plot showing the proposed activation function as well.
"
iclr_2018_HJDV5YxCW,"All of the reviewers find the approach interesting, but they have reservations regarding the practical impact and empirical evaluation. The paper needs improvement both on the motivation and on the experimental results by including more baseline methods and neural architectures.
"
iclr_2018_SJgf6Z-0W,"All of the reviewers agree that the paper presents strong experimental results on continuous control benchmarks. The reviewers raised concerns regarding the analysis of the behavior of the algorithm, the possible impact of the technique, and requested more references and comparison with related work. The paper has significantly improved since the initial submission, but still not able fully satisfactory to the reviewers, partly due to the large extent of the changes needed.
"
iclr_2018_r1BRfhiab,"All of the reviewers have found some aspects of the formulation interesting, but they raised concerns regarding the practical use of the experimental setup.
"
iclr_2018_SJD8YjCpW,"An empirical study of weight sharing for neural networks is interesting, but all of the reviewers found the experiments insufficient without enough baseline comparisons."
iclr_2018_ByL48G-AW,"Evaluating simple baselines for continuous control is important and nearest neighbor search methods are interesting. However, the reviewers think that the paper lacks citation and comparison to some prior work and evaluation on more challenging benchmarks."
iclr_2018_rkw-jlb0W,"Dear authors,

While the reviewers appreciated your analysis, they all expressed concerns about the significance of the paper. Indeed, given the plethora of GAN variants, it would have been good to get stronger evidence about the advantages of the Dudley GAN. Even though I agree it is difficult to provide a clean comparison between generative models because of the lack of clear objectives, the LL on one dataset and images generated is limited. For instance, it would have been nice to show robustness results as this is a clear issue with GANs."
iclr_2018_SJtChcgAW,"Dear authors,

While the reviewers appreciated the idea, the significant loss of accuracy was a concern. Even though you made significant changes to the submission, it is unfortunately unrealistic to ask the reviewers to do another review of a heavily modified version in such a short amount of time.

Thus, I cannot accept this paper for publication but I encourage you to address the reviewers' concerns and resubmit at a later conference."
iclr_2018_Sy-tszZRZ,"Dear authors,

The reviewers appreciated your work and recognized the importance of theoretical work to understand the behaviour of deep nets. That said, the improvement over existing work (especially Montufar, 2017) is minor. This, combined with the limited attraction of such work, means that the paper will not be accepted.

I acknowledge the major modifications done but it is up to the reviewers to decide whether or not they agree to re-review a significantly updated version."
iclr_2018_H1l8sz-AW,"Dear authors,

Despite the desirable goal, that is to move away from regularization in parameter space toward regularization in function space, the reviewers all thought that the paper was not convincing enough, both in the choice of the particular regularization and in the experimental section.

While I appreciate that you have done a major rework of the paper, the rebuttal period should not be used for that and we can not expect the reviewers to do a complete re-review of a new version.

This paper thus cannot be accepted to ICLR."
iclr_2018_ry831QWAb,"The paper proposes to study the impact of normalizing the gradient for each layer before applying existing techniques such as SG + momentum, Adam or AdaGrad. The study is done on a reasonable number of datasets and, after the reviewers' comments, confidence intervals have been added,  although Table 1 puts results in bold but many of these results are not statistically significant.

The paper, however, lacks a proper analysis of the results. Two main things could be improved:
- Normalization does not always have the same effect but the reasons for it are not discussed. This needs not be done theoretically but a more thorough analysis would have been appreciated.
- There is no hyperparameter tuning, which means that the results are heavily dependent on which hyperparameters were chosen. Thus, it is hard to draw any conclusion.

Regarding the seemingly conflicting remarks of the two reviewers, it all depends on what the paper is trying to achieve. If it tries to show that is it state-of-the-art, then comparing to state-of-the-art algorithms on every dataset is crucial. If it tries to study the impact of one specific change, in this case layer normalization, on the optimization, then comparing to the vanilla version is fine. The paper seems to try to address the latter so it is OK if it is not compared to all the state-of-the-art algorithms. However, proper tuning of existing methods is still required.

Ultimately, a better understanding of layer normalization could be useful but the paper is not convincing enough to provide that understanding. There is no need to increase the number of datasets but it should rather focus on designing setups to test and validate hypotheses."
iclr_2018_H1pri9vTZ,"The idea of extending deep nets to infinite dimensional inputs is interesting but, as the reviewers noted, the execution does not have the quality we can expect from an ICLR publication. I encourage the authors to consider the meaningful comments that were made and modify the paper accordingly."
iclr_2018_rJma2bZCW,"Dear authors,

The reviewers agreed that the theoretical part lacked novelty and that the paper should focus on its experimental part which at the moment is not strong enough to warrant publication.

Regarding the theoretical part, here are the main concerns:
- Even though it is used in previous works, the continuous time approximation of stochastic gradient overlooks its practical behaviour, especially since a good rule of thumb is to use as large as stepsize as possible (without reaching divergence), as for instance mentioned in The Marginal Value of Adaptive Gradient Methods in Machine Learning by Wilson et al.
- The isotropic approximation is very strong and I don't know settings where this would hold. Since it seems central to your statements, I wonder what can be deduced from the obtained results.
- I do not think the Gaussian assumption is unreasonable and I am fine with it. Though there are clearly cases where this will not be true, it will probably be OK most of the time.

I encourage the authors to focus on the experimental part in a resubmission."
iclr_2018_rk3mjYRp-,"Dear authors,

The authors all agreed that this was an interesting topic but that the novelty, either theoretical or empirical, was lacking. This, the paper cannot be accepted to ICLR in its current state but I encourage the authors to make the recommended updates and to push their idea further."
iclr_2018_HyxjwgbRZ,"Dear authors,

After carefully reading the reviews, the rebuttal, and going through the paper, I regret to inform you that this paper does not meet the requirements for publication at ICLR.

While the variance analysis is definitely of interest, the reality of the algorithm does not match the claims. The theoretical rate is worse than that of SG but this could be an artefact of the analysis. Sadly, the experimental setup lacks in several ways:
- It is not yet clear whether escaping the saddle points is really an issue in deep learning as the loss function is still poorly understood.
- This analysis is done in the noiseless setting despite your argument being based around the variance of the gradients.
- You report the test error on CIFAR-10. While interesting and required for an ML paper, you introduce an optimization algorithm and so the quantity that matters the most is the speed at which you achieve a given training accuracy. Also, your table lists the value of the test accuracy rather than the speed of increase. Thus, you test the generalization ability of your algorithm while making claims about the optimization performance."
iclr_2018_B1uvH_gC-,"Dear authors,

Thank you for your submission to ICLR. Sadly, the reviewers were not convinced by the novelty of your approach nor by its experimental results. Thus, your paper cannot be accepted to ICLR."
iclr_2018_Sk0pHeZAW,"Dear authors,

I agree with the reviewers that the paper tries to do several things at once and the results are not that convincing. Overall, this work is mostly incremental, which is fine if there is no issue in the execution. Thus, I regret to inform you that this paper will not be accepted to ICLR."
iclr_2018_r1ISxGZRb,"The reviewers were uniformly unimpressed with the contributions of this paper. The method is somewhat derivative and the paper is quite long and lacks clarity. Moreover, the tactic of storing autoencoder variables rather than full samples is clearly an improvement, but it still does not allow the method to scale to a truly lifelong learning setting. "
iclr_2018_S14EogZAZ,"The authors present a toy stacking task where the goal is to stack blocks to match a given configuration, and a method that is a slightly modified DQN algorithm where the target configuration is observed by the network as well as the current state. There are a few problems with this paper. First, the method lacks novelty - it is very similar to DQN. Second, the claims of learning physical intuitions is not borne out by the method or experimental results. Third, the tasks are very simple and there is no held-out test set of target configurations. "
iclr_2018_rJssAZ-0-,"The paper proposes an extension to the reverse curriculum RL approach which uses a discriminator to label states as being on a goal trajectory or off the goal trajectory. The paper is well-written, with good empirical results on a number of task domains. However, the method relies on a number of assumptions on the ability of the agent to reset itself and the environment which are unrealistic and limiting, and beg the question as to why use the given method at all if this capability is assumed to exist. Overall, the method lacks significance and quality, and the motivation is not clear enough.
"
iclr_2018_BkeC_J-R-,"The proposed method combines supervised pretraining given some expert data and further uses the supervision to regularize the Q-updates to prevent the agent from exploring 'nonsense' directions. There a significant problems with the paper: the approach is not novel, the assumption of large amounts of expert data is problematic, and the claim of vastly accelerated learning is not supported empirically, either in the main paper or in the additional mujoco experiments added in the appendix."
iclr_2018_HktXuGb0-,"The paper presents a method for learning from expert state trajectories using a similarity metric in a learned feature space. The approach uses only the states, not the actions of the expert. The reviewers were variously dissatisfied with the novelty, the theoretical presentation, and the robustness of the approach. Though it empirically works better than the baselines (without expert demos) this is not surprising, especially since thousands of expert demonstrations were used. This would have been more impressive with fewer demonstrations, or more novelty in the method, or more evidence of robustness when the agent's state is far from the demonstrations."
iclr_2018_BJgVaG-Ab,"The authors make an argument for constructing an MDP from the formal structures of temporal logic and associated finite state automata and then applying RL to learn a policy for the MDP. This does not provide a solution for low-level skill composition, because there are discontinuities between states, but does provide a means for high level skill composition.

The reviewers agreed that the paper suffered from sloppy writing and unclear methods. They had concerns about correctness, and were not impressed by the novelty (combining TL and RL has been done previously). These concerns tip this paper to rejection."
iclr_2018_rJFOptp6Z,The authors propose a distillation-based approach that is applied to transfer knowledge from a classification network to non-classification tasks (face alignment and verification). The writing is very imprecise - for instance repeatedly referring to a 'simple trick' rather than actually defining the procedure - and the method is described in very task-specific ways that make it hard to understand how or whether it would generalize to other problems.
iclr_2018_Sktm4zWRb,"The authors have proposed a 'soft' version of VIN which is differentiable, where the cost function is trained by behavior cloning / imitation learning from expert/computer trajectories. The method is applied to a toy problem and to real historical data from mars rovers. The paper does not acknowledge nor compare against other methods, and the contribution is unclear, as is the justification for some of the aspects of the method.  Additionally it is difficult to interpret the relevance or significance of the results (45% correct)."
iclr_2018_S1xDcSR6W,"This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
iclr_2018_SJd0EAy0b,"This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
iclr_2018_S1viikbCW,"This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
iclr_2018_ryZ3KCy0W,"This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
iclr_2018_BJhxcGZCW,"The authors seem to miss important related literature for their comparison.
They also tuned hyperparameters and tested on the same validation set.
They should split between train/validation/test.

Reviews are just too low across the board to accept."
iclr_2018_ryA-jdlA-,"This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
iclr_2018_SkF2D7g0b,"The paper explores an increasingly important questions, especially showing the attack on existing APIs. The update to the paper has also improved it, but the paper is still not yet as impactful as it could be and needs much more comprehensive analysis to correctly appreciate its benefits and role."
iclr_2018_r1RF3ExCb,"This paper looks at  building new density estimation methods and new methods for tranformations and autoregressive models. The request from reviewers for comparison improves the paper. These models have seen a wide range of applications and have been highly successful, needing the added benefits shown and their potential impact to be expanded further."
iclr_2018_rkONG0xAW,"This is an interesting paper and addresses an important problem of neural networks with memory constrains. New experiments have been added that add to the paper, but the full impact of the paper is not yet realised, needing further exploration of models of current practice, wider set of experiments and analysis, and additional clarifying discussion."
iclr_2018_SJIA6ZWC-,"The paper is interesting, and the update to the paper and additional experiments has already improved it in many ways, but the paper still does still not have as much impact as it could, by further strengthening the comparisons and usefulness in many of situations of current practice."
iclr_2018_ByW5yxgA-,"The paper addresses and interesting problem, but the reviewers found that the paper is not as strong as it could be: improving the range of evaluated data (significantly improve the convincingness of the experiments, and clearly adressing any alternatives, their limitations and as baselines)."
iclr_2018_r1uOhfb0W,"This paper is interesting since it goes to showing the role of model averaging. The clarifications made improve the paper, but the impact of the paper is still not realised: the common confusion on the retraining can be re-examined, clarifications in the methodology and evaluation, and deeper contextulaisation of the wider literature."
iclr_2018_HJJ0w--0W,"This paper address the increasingly studied problem of predictions over long-term horizons. Despite this, and the important updates from the authors, the paper is not yeat ready and improvements identified include more control over the fair comparisons, improved clarity in exposition."
iclr_2018_Skx5txzb0W,"The subject of model evaluation will always be a contentious one, and the reviewers were not yet fully-convinced by the discussion. The points you bring up at the end of your rresponse already point to directions for improvement as well as a greater degree of precision and control."
iclr_2018_SkwAEQbAb,"The paper addresses the important question of determining the intrinsic dimensionality, but there remain several issue, which make the paper not ready at this point: unclear exposition, lack of contextualisation of existing work and seemingly limited insights. The reviewers have provided many suggestions to improve the paper which we hope will be useful to improve the paper."
iclr_2018_rJ5C67-C-,"While there are some interesting and novel aspects in this paper, none of the reviewers recommends acceptance."
iclr_2019_B1gabhRcYX,"The first reviewer summarizes the contribution well: This paper combines [a CNN that computes both a multi-scale feature pyramid and a depth prediction, which is expressed as a linear combination of ""depth bases""]. This is used to [define a dense re-projection error over the images, akin to that of dense or semi-dense methods]. [Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM). By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.] 

Strengths:
While combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting. The authors accounted for reviewer feedback during the review cycle and improved the manuscript leading to an increased rating. 

Weaknesses:
Weaknesses were addressed during the rebuttal including better evaluation of their predicted lambda and comparison with CodeSLAM.

Contention:
This paper was not particularly contentious, there was a score upgrade due to the efforts of the authors during the rebuttal period.

Consensus:
This paper addresses an interesting  area of research at the intersection of geometric computer vision and deep learning and should be of considerable interest to many within the ICLR community. The discussion of the paper highlighted some important nuances of terminology regarding the characterization of different methods. This paper was also rated the highest in my batch. As such, I recommend this paper for an oral presentation. "
iclr_2019_B1l08oAct7,"The manuscript proposes deterministic approximations for Bayesian neural networks as an alternative to the standard Monte-Carlo approach. The results suggest that the deterministic approximation can be more accurate than previous methods. Some explicit contributions include efficient moment estimates and empirical Bayes procedures. 

The reviewers and ACs note weakness in the breadth and complexity of models evaluated, particularly with regards to ablation studies. This issue seems to have been addressed to the reviewer's satisfaction by the rebuttal. The updated manuscript also improves references to related prior work.

Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. We recommend acceptance."
iclr_2019_B1l6qiR5F7,"This paper presents a substantially new way of introducing a syntax-oriented inductive bias into sentence-level models for NLP without explicitly injecting linguistic knowledge. This is a major topic of research in representation learning for NLP, so to see something genuinely original work well is significant. All three reviewers were impressed by the breadth of the experiments and by the results, and this will clearly be among the more ambitious papers presented at this conference.

In preparing a final version of this paper, though, I'd urge the authors to put serious further effort into the writing and presentation. All three reviewers had concerns about confusing or misleading passages, including the title and the discussion of the performance of tree-structured models so far."
iclr_2019_B1xsqj09Fm,The paper proposes a set of tricks leading to a new SOTA for sampling high resolution images. It is clearly written and the presented contribution will be of high interest for practitioners.
iclr_2019_Bklr3j0cKX,"This paper proposes a new unsupervised learning approach based on maximizing the mutual information between the input and the representation. The results are strong across several image datasets. Essentially all of the reviewer's concerns were directly addressed in revisions of the paper, including additional experiments. The only weakness is that only image datasets were experimented with; however, the image-based experiments and comparisons are extensive. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation."
iclr_2019_ByeZ5jC5YQ,"The paper presents a novel strategy for statistically motivated feature selection i.e. aimed at controlling the false discovery rate. This is achieved by extending knockoffs to complex predictive models and complex distributions via (multiple) generative adversarial networks. 

The reviewers and ACs noted weakness in the original submission which seems to have been fixed after the rebuttal period -- primary related to missing experimental details. There was also some concern (as is common with inferential papers) that the claims are difficult to evaluate on real data, as the ground truth is unknown. To this end, the authors provide empirical results with simulated data that address this issue. There is also some concern that more complex predictive models are not evaluated.

Overall the reviewers and AC have a positive opinion of this paper and recommend acceptance."
iclr_2019_Byg3y3C9Km,"This paper presents a differentiable simulator for protein structure prediction that can be trained end-to-end. It makes several contributions to this research area. Particularly training a differentiable sampling simulator could be of interest to a wider community.

The main criticism comes from the clarity for the machine learning community and empirical comparison with the state-of-the-art methods. The authors' feedback addressed a few  confusions in the description, and I recommend the authors to further polish the text for better readability. R4 argues that a good comparison with the state-of-the-art method in this field would be difficult and the comparison with an RNN baseline is rigorously carried out.

After discussion, all reviewers agree that this paper deserves a publication at ICLR."
iclr_2019_Bygh9j09KX,"This paper proposes a hypothesis about the kinds of visual information for which popular neural networks are most selective.  It then proposes a series of empirical experiments on synthetically modified training sets to test this and related hypotheses.  The main conclusions of the paper are contained in the title, and the presentation was consistently rated as very clear.  As such, it is both interesting to a relatively wide audience and accessible.

Although the paper is comparatively limited in theoretical or algorithmic contribution, the empirical results and experimental design are of sufficient quality to inform design choices of future neural networks, and to better understand the reasons for their current behavior.

The reviewers were unanimous in their appreciation of the contributions, and all recommended that the paper be accepted.

"
iclr_2019_H1xSNiRcF7,"The manuscript presents a promising new algorithm for learning geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures. The manuscript builds on the build on the box lattice model, extending prior work by relaxing the box embeddings via Gaussian convolutions. This is shown to be particularly effective for non-overlapping boxes, where the previous method fail.

The primary weakness identified by reviewers was the writing, which was thought to be lacking some context, and may be difficult to approach for the non-domain expert. This can be improved by including an additional general introduction. Otherwise, the manuscript was well written.

Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. In our opinion, this paper is a clear accept."
iclr_2019_HJx54i05tX,"This paper analyzes random auto encoders in the infinite dimension limit with an assumption that the weights are tied in the encoder and decoder. In the limit the paper is able to show the random auto encoder transformation  as doing an approximate inference on data. The paper is able to obtain principled initialization strategies for training deep autoencoders using this analysis, showing the usefulness of their analysis. Even though there are limitations of paper such as studying only random models, and characterizing them only in the limit, all the reviewers agree that the analysis is novel and gives insights on an interesting problem. "
iclr_2019_HkNDsiC9KQ,"The reviewers all agree that the idea is interesting, the writing clear and the experiments sufficient. 

To improve the paper, the authors should consider better discussing their meta-objective and some of the algorithmic choices. "
iclr_2019_HygBZnRctX,"This paper proposes an approach for learning to transfer knowledge across multiple tasks. It develops a principled approach for an important problem in meta-learning (short horizon bias). Nearly all of the reviewer's concerns were addressed throughout the discussion phase. The main weakness is that the experimental settings are somewhat non-standard (i.e. the Omniglot protocol in the paper is not at all standard). I would encourage the authors to mention the discrepancies from more standard protocols in the paper, to inform the reader. The results are strong nonetheless, evaluating in settings where typical meta-learning algorithms would struggle. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation."
iclr_2019_HylzTiC5Km,"All reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models. The AC concurs.
"
iclr_2019_S1x4ghC9tQ,The reviewers agree that this is a novel paper with a convincing evaluation.
iclr_2019_S1xq3oR5tQ,The paper advocates neuroscience-based V1 models to adapt CNNs.  The results of the simulations are convincing from a neuroscience-perspective.  The reviewers equivocally recommend publication.
iclr_2019_SkVhlh09tX,"Very solid work, recognized by all reviewers as worthy of acceptance. Additional readers also commented and there is interest in the open source implementation that the authors promise to provide."
iclr_2019_r1lYRjC9F7,"All reviewers agree that the presented audio data augmentation is very interesting, well presented, and clearly advancing the state of the art in the field. The authors’ rebuttal clarified the remaining questions by the reviewers. All reviewers recommend strong acceptance (oral presentation) at ICLR. I would like to recommend this paper for oral presentation due to a number of reasons including the importance of the problem addressed (data augmentation is the only way forward in cases where we do not have enough of training data), the novelty and innovativeness of the model, and the clarity of the paper. The work will be of interest to the widest audience beyond ICLR."
iclr_2019_r1xlvi0qYm,"Well-written paper that motivates through theoretical analysis new memory writing methods in memory augmented neural networks. Extensive experimental analysis support and demonstrate the advantages of the new solutions over other recurrent architectures.
Reviewers suggested extension and clarification of the analysis presented in the paper, for example, for different memory sizes. The paper was revised accordingly. Another important suggestion was considering ACT as a baseline. Authors explained clearly why it wasn't considered as a baseline, and updated the paper to include references and explanations in the paper as well."
iclr_2019_rJEjjoR9K7,"The paper presents a new approach for domain generalization whereby the original supervised model is trained with an explicit objective to ignore so called superficial statistics present in the training set but which may not be present in future test sets. The paper proposes using a differentiable variant of gray-level co-occurrence matrix to capture the textural information and then experiments with two techniques for learning feature invariance. All reviewers agree the approach is novel, unique, and potentially high impact to the community. 

The main issues center around reproducibility as well as the intended scope of problems this approach addresses. The authors have offered to include further discussions in the final version to address these points. Doing so will strengthen the paper and aid the community in building upon this work. "
iclr_2019_rJVorjCcKQ,"The authors propose a new method of securely evaluating neural networks. 

The reviewers were unanimous in their vote to accept. The paper is very well written, the idea is relatively simple, and so it is likely that this would make a nice presentation."
iclr_2019_rJgMlhRctm,"Strong paper in an interesting new direction.
More work should be done in this area."
iclr_2019_rJl-b3RcF7,"The authors posit and investigate a hypothesis -- the “lottery ticket hypothesis” -- which aims to explain why overparameterized neural networks are easier to train than their sparse counterparts. Under this hypothesis, randomly initialized dense networks are easier to train because they contain a larger number of “winning tickets”.
This paper received very favorable reviews, though there were some notable points of concern. The reviewers and the AC appreciated the detailed and careful experimentation and analysis. However, there were a couple of points of concern raised by the reviewers: 1) the lack of experiments conducted on large-scale tasks and models, and 2) the lack of a clear application of the idea beyond what has been proposed previously. 

Overall, this is a very interesting paper with convincing experimental validation and as such the AC is happy to accept the work."
iclr_2019_rJxgknCcK7,"This paper proposes the use of recently propose neural ODEs in a flow-based generative model. 

As the paper shows, a big advantage of a neural ODE in a generative flow is that an unbiased estimator of the log-determinant of the mapping is straightforward to construct. Another advantage, compared to earlier published flows, is that all variables can be updated in parallel, as the method does not require ""chopping up"" the variables into blocks.  The paper shows significant improvements on several benchmarks, and seems to be a promising venue for further research.

A disadvantage of the method is that the authors were unable to show that the method could produce results that were similar (of better than) the SOTA on the more challenging benchmark of CIFAR-10. Another downside is its computational cost. Since neural ODEs are relatively new, however, these problems might resolved with further refinements to the method. "
iclr_2019_ryGs6iA5Km,"Graph neural networks are an increasingly popular topic of research in machine learning, and this paper does a good job of studying the representational power of some newly proposed variants. The framing of the problem in terms of the WL test, and the proposal of the GIN architecture is a valuable contribution. Through the reviews and subsequent discussion, it looks like the issues surrounding Theorem 3 have been resolved, and therefore all of the reviewers now agree that this paper should be accepted. There may be some interesting followup work based on studying depth, as pointed out by reviewer 1, but this may not be an issue in GIN and is regardless a topic for future research."
iclr_2019_B1G5ViAqFm,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.
 
- The paper tackles an interesting and challenging problem with a novel approach.
- The method gives improves improved performance for the surface reconstruction task.

2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

The paper
- lacks clarity in some areas
- doesn't sufficiently explain the trade-offs between performing all computations in the spectral domain vs the spatial domain. 

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

Reviewers had a divergent set of concerns. After the rebuttal, the remaining concerns were:
- the significance of the performance improvements. The AC believes that the quantitative and qualitative results in Table 3 and Figures 5 and 6 show significant improvements with respect to two recent methods.
- a feeling that the proposed method could have been more efficient if more computations were done in the spectral domain. This is a fair point but should be considered as suggestions for improvement and future work rather than grounds for rejection in the AC's view.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers did not reach a consensus. The final decision is aligned with the more positive reviewer, AR1, because AR1 was more confident in his/her review and because of the additional reasons stated in the previous section.
"
iclr_2019_B1G9doA9F7,"The authors propose a method for low-resource domain adaptation where the number of examples available in the target domain are limited. The proposed method modifies the basic approach in a CycleGAN by augmenting it with a “content” (task-specific) loss, instead of the standard reconstruction error. The authors also demonstrate experimentally that it is important to enforce the loss in both directions (target → source and source --> target). Experiments are conducted on both supervised as well as unsupervised settings.
The main concern expressed by the reviewers relates to the novelty of the approach since it is a relatively straightforward extension of CycleGAN/CyCADA, but in the view of a majority of reviewers the work serves a useful contribution as a practical method for developing systems in low-resource conditions where it is feasible to label a few new instances. Although the reviewers were not unanimous in their recommendations, on balance in the view of the AC the work is a useful contribution with clear and detailed experiments in the revised version.
"
iclr_2019_B1GAUs0cKQ,The authors describe a very counterintuitive type of layer: one with mean zero Gaussian weights. They show that various Bayesian deep learning algorithms tend to converge to layers of this variety. This work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods. 
iclr_2019_B1GMDsR5tm,"The paper investigates a novel initialisation method to improve Equilibrium Propagation.  In particular, the results are convincing, but the reviewers remain with small issues here and there.

An issue with the paper is the biological plausibility of the approach.  Nonetheless publication is recommended.  "
iclr_2019_B1MXz20cYQ,"Important problem (explainable AI); sensible approach, one of the first to propose a method for the counter-factual question (if this part of the input were different, what would the network have predicted). Initially there were some concerns by the reviewers but after the author response and reviewer discussion, all three recommend acceptance (not all of them updated their final scores in the system). "
iclr_2019_B1VZqjAcYX,"This method proposes a criterion (SNIP) to prune neural networks before training.  The pro is that SNIP can find the architecturally important parameters in the network without full training. The con is that SNIP only evaluated on small datasets (mnist, cifar, tiny-imagenet) and it's uncertain if the same heuristic works on large-scale dataset. Small datasets can always achieve high pruning ratio, so evaluation on ImageNet is quite important for pruning work. The reviewers have consensus on accept. The authors are recommended to compare with previous work [1][2] to make the paper more convincing. 

[1] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NIPS, 2015.

[2] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 2016."
iclr_2019_B1e0X3C9tQ,"The reviewers acknowledge the value of the careful analysis of Gaussian encoder/decoder VAE presented in the paper. The proposed algorithm shows impressive FID scores that are comparable to those obtained by state of the art GANs. The paper will be a valuable addition to the ICLR program. 

"
iclr_2019_B1exrnCcF7,All reviewers agree that the proposed method interesting and well presented. The authors' rebuttal addressed all outstanding raised issues. Two reviewers recommend clear accept and the third recommends borderline accept. I agree with this recommendation and believe that the paper will be of interest to the audience attending ICLR. I recommend accepting this work for a poster presentation at ICLR.
iclr_2019_B1ffQnRcKX,"
pros:
- the paper is well-written and presents a nice framing of the composition problem
- good comparison to prior work
- very important research direction

cons:
- from an architectural standpoint the paper is somewhat incremental over Routing Networks [Rosenbaum et al]
- as Reviewers 2 and 3 point out, the experiments are a bit weak, relying on heuristics such as a window over 3 symbols in the multi-lingual arithmetic case, and a pre-determined set of operations (scaling, translation, rotation, identity) in the MNIST case.

As the authors state, there are three core ideas in this paper (my paraphrase):

(1) training on a set of compositional problems (with the right architecture/training procedure) can encourage the model to learn modules which can be composed to solve new problems, enabling better generalization. 
(2) treating the problem of selecting functions for composition as a sequential decision-making problem in an MDP
(3) jointly learning the parameters of the functions and the (meta-level) composition policy.

As discussed during the review period, these three ideas are already present in the Routing Networks (RN) architecture of Rosenbaum et al.  However CRL offers insights and improvements over RN algorithmically in a several ways:

(1) CRL uses a curriculum learning strategy.  This seems to be key in achieving good results and makes a lot of sense for naturally compositional problems.
(2) The focus in RN was on using the architecture to solve multi-task problems in object recognition. The solutions learned in image domains while ""compositional"" are less clearly interpretable.  In this paper (CRL) the focus is more squarely on interpretable compositional tasks like arithmetic and explores extrapolation.
(3) The RN architecture does support recursion (and there are some experiments in this mode) but it was not the main focus.  In this paper (CRL) recursion is given a clear, prominent role.

I appreciate that the authors' engagement in the discussion period. My feeling is that  the paper offers nice improvements, a useful framing of the problem, a clear recursive formulation, and a more central focus on naturally compositional problems.  I am recommending the paper for acceptance but suggest that the authors remove or revise their contributions (3) and (4) on pg. 2 in light of the discussion on routing nets.

Routing Networks, Adaptive Selection of Non-Linear Functions for Multi-task Learning, ICLR 2018"
iclr_2019_B1fpDsAqt7,"Important problem (modular & interpretable approaches for VQA and visual reasoning); well-written manuscript, sensible approach. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance. "
iclr_2019_B1g30j0qF7,"There has been a recent focus on proving the convergence of Bayesian fully connected networks to GPs. This work takes these ideas one step further, by proving the equivalence in the convolutional case.

All reviewers and the AC are in agreement that this is interesting and impactful work. The nature of the topic is such that experimental evaluations and theoretical proofs are difficult to carry out in a convincing manner, however the authors have done a good job at it, especially after carefully taking into account the reviewers’ comments.

"
iclr_2019_B1gTShAct7,"Pros:
- novel method for continual learning
- clear, well written
- good results
- no need for identified tasks
- detailed rebuttal, new results in revision

Cons:
- experiments could be on more realistic/challenging domains

The reviewers agree that the paper should be accepted."
iclr_2019_B1gstsCqt7,"While there has been lots of previous work on training dictionaries for sparse coding, this work tackles the problem of doing son in a purely local way. While previous work suggests that the exact computation of gradient addressed in the paper is not necessarily critical, as noted by reviewers, all reviewers agree that the work still makes important contributions through both its theoretical analyses and presented experiments. Authors are encouraged to work on improving clarity further and delineating their contribution more precisely with respect to previous results."
iclr_2019_B1lKS2AqtX,"Strengths: Strong results on future frame video prediction using a 3D convolutional network. Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. Good ablation study.

Weaknesses: Comparisons with older action recognition methods. Some concerns about novelty, the main contribution is the E3D-LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism. 

Contention: Authors point to novelty in 3D convolutions inside the RNN.

Consensus: All reviewers give a final score of 7- well done experiments helped address concerns around novelty. Easy to recommend acceptance given the agreement.
"
iclr_2019_B1lnzn0ctQ,This is a well executed paper that makes clear contributions to the understanding of unrolled iterative optimization and soft thresholding for sparse signal recovery with neural networks.
iclr_2019_B1lz-3Rct7,"Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"
iclr_2019_B1xJAsA5F7,"The revisions made by the authors convinced the reviewers to all recommend accepting this paper. Therefore, I am recommending acceptance as well. I believe the revisions were important to make since I concur with several points in the initial reviews about additional baselines. It is all too easy to add confusion to the literature by not including enough experiments. "
iclr_2019_B1xVTjCqKQ,"This paper studies deep convolutional architectures to perform compressive sensing of natural images, demonstrating improved empirical performance with an efficient pipeline. 
Reviewers reached a consensus that this is an interesting contribution that advances data-driven methods for compressed sensing, despite some doubts about the experimental setup and the scope of the theoretical insights. We thus recommend acceptance as poster. "
iclr_2019_B1xWcj0qYm,"This paper studies the task of learning a binary classifier from only unlabeled data. They first provide a negative result, i.e., they show it is impossible to learn an unbiased estimator from a set of unlabeled data. Then they provide an empirical risk minimization method which works when given two sets of unlabeled data, as well as the class priors. 

The four submitted reviews were unanimous in their vote to accept. The results are impactful, and might make for an interesting oral presentation."
iclr_2019_B1xY-hRctX,"
pros:
- The paper presents an interesting forward chaining model which makes use of meta-level expansions and reductions on predicate arguments in a neat way to reduce complexity.  As Reviewer 3 points out, there are a number of other papers from the neuro-symbolic community that learn relations (logic tensor networks is one good reference there). However using these meta-rules you can mix predicates of different arities in a principled way in the construction of the rules, which is something I haven't seen.
- The paper is reasonably well written (see cons for specific issues)
- There is quite a broad evaluation across a number of different tasks.  I appreciated that you integrated this into an RL setting for tasks like blocks world.
- The results are good on small datasets and generalize well

cons:
- (scalability) As both Reviewers 1 and 3 point out, there are scalability issues as a function of the predicate arity in computing the set of permutations for the output predicate computation.
- (interpretability) As Reviewer 2 notes, unlike del-ILP, it is not obvious how symbolic rules can be extracted.  This is an important point to address up front in the text. 
- (clarity) The paper is confusing or ambiguous in places:

-Initially I read the 1,2,3 sequence at the top of 3 to be a deduction (and was confused) rather than three applications of the meta-rules.  Maybe instead of calling that section ""primitive logic rules"" you can call them ""logical meta-rules"".

-Another confusion, also mentioned by reviewer 3 is that you are assuming that free variables (e.g. the ""x"" in the expression ""Clear(x)"") are implicitly considered universally quantified in your examples but you don't say this anywhere.  If I have the fact ""Clear(x)"" as an input fact, then presumably you will interpret this as ""for all x Clear(x)"" and provide an input tensor to the first layer which will have all 1.0's along the ""Clear"" relation dimension, right?

-It seems that you are making the assumption that you will never need to apply a predicate to the same object in multiple arguments?  If not, I don't see why you say that the shape of the tensor will be m x (m-1) instead of m^2.  You need to be able to do this to get reflexivity for example: ""a <= a"".

-I think you are implicitly making the closed world assumption (CWA) and should say so.

-On pg. 4 you say ""The facts are tensors that encode relations among multiple objectives, as described in Sec. 2.2."".  What do you mean by ""objectives""?  I would say the facts are tensors that encode relations among multiple objects.

-On pg. 5 you say ""We finish this subsection, continuing with the blocks world to illustrate the forward
propagation in NLM"".  I see no mention of blocks world in this paragraph. It just seems like a description of what happens at one block, generically.

-In many places you say that this model can compute deduction on first-order predicate calculus (FOPC) but it seems to me that you are limited to horn logic (rule logic) in which there is at most one positive literal per clause (i.e. rules of the form: b1 AND b2 AND ... AND bn => h).  From what I can tell you cannot handle deduction on clauses such as b1 AND b2 => h1 or (h2 and h3).

-There is not enough description of the exact setup for each experiment. For example in blocks world, how do you choose predicates for each layer?  How many exactly for each experiment?  You make it seem on p3 that you can handle recursive predicates but this seems to not have been worked out completely in the appendix.  You should make this clear.

-In figure 1 you list Move as if its a predicate like On but it's a very different thing. On is  predicate describing a relation in one state.  Move is an action which updates a state by changing the values of predicates.  They should not be presented in the same way.

-You use ""min"" and ""max"" for ""and"" and ""or"" respectively.  Other approaches have found that using the product t-norm t-norm(x,y) = x * y helps with gradient propagation.  del-ILP discusses this in more detail on p 19.  Did you try these variations?

-I think it would be helpful to somewhere explicitly describe the actual MLP model you use for deduction including layer sizes and activation functions.

-p. 5. typo: ""Such a parameter sharing mechanism is crucial to the generalization ability of NLM to
problems ov varying sizes."" (""ov"" -> ""of"")

-p. 6. sec 3.1 typo: ""For ∂ILP, the set of pre-conditions of the symbols is used direclty as input of the system."" (""direclty"" -> ""directly"")

I think this is a valuable contribution and novel in the particulars of the architecture (eg. expand/reduce) and am recommending acceptance.  But I would like to see a real effort made to sharpen the writing and make the exposition crystal clear.  Please in particular pay attention to Reviewer 3's comments.

"
iclr_2019_B1xf9jAqFQ,"The authors obtain nice speed improvements by learning to skip and jump over input words when processing text with an LSTM. At some points the reviewers considered the work incremental since similar ideas have already been explored, but at the end two of the reviewers ended up endorsing the paper with strong support."
iclr_2019_B1xhQhRcK7,"
* Strengths

The paper addresses a timely topic, and reviewers generally agreed that the approach is reasonable and the experiments are convincing. Reviewers raised a number of specific concerns (which could be addressed in a revised version or future work), described below.

* Weaknesses

Some reviewers were concerned the baselines are weak. Several reviewers were concerned that relying on failures observed during training could create issues by narrowing the proposal distribution (Reviewer 3 characterizes this in a particularly precise manner). In addition, there was a general feeling that more steps are needed before the method can be used in practice (but this could be said of most research).

* Recommendation

All reviewers agreed that the paper should be accepted, although there was also consensus that the paper would benefit from stronger baselines and more close attention to issues that could be caused by an overly narrow proposal distribution. The authors should consider addressing or commenting on these issues in the final version."
iclr_2019_BJG0voC9YQ,see my comment to the authors below
iclr_2019_BJe-DsC5Fm,"This is a solid paper that proposes and analyzes a sound approach to zero order optimization, covering a variants of a simple base algorithm.  After resolving some issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Some concerns regarding the necessity for such algorithms persisted, but the connection to adversarial examples provides an interesting motivation."
iclr_2019_BJe0Gn0cY7,"Strengths:  The proposed method is relatively principled.  The paper also demonstrates a new ability: training VAEs with autoregressive decoders that have meaningful latents.  The paper is clear and easy to read.

Weaknesses:  I wasn't entirely convinced by the causal/anticausal formulation, and it's a bit unfortunate that the decoder couldn't have been copied without modification from another paper.

Points of contention:
It's not clear how general the proposed approach is, or how important the causal/anti-causal idea was, although the authors added an ablation study to check this last question.

Consensus:  All reviewers rated the paper above the bar, and the objections of the two 6's seem to have been satisfactorily addressed by the rebuttal and paper update."
iclr_2019_BJe1E2R5KX,"This paper proposes model-based reinforcement learning algorithms that have theoretical guarantees. These methods are shown to good results on Mujuco benchmark tasks. All of the reviewers have given a reasonable score to the paper, and the paper can be accepted."
iclr_2019_BJeOioA9Y7,"The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients. The authors have done thorough experiments and the value of the approach seems clear, especially compared against progressive nets and pathnets. The paper is well-written and interesting, and the approach is novel. The reviewers have discussed the paper in detail and agree, with the AC, that it should be accepted."
iclr_2019_BJeWUs05KQ,"This paper proposes an approach for imitation learning from unsegmented demonstrations. The paper addresses an important problem and is well-motivated. Many of the concerns about the experiments have been addressed with follow-up comments. We strongly encourage the authors to integrate the new results and additional literature to the final version. With these changes, the reviewers agree that the paper exceeds the bar for acceptance. Thus, I recommend acceptance."
iclr_2019_BJej72AqF7,"While the reformulation of RNNs is not practical as it is missing sigmoids and tanhs that are common in LSTMs it does provide an interesting analysis of traditional RNNs and a technique that's novel for many in the ICLR community.
"
iclr_2019_BJemQ209FQ,"All reviewers (including those with substantial expertise in RL) were solid in their praise for this paper that is also tackling an interesting application that is much less well studied but deserves attention.

"
iclr_2019_BJfIVjAcKm,"This paper introduced a concept called ReLU stability to motivate regularization and enable fast verification. Most of the analysis was presented empirically on two simple datasets and with low-performing models. I feel theoretical analysis and more comprehensive and realistic empirical studies would make the paper stronger. In general, the contribution of this paper is original and interesting. 
"
iclr_2019_BJfOXnActQ,"The reviewers think that incorporating class conditional dependencies into the metric space of a few-shot learner is a sufficiently good idea to merit acceptance. The performance isn’t necessarily better than the state-of-the-art approaches like LEO, but it is nonetheless competitive. One reviewer suggests incorporating a pre-training strategy to strengthen your results. In terms of experimental details, one reviewer pointed out that the embedding network architecture is quite a bit more powerful than the base learner and would like some additional justification for this. They would also like more detail on the computing the MAML gradients in the context of this method. Beyond this, please ensure that you have incorporated all of the clarifications that were required during the discussion phase."
iclr_2019_BJfYvo09Y7,"A hierarchical method is presented for developing humanoid motion control,
using low-level control fragments, egocentric visual input, recurrent high-level control.
It is likely the first demonstration of 3D humanoids learning to do memory-enabled tasks using only
proprioceptive and head-based ego-centric vision. The use of control fragments as opposed
to mocapclip-based skills allows for finer-grained repurposing of pieces of motion, while
still allowing for mocap-based learning

Weaknesses: It is largely a mashup up of previously known results (R2).  Caveat: this can be said for all research
at some sufficient level of abstraction. The motions are jerky when transitions happen between control fragments (R2,R3).
There are some concerns as to whether the method compares against other methods; the authors note
that they are either not directly comparable, i.e., solving a different problem, or are implicitly
contained in some of the comparisons that are performed in the paper.

Overall, the reviewers and AC are in broad agreement regarding the strengths and weaknesses of the paper.

The AC believes that the work will be of broad interest. Demonstrating memory-enabled, vision-driven,
mocap-imitating skills is a broad step forward. The paper also provides a further datapoint as 
to which combinations of method work well, and some of the specific features required to make them work.

The paper could acknowledge motion quality artifacts, as noted by the reviewers and 
in the online discussion.  Suggest to include  [Peng et al 2017] as some of the most relevant related HRL humanoid control work, as per the reviews & discussion.

"
iclr_2019_BJg4Z3RqF7,"This paper proposes a GAN-based method to recover images from a noisy version of it. The paper builds upon existing works on AmbientGAN and CS-GAN. By combining the two approaches, the work finds a new method that performs better than existing approaches.

The paper clearly has new interesting ideas which have been executed well. Two of the reviewers have voted in favour of acceptance, with one of the reviewer providing an extensive and detailed review. The third reviewer however has some doubts which were not resolved completely after the rebuttal.

Upon reading the work myself, I am convinced that this will be interesting to the community. However, I will recommend the authors to take the comments of Reviewer 2 into account and do whatever it takes to resolve issues pointed by the reviewer.

During the review process, another related work was found to be very similar to the approach discussed in this work. This work should be cited in the paper, as a prior work that the authors were unaware of. 
https://arxiv.org/abs/1812.04744
Please also discuss any new insights this work offers on top of this existing work.

Given that the above suggestions are taken into account, I recommend to accept this paper.
"
iclr_2019_BJg9DoR9t7,"This paper proposes an interesting approach to leveraging crowd-sourced labels, along with an ML model learned from the data itself. 

The reviewers were unanimous in their vote to accept."
iclr_2019_BJgK6iA5KX,The paper suggests using meta-learning to tune the optimization schedule of alternative optimization problems. All of the reviewers agree that the paper is worthy of publication at ICLR. The authors have engaged with the reviewers and improved the paper since the submission. I asked the authors to address the rest of the comments in the camera ready version.
iclr_2019_BJgLg3R9KQ,"This paper presents a large-scale annotation of human-derived attention maps for ImageNet dataset. This annotation can be used for training more accurate and more interpretable attention models (deep neural networks) for object recognition. All reviewers and AC agree that this work is clearly of interest to ICLR and that extensive empirical evaluations show clear advantages of the proposed approach  in terms of improved classification accuracy. In the initial review, R3 put this paper below the acceptance bar requesting major revision of the manuscript and addressing three important weaknesses: (1) no analysis on interpretability; (2) no details about statistical analysis; (3) design choices of the experiments are not motivated. Pleased to report that based on the author respond, the reviewer was convinced that the most crucial concerns have been addressed in the revision. R3 subsequently increased assigned score to 6. As a result, the paper is not in the borderline bucket anymore.
The specific recommendation for the authors is therefore to further revise the paper taking into account a better split of the material in the main paper and its appendix. The additional experiments conducted during rebuttal (on interpretability) would be better to include in the main text, as well as explanation regarding statistical analysis. 
"
iclr_2019_BJgRDjR9tQ,"
* Strengths

This paper presents a very interesting connection between GANs and robust estimation in the presence of corrupted training data. The conceptual ideas are novel and can likely be extended in many further directions. I would not be surprised if this opens up a new line of research.

* Weaknesses

The paper is poorly written. Due to disagreement among the authors and my interest in the topic, I read the paper in detail myself. I think it would be difficult for a non-expert to understand the key ideas and I strongly encourage the authors to carefully revise the paper to reach a broader audience and highlight the key insights. Additionally, the experiments are only on toy data.

* Discussion

One of the reviewers was concerned about the lack of efficiency guarantees for the proposed algorithm (indeed, the algorithm requires training GANs which are currently beyond the reach of theory and finicky in practice). That reviewer points to the fact that most papers in the robustness literature are concerned with computational efficiency and is concerned that ignoring this sidesteps one of the key challenges. The reviewer is also concerned about the restriction to parametric or nearly-parametric families (e.g. Gaussians and elliptical distributions). Other reviewers were more positive and did not see these as major issues.

* Decision

In my opinion, the lack of efficiency guarantees is not a huge issue, as the primary contribution of the paper is pointing out a non-obvious conceptual connection between two literatures. The restriction to parametric families is more concerning, but it seems possible this could be removed with further developments. The main reason for accepting the paper (despite concerns about the writing) is the importance of the conceptual connection. I think this connection is likely to lead to a new line of research and would like to get it out there as soon as possible.

* Comments

Despite the accept decision, I again urge the authors to improve the quality of exposition to ensure that a large audience can appreciate the ideas."
iclr_2019_BJg_roAcK7,"This manuscript proposes a new algorithm for instance-wise feature selection. To this end, the selection is achieved by combining three neural networks trained via an actor-critic methodology. The manuscript highlight that beyond prior work, this strategy enables the selection of a different number of features for each example. Encouraging results are provided on simulated data in comparison to related work, and on real data.

The reviewers and AC note issues with the evaluation of the proposed method. In particular, the evaluation of computer vision and natural language processing datasets may have further highlighted the performance of the proposed method. Further, while technically innovative, the approach is closely related to prior work (L2X) -- limiting the novelty. 

The paper presents a promising new algorithm for training generative adversarial networks. The mathematical foundation for the method is novel and thoroughly motivated, the theoretical results are non-trivial and correct, and the experimental evaluation shows a substantial improvement over the state of the art."
iclr_2019_BJgklhAcK7,"This work builds on MAML by (1) switching from a single underlying set of parameters to a distribution in a latent lower-dimensional space, and (2) conditioning the initial parameter of each subproblem on the input data.
All reviewers agree that the solid experimental results are impressive, with careful ablation studies to show how conditional parameter generation and optimization in the lower-dimensional space both contribute to the performance. While there were some initial concerns on clarity and experimental details, we feel the revised version has addressed those in a satisfying way."
iclr_2019_BJgqqsAct7,"The paper combines PAC-Bayes bound with network compression to derive a generalization bound for large-scale neural nets such as ImageNet. The approach is novel and interesting and  the paper is well-written. The authors provided detailed replies and improvements in response to reviewers questions, and all reviewers agree this is a very nice contribution."
iclr_2019_BJl6AjC5F7,"This paper investigates learning to represent edit operations for two domains: text and source code. The primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits). The technical novelty is relatively weak.

Pros:
The paper introduces a new dataset for source code edits.  

Cons:
Reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. As a result, R3 updated their score from 4 to 6. 

Verdict:
Possible weak accept. None of the remaining issues after the rebuttal is a serious deal breaker (e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real-world application of the automatic edits). However, the overall impact and novelty of the paper is relatively weak."
iclr_2019_BJl6TjRcY7,"Strengths:  One-shot physics-based imitation at a scale and with efficiency not seen before.
Clear video, paper, and related work.

Weaknesses described include:  the description of a secondary contribution (LFPC) 
takes up too much space (R1,4); results are not compelling (R1,4); prior art in graphics and robotics (R2,6);
concerns about the potential limitations of the linearization used by LFPC.

The original reviews are negative overall (6,3,4). The authors have posted detailed replies.
R1 has posted a followup, standing by their score. We have not heard more from R2 and R3.

The AC has read the paper, watched the video, and read all the reviews.
Based on expertise in this area, the AC endorses the author's responses to R1 and R2. 
Being able to compare LFPC to more standard behavior cloning is a valuable data point for the community; 
there is value in testing simple and efficient models first.
The AC identifies the following recent (Nov 2018) paper as being the closest work, which is not identified by the authors or the reviewers. The approach being proposed in the submitted paper demonstrates equal-or-better scalability,
learning efficiency, and motion quality, and includes examples of learned high-level behaviors.
An elaboration on HL/LL control: the DeepLoco work also learns mocap-based LL-control with learned HL behaviors.
       although with a more dedicated structure.
       Physics-based motion capture imitation with deep reinforcement learning
       https://dl.acm.org/citation.cfm?id=3274506

Overall, the AC recommends this paper to be accepted as a paper of interest to ICLR. 
This does partially discount R3 and R1, who may not have worked as directly on these specific problems before.

The AC requests is rating the confidence as ""not sure"" to flag this for the program committee chairs, in light of the fact that this discounts the R1 and R3 reviews.
The AC is quite certain in terms of the technical contributions of the paper.
"
iclr_2019_BJlgNh0qKQ,"This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi-supervised dependency parsing. The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto-encoder framework. Significant gains are found through semi-supervised learning.

The largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over-stating the perceived utility.

Overall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted."
iclr_2019_BJluy2RcFm,"AR1 is concerned about whether higher-order interactions are modeled explicitly and if pi-SGD convergence conditions can be easily satisfied. AR2 is concerned that basic JP has been conceptually discussed in the literature and \pi-SGD is not novel because it was realized by Hamilton et al. (2017) and Moore & Neville (2017). However, the authors provide some theoretical analysis for this setting in contrast to prior works. AR1 is also concerned that the effect of higher-order information has not been 'disentangled' experimentally from order invariance. AR4 is concerned about  poor performance of higher order Janossy pooling compared to k =1 case and asks about the number of hyper-parameters. The authors showed a harder task of computing the variance of a sequence of numbers in response.

On balance, despite justified concerns of AR2 about novelty and AR1 about experimental verification, the work appears to tackle an interesting topic.  Reviewers find the problem interesting and see some hope in the proposed solutions. On balance, AC recommends this paper to be accepted at ICLR. The authors are asked to update manuscript to reflect honestly weaknesses as expressed by reviewers, e.g. issue with effects of 'higher-order information' and 'disentangled' from order invariance."
iclr_2019_BJlxm30cKm,"This paper is an analysis of the phenomenon of example forgetting in deep neural net training. The empirical study is the first of its kind and features convincing experiments with architectures that achieve near state-of-the-art results. It shows that a portion of the training set can be seen as support examples. The reviewers noted weaknesses such as in the measurement of the forgetting itself and the training regiment. However, they agreed that their concerns we addressed by the rebuttal. They also noted that the paper is not forthcoming with insights, but found enough value in the systematic empirical study it provides."
iclr_2019_BJx0sjC5FX,"AR1 seeks the paper to be more standalone and easier to read. As this comment comes from the reviewer who is very experienced in tensor models, it is highly recommended that the authors make further efforts to make the paper easier to follow. AR2 is concerned about  the manually crafted role schemes and alignment discrepancy of results between these schemes and RNNs. To this end, the authors hypothesized further reasons as to why this discrepancy occurs. AC encourages authors to make further efforts to clarify this point without overstating the ability of tensors to model RNNs (it would be interesting to see where these schemes and RNN differ). Lastly, AR3 seeks more clarifications on contributions.

While the paper is not ground breaking, it offers some starting point on relating tensors and RNNs. Thus, AC recommends an accept. Kindly note that tensor outer products have been used heavily in computer vision, i.e.:
- Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection by Koniusz et al. (e.g. section 3 considers bi-modal outer tensor product for combining multiple sources: one source can be considered a filter, another as role (similar to Smolensky at al. 1990), e.g. a spatial grid number refining local role of a visual word. This further is extended to multi-modal cases (multiple filter or role modes etc.) )
- Multilinear image analysis for facial recognition (e.g. so called tensor-faces) by Vasilescu et al.
- Multilinear independent components analysis by Vasilescu et al.
- Tensor decompositions for learning latent variable models by Anandkumar et al.

Kindly  make connections to these works in your final draft (and to more prior works).
 "
iclr_2019_BJxgz2R9t7,"This paper introduces a new graph neural network architecture designed to learn to solve Circuit SAT problems, a fundamental problem in computer science. The key innovation is the ability to to use the DAG structure as an input, as opposed to typical undirected (factor graph style) representations of SAT problems. The reviewers appreciated the novelty of the approach as well as the empirical results provided that demonstrate the effectiveness of the approach.  Writing is clear. While the comparison with NeuroSAT is interesting and useful, there is no comparison with existing SAT solvers which are not based on learning methods. So it is not clear how big the gap with state-of-the-art is. Overall, I recommend acceptance, as the results are promising and this could inspire other researchers working on neural-symbolic approaches to search and optimization problems."
iclr_2019_BJxh2j0qYm,"The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss. The method are simple and effective. The paper is clear and easy to follow. However, the real speedup on CPU/GPU is not demonstrated beyond the theoretical FLOPs reduction. Reviewers are also concerned that the idea of dynamic channel pruning is not novel. The evaluation is on fairly old networks."
iclr_2019_BJxhijAcY7,The Reviewers noticed that the paper undergone many editions and raise concern about the content. They encourage improving experimental section further and strengthening the message of the paper. 
iclr_2019_BJxssoA5KX,"This paper proposes a novel dataset of bouncing balls and a way to learn the dynamics of the balls when colliding. The reviewers found the paper well-written, tackling an interesting and hard problem in a novel way. The main concern (that I share with one of the reviewers) is about the fact that the paper proposes both a new dataset/environment *and* a solution for the problem. This made it difficult the for the authors to provide baselines to compare to.  The ensuing back and forth had the authors relax some of the assumptions from the environment and made it possible to evaluate with interaction nets.

The main weakness of the paper is the relatively contrived setup that the authors have come up with. I will summarize some of the discussion that happened as a result of this point: it is relatively difficult to see how this setup that the authors have and have studied (esp. knowing the groundtruth impact locations and the timing of the impact) can generalize outside of the proposed approach. There is some concern that the comparison with interaction nets was not entirely fair.

I would recommend the authors redo the comparisons with interaction nets in a careful way, with the right ablations, and understand if the methods have access to the same input data (e.g. are interaction nets provided with the bounce location?). 

Despite the relatively high average score, I think of this paper as quite borderline, specifically because of the issues related to the setup being too niche. Nonetheless, the work does have a lot of scientific value to it, in addition to a new simulation environment/dataset that other researchers can then use. Assuming the baselines are done in a way that is trustworthy, the ablation experiments and discussion will be something interesting to the ICLR community."
iclr_2019_BJxvEh0cFQ,Reviewers largely agree that the proposed method for finetuning the deep neural networks is interesting and empirical results clearly show the benefits over finetuning only the last layer. I recommend acceptance. 
iclr_2019_BJzbG20cFQ,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- The problem is well-motivated and related work is thoroughly discussed
- The evaluation is compelling and extensive.

2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- Very dense. Clarity could be improved in some sections.

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

No major points of contention.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be accepted.
"
iclr_2019_BkG5SjR5YQ,"The submission evaluates maximum mean discrepancy estimators for post selection inference.
It combines two contributions: (i) it proposes an incomplete u-statistic estimator for MMD, (ii) it evaluates this and existing estimators in a post selection inference setting.

The method extends the post selection inference approach of (Lee et al. 2016) to the current u-statistic approach for MMD.  The top-k selection problem is phrased as a linear constraint reducing it to the problem of Lee et al.  The approach is illustrated on toy examples and a GAN application.

The main criticism of the problem is the novelty of the paper.  R1 feels that it is largely just the combination of two known approaches (although it appears that the incomplete estimator is key), while R3 was significantly more impressed.  Both are senior experts in the topic.

On the balance, the reviewers were more positive than negative.  R2 felt that the authors comments helped to address their concerns, while R3 gave detailed arguments in favor of the submission and championed the paper.  The paper provides an additional interesting framework for evaluation of estimators, and considers their application in a broader context of post-selection inference."
iclr_2019_BkG8sjR5Km,"The paper studies population-based training for MARL with co-play, in MuJoCo (continuous control) soccer. It shows that (long term) cooperative behaviors can emerge from simple rewards, shaped but not towards cooperation.

The paper is overall well written and includes a thorough study/ablation. The weaknesses are the lack of strong comparisons (or at least easy to grasp baselines) on a new task, and the lack of some of the experimental details (about reward shaping, about hyperparameters).

The reviewers reached an agreement. This paper is welcomed to be published at ICLR."
iclr_2019_BkMiWhR5K7,"This paper is on the problem of adversarial example generation in the setting where the predictor is only accessible via function evaluations with no gradients available. The associated problem can be cast as a blackbox optimization problem wherein finite difference and related gradient estimation techniques can be used. This setting appears to be pervasive. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. "
iclr_2019_BkN5UoAqF7,"The paper proposes a simple method for improving the sample efficiency of GAIL, essentially a way of turning inverse reinforcement learning into classification. As reviewers noted, the method is based on a simple idea with potentially broad applicability.

Concerns were raised about the multiple components of the system and what each contributed, and missing pointers to the literature. A baseline wherein GAIL is initialized with behaviour cloning, although only suggested but not tried in previous works. The authors did, however, attempt this setting and found it to hurt, not help, performance. I find this surprising and would urge the authors to validate that this isn't merely an uninteresting artifact of the setup, however I commend the authors for trying it and don't believe that a surprising result in this regard is a barrier to publication.

As several reviewers did not provide feedback on revisions addressing their concerns, this Area Chair was left to determine to a large degree whether or not reviewer concerns were in fact addressed.  I thank AnonReviewer4 for revisiting their review towards the end of the period, and concur with them that many of the concerns raised by reviewers have indeed been adequately dealt with.  "
iclr_2019_Bke4KsA5FX,"This paper presents an interesting method for code generation using a graph-based generative approach.  Empirical evaluation shows that the method outperforms relevant baselines (PHOG).

There is consensus among reviewers that the methods are novel and is worth acceptance to ICLR."
iclr_2019_BkeStsCcKQ,"Irrespective of their taste for comparisons of neural networks to biological organisms, all reviewers agree that the empirical observations in this paper are quite interesting and well presented. While some reviewers note that the paper is not making theoretical contributions, the empirical results in themselves are intriguing enough to be of interest to ICLR audiences."
iclr_2019_BkeU5j0ctQ,"This paper combines two different types of existing optimization methods, CEM/CMA-ES and DDPG/TD3, for policy optimization. The approach resembles ERL but demonstrates good better performance on a variety of continuous control benchmarks.  Although I feel the novelty of the paper is limited, the provided promising results may justify the acceptance of the paper."
iclr_2019_BkedznAqKQ,"The reviewers unanimously agreed that the paper was a significant advance in the field of machine learning on graph-structured inputs. They commented particularly on the quality of the research idea, and its depth of development. The results shared by the researchers are compelling, and they also report optimal hyperparameters, a welcome practice when describing experiments and results.

A small drawback the reviewers highlighted is the breadth of the content in the paper, which gave the impression of a slight lack of focus. Overall, the paper is a clear advance, and I recommend it for acceptance. "
iclr_2019_BkfbpsAcF7,"This paper studies the roots of the existence of adversarial perspective from a new perspective. This perspective is quite interesting and thought-provoking. However, some of the contributions rely on fairly restrictive assumptions and/or are not properly evaluated. 

Still, overall, this paper should be a valuable addition to the program. "
iclr_2019_Bkg2viA5FQ,"The paper generalizes the concept of ""hindsight"", i.e. the recycling of data from trajectories in a goal-based system based on the goal state actually achieved, to policy gradient methods.

This was an interesting paper in that it scored quite highly despite all three reviewers mentioning incrementality or a relative lack of novelty. Although the authors naturally took some exception to this, AC personally believes that properly executed, contributions that seem quite straightforward in hindsight (pun partly intended) can be valuable in moving the field forward: a clean and didactic presentation of theory backed by well-designed and extensive empirical investigation (both of which are adjectives used by reviewers to describe the empirical work in this paper) can be as valuable, or moreso, than a poorly executed but higher-novelty works. To quote AnonReviewer3, ""HPG is almost certainly going to end up being a widely used addition to the RL toolbox"".

Feedback from reviewers prompted extensive discussion and a direct comparison with Hindsight Experience Replay which reviewers agreed added significant value to the manuscript, earning it a post-rebuttal unanimous rating of 7. It is therefore my pleasure to recommend acceptance."
iclr_2019_Bkg3g2R9FX,"The paper was found to be well-written and conveys interesting idea. However the AC notices a large body of clarifications that were provided to the reviewers (regarding the theory, experiments, and setting in general) that need to be well addressed in the paper. "
iclr_2019_Bkg6RiCqY7,"Evaluating this paper is somewhat awkward because it has already been through multiple reviewing cycles, and in the meantime, the trick has already become widely adopted and inspired interesting follow-up work. Much of the paper is devoted to reviewing this follow-up work. I think it's clearly time for this to be made part of the published literature, so I recommend acceptance. (And all reviewers are in agreement that the paper ought to be accepted.)

The paper proposes, in the context of Adam, to apply literal weight decay in place of L2 regularization. An impressively thorough set of experiments are given to demonstrate the improved generalization performance, as well as a decoupling of the hyperparameters. 

Previous versions of the paper suffered from a lack of theoretical justification for the proposed method. Ordinarily, in such cases, one would worry that the improved results could be due to some sort of experimental confound. But AdamW has been validated by so many other groups on a range of domains that the improvement is well established. And other researchers have offered possible explanations for the improvement.
"
iclr_2019_Bkg8jjC9KQ,"This paper investigates the usage of the extragradient step for solving saddle-point problems with non-monotone stochastic variational inequalities, motivated by GANs. The authors propose an assumption weaker/diffrerent than the pseudo-monotonicity of the variational inequality for their convergence analysis (that they call ""coherence""). Interestingly, they are able to show the (asympotic) last iterate convergence for the extragradient algorithm in this case (in contrast to standard results which normally requires averaging of the iterates for the stochastic *and* mototone variational inequality such as the cited work by Gidel et al.). The authors also describe an interesting difference between the gradient method without the extragradient step (mirror descent) vs. with (that they called optimistic mirror descent).

R2 thought the coherence condition was too related to the notion of pseudo-monoticity for which one could easily extend previous known convergence results for stochastic variational inequality. The AC thinks that this point was well answered by the authors rebuttal and in their revision: the conditions are sufficiently different, and while there is still much to do to analyze non variational inequalities or having realistic assumptions, this paper makes some non-trivial and interesting steps in this direction. The AC thus sides with expert reviewer R1 and recommends acceptance."
iclr_2019_BkgBvsC9FQ,"This paper tackles the task of end-to-end systems for dialogue generation and proposes a novel, improved GAN for dialogue modeling, which adopts conditional Wasserstein Auto-Encoder to learn high-level representations of responses. In experiments, the proposed approach is compared to several state-of-the-art baselines on two dialog datasets, and improvements are shown both in terms of objective measures and human evaluation, making a strong support for the proposed approach.
Two reviewers suggest similarities with a recent ICML paper on ARAE and request including reference to it and also request examples demonstrating differences, which are included in the latest version of the paper."
iclr_2019_BkgPajAcY7,"This paper provides a new family of untrained/randomly initialized sentence encoder baselines for a standard suite of NLP evaluation tasks, and shows that it does surprisingly well—very close to widely-used methods for some of the tasks. All three reviewers acknowledge that this is a substantial contribution, and none see any major errors or fatal flaws.

One reviewer had initially argued the experiments and discussion are not as thorough as would be typical for a strong paper. In particular, the results are focused on a single set of word embeddings and a narrow class of architectures. I'm sympathetic to this concern, but since there don't seem to be any outstanding concerns about the correctness of the paper, and since the other reviewers see the contribution as quite important, I recommend acceptance. [Update: This reviewer has since revised their review to make it more positive.]

(As a nit, I'd ask the authors to ensure that the final version of the paper fits within the margins.)"
iclr_2019_BkgWHnR5tm,"Lean in favor

Strengths:  The paper tackles the difficult problem of automatic robot design. The approach uses graph neural
networks to parameterize the control policies, which allows for weight sharing / transfer to new policies even
as the topology changes.  Understanding how to efficiently explore through non-differentiable changes to the body
is an important problem (AC). The authors will release the code and environments, which will be useful in an area where there are 
currently no good baselines (AC). 

Weaknesses: There are concerns (particularly R2, R1) over the lack of a strong baseline, and with the results 
being demonstrated on a limited number of environments (R1)  (fish, 2D walker). In response, the authors clarified the nomenclature and
description of a number of the baselines, and added others. AC: there is no submitted video (searches for ""video"" on the PDF text
produces no hits); this is seen by the AC as being a real limitation from the perspective of evaluation. 
AC agrees with some of the reviewer remarks that some of the original stated claims are too strong.
  AC: the simplified fluid model of Mujoco (http://mujoco.org/book/computation.html#gePassive) is
unable to model the fluid state, in particular the induced fluid vortices that are responsible for a
good portion of fish locomotion, i.e., ""Passive and active flow control by swimming fishes and
mammals"" and other papers. Acknowledging this kind of limitation will make the paper stronger, not weaker;
the ML community can learn from much existing work at the interface of biology and fluid mechancis.

There remain points of contention, i.e., the sufficiency of the baselines. However, the reviewers R2 and R3 have
not responded to the detailed replies from the authors, including additional baselines (totaling 5 at present) 
and pointing out that baselines such as CMA-ES (R2) in a continuous space and therefore do not translate in any obvious way
to the given problem at hand.  

On balance, with the additional baselines and related clarifications, the AC feels that this paper makes a 
useful and valid contribution to the field, and will help establish a benchmark in an important area.
The authors are strongly encouraged to further state caveats and limitations, and to emphasize why some
candidate baseline methods are not readily applicable.
"
iclr_2019_BkgtDsCcKQ,Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.
iclr_2019_BkgzniCqY7,This paper contributes a novel approach to evaluating the robustness of DNN based on structured sparsity to exploit the underlying structure of the image and introduces a method to solve it. The proposed approach is well evaluated and the authors answered the main concerns of the reviewers. 
iclr_2019_Bkl-43C9FQ,"The paper presents a simple and effective convolution kernel for CNNs on spherical data (convolution by a linear combination of differential operators). The proposed method is efficient in the number of parameters and achieves strong classification and segmentation performance in several benchmarks. The paper is generally well written but the authors should clarify the details and address reviewer comments (for example, clarity/notations of equations) in the revision. 
"
iclr_2019_BklCusRct7,"This is a well-written paper that shows how to use optimal transport to perform smooth interpolation, between two random vectors sampled from the prior distribution of the latent space of a deep generative model. By encouraging the marginal of the interpolated vector to match the prior distribution, these interpolated distribution-preserving random vectors in the latent space are shown to result in better image interpolation quality for GANs. The problem is of interest to the community and the resulted solutions are simple to implement. 

As pointed out by Reviewer 1, the paper could be made clearly more convincing by showing that these distribution preservation operations also help perform interpolation in the latent space of VAEs, and the AC strongly encourages the authors to add these results if possible. 

The AC appreciates that the authors have added experiments to satisfactorily address his/her concern:

""Suppose z_1,z_2 are independent, and drawn from N(\mu,\Sigma), then t z_1 + (1-t)z_2 ~ N(\mu, (t^2+(1-t)^2)\Sigma). If one lets y | z_1, z_2 ~ N(t z_1 + (1-t)z_2, (1-t^2-(1-t)^2)\Sigma) as the latent space interpolation, then marginally we have y ~ N(\mu, \Sigma). This is an extremely simple and fast procedure to make sure that the latent space interpolation y is highly related to the linear interpolation t z_1 + (1-t)z_2 but also satisfies  y ~ N(\mu, \Sigma).""

The AC strongly encourages the authors to add these new results into their revision, and highlight ""smooth interpolation"" as an important characteristic in addition to ""distribution preserving."" A potential suggestion is changing ""Distribution Preserving Operations"" in the title to ""Distribution Preserving Smooth Operations.""
"
iclr_2019_BklHpjCqKm,"The paper looks at a novel form of physics-constrained system identification for a multi-link robot,
although it could also be applied more generally.  The contributions is in many simple; this is seen
in a good light (R1, R3) or more modestly (R2). R3 notes surprise that this hasn't been done before.
Results are demonstrated on a simualted 2-dof robot and real Barrett WAM arm, better than a pure
neural network modeling approach, PID control, or an analytic model.  

Some aspects of the writing needed to be addressed, i.e., PDE vs ODE notations. 
The point of biggest concern is related to positioning the work relative to other system-identification
literature, where there has been an abundance of work in the robotics and control literature.
There is no final consensus on this point for R3;  R3 did not receive the email notification of the author's detailed reply,
and notes that the author has clarified some respects, but still has concerns, and did not have time to further
provide feedback on short notice.  

In balance, the AC believes that this kind of constrained learning of models is underexplored, and
notes that the reviewers (who have considerable shared expertise in robotics-related work) believe
that this is a step in the right direction and that it is surprising this type of approach has not
been investigated yet.  The authors have further reconciled their work with earlier sys-ID work, and
can further describe how their work is situated with respect to prior art in sys-ID (as they do in
their discussion comments).  The AC recommends that: (a) the abstract explicitly mention ""system
identification"" as a relevant context for the work in this paper, given that the ML audience should
be (or can be) made aware of this terminology; and (b) push more of the math related to the
development of the necessary derivatives to an appendix, given that the particular use of the
derivations seems to be more in support of obtaining the performance necessary for online use,
rather than something that cannot be accomplished with autodiff.
"
iclr_2019_BklMjsRqY7,"The authors present a theoretical and practical study on low-precision training of neural networks. They introduce the notion of variance retention ratio (VRR) that determines the accumulation bit-width for
precise tailoring of computation hardware.  Empirically, the authors show that their theoretical result extends to practical implementation in three standard benchmarks.

A criticism of the paper has been certain hyperparameters that a reviewer found to be chosen rather arbitrarily, but I think the reviewers do a reasonable job in rebutting it. 

Overall, there is consensus that the paper presents an interesting framework and does both practical and empirical analysis, and it should be accepted."
iclr_2019_Bklfsi0cKm,"This paper builds on a promising line of literature developing connections between Gaussian processes and deep neural networks.  Viewing one model under the lens of (the infinite limit of) another can lead to neat new insights and algorithms.  In this case the authors develop a connection between convolutional networks and Gaussian processes with a particular kind of kernel.  The reviews were quite mixed with one champion and two just below borderline.

The reviewers all believed the paper had contributions which would be interesting to the community (such as R1: ""the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own"" and R2: ""I really like the idea of authors that kernels based on convolutional networks might be more practical compared to the ones based on fully connected networks"").  All the reviewers found the contribution of the covariance function to be novel and exciting.

Some cited weaknesses of the paper were that the authors didn't analyze the uncertainty from the model (arguably the reasoning for adopting a Bayesian treatment), novelty in appealing to the central limit theorem to arrive at the connection, and scalability of the model.

In the review process it also became apparent that there was another paper with a substantially similar contribution.  The decision for this paper was calibrated accordingly with that work.

Weighing the strengths and weaknesses of the paper and taking into account a reviewer willing to champion the work it seems there is enough novel contribution and interest in the work to justify acceptance.

The authors provided responses to the reviewer concerns including calibration plots and timing experiments in the discussion period and it would be appreciated if these can be incorporated into the camera ready version."
iclr_2019_BklhAj09K7,"This paper proposes a new solution for tackling domain adaptation across disjoint label spaces. Two of the reviewers agree that the main technical approach is interesting and novel. The final reviewer asked for clarification of the problem setting which the authors have provided in their rebuttal. We encourage the authors to include this in the final version. However, there is also a consensus that more experimental evaluation would improve the manuscript and complete experimental details are needed for reliable reproduction."
iclr_2019_BkloRs0qK7,"This paper has two main contributions. The first is that it proposes a specific framework for measuring catastrophic forgetting in deep neural networks that incorporates three application-oriented constraints: (1) a low memory footprint, which implies that data from prior tasks cannot be retained; (2) causality, meaning that data from future tasks cannot be used in any way, including hyperparameter optimization and model selection; and (3) update complexity for new tasks that is moderate and also independent of the number of previously learned tasks, which precludes replay strategies. The second contribution is an extensive study of catastrophic forgetting, using different sequential learning tasks derived from 9 different datasets and examining 7 different models. The key conclusions from the study are that (1) permutation-based tasks are comparatively easy and should not be relied on to measure catastrophic forgetting; (2) with the application-oriented contraints in effect, all of the examined models suffer from catastrophic forgetting (a result that is contrary to a number of other recent papers); (3) elastic weight consolidation provides some protection against catastrophic forgetting for simple sequential learning tasks, but fails for more complex tasks; and (4) IMM is effective, but only if causality is violated in the selection of the IMM balancing parameter. The reviewer scores place this paper close to the decision boundary. The most negative reviewer (R2) had concerns about the novelty of the framework and its application-oriented constraints. The authors contend that recent papers on catastrophic forgetting fail to apply these quite natural constraints, leading to the deceptive conclusion that catastrophic forgetting may not be as big of a problem as it once was. The AC read a number of the papers mentioned by the authors and agrees with them: these constraints have been, at least at times, ignored in the literature, and they shouldn't be ignored. The other two reviewers appreciated the scope and rigor of the empirical study. On the balance, the AC thinks this is an important contribution and that it should appear at ICLR."
iclr_2019_BkltNhC9FX,"The reviewers of this paper agreed that it has done a stellar job of presenting a novel and principled approach to attention as a latent variable, providing a new and sound set of inference techniques to this end. This builds on top of a discussion of the limitations of existing deterministic approaches to attention, and frames the contribution well in relation to other recurrent and stochastic approaches to attention. While there are a few issues with clarity surrounding some aspects of the proposed method, which the authors are encouraged to fine-tune in their final version, paying careful attention to the review comments, this paper is more or less ready for publication with a few tweaks. It makes a clear, significant, and well-evaluate contribution to the field of attention models in sequence to sequence architectures, and will be of great interest to many attendees at ICLR."
iclr_2019_Bkx0RjA9tX,"All reviewers recommend accept. 
Discussion can be consulted below."
iclr_2019_BkxWJnC9tX,"
pros:
- good, clear writing
- interesting analysis
- very important research area
- nice results on multi-task omniglot

cons:
- somewhat limited experimental evaluation

The reviewers I think all agree that the work is interesting and the paper well-written. I think there is still a need for more thorough experiments (which it sounds like the authors are undertaking).  I recommend acceptance.

"
iclr_2019_Bkxbrn0cYX,"Two of the reviewers raised their scores during the discussion phase noting that the revised version was clearer and addressed some of their concerns.  As a result, all the reviewers ultimately recommended acceptance.  They particularly enjoyed the insights that the authors shared from their experiments and appreciated that the experiments were quite thorough.  All the reviewers mentioned that the work seemed somewhat incremental, but given the results, insights and empirical evaluation decided that it would still be a valuable contribution to the conference.  One reviewer added feedback about how to improve the writing and clarity of the paper for the camera ready version."
iclr_2019_BkzeUiRcY7,"The paper addresses a variant of multi-agent reinforcement learning that aligns well with real-world applications - it considers the case where agents may have individual, diverging preferences. The proposed approach trains a ""manager"" agent which coordinates the self-interested worker agents by assigning them appropriate tasks and rewarding successful task completion (through contract generation). The approach is empirically validated on two grid-world domains: resource collection and crafting. The reviewers point out that this formulation is closely related to the principle-agent problem known in the economics literature, and see a key contribution of the paper in bringing this type of problem into the deep RL space.

The reviewers noted several potential weaknesses: They asked to clarify the relation to prior work, especially on the principle-agents work done in other areas, as well as connections to real world applications. In this context, they also noted that the significance of the contribution was unclear. Several modeling choices were questioned, including the choice of using rule-based agents for the empirical results presented in the main paper, and the need for using deep learning for contract generation. They asked the authors to provide additional details regarding scalability and sample complexity of the approach.

The authors carefully addressed the reviewer concerns, and the reviewers have indicated that they are satisfied with the response and updates to the paper. The consensus is to accept the paper.

The AC is particularly pleased to see that the authors plan to open source their code so that experiments can be replicated, and encourages them to do so in a timely manner. The AC also notes that the figures in the paper are very small, and often not readable in print - please increase figure and font sizes in the camera ready version to ensure the paper is legible when printed."
iclr_2019_ByGuynAct7,"This paper proposes factorized prior distributions for CNN weights by using explicit and implicit parameterization for the prior. The paper suggest a few tractable methods to learn the prior and the model jointly. The paper, overall, is interesting.

The reviewers have had some disagreement regarding the effectiveness of the method. The factorized prior may not be the most informative prior and using extra machinery to estimate it might deteriorates the performance. On the other hand, estimating a more informative prior might be difficult. It is extremely important to discuss this trade-off in the paper. I strongly recommend for the authors to discuss the pros and cons of using priors that are weakly informative vs strongly informative.

The idea of using a hierarchical model has been around, e.g., see the paper on ""Hierarchical variational models"" and more recently ""semi-implicit Variational Inference"". Please include a related work on such existing work. Please discuss why your proposed method is better than these existing methods.

Conditioned on the two discussions added to the paper, we can accept it.
"
iclr_2019_ByME42AqK7,"The paper proposes an evolutionary architecture search method which uses weight inheritance through network morphism to avoid training candidate models from scratch.  The method can optimise multiple objectives (e.g. accuracy and inference time), which is relevant for practical applications, and the results are promising and competitive with the state of the art. All reviewers are generally positive about the paper. Reviewers’ feedback on improving presentation and adding experiments with a larger number of objectives has been addressed in the new revision. 

I strongly encourage the authors to add experiments on the full ImageNet dataset (not just 64x64) and/or language modelling -- the two benchmarks widely used in neural architecture search field."
iclr_2019_ByMHvs0cFQ,"The authors derive and experiment with quaternion-based recurrent neural networks, and demonstrate their effectiveness on speech recognition tasks (TIMIT and WSJ), where the authors demonstrate that the proposed models can achieve the same accuracy with fewer parameters than conventional models. The reviewers were unanimous in recommending that the paper be accepted."
iclr_2019_ByMVTsR5KQ,"This paper proposes a GAN model to synthesize raw-waveform audio by adapting the popular DC-GAN architecture to handle audio signals. Experimental results are reported on several datasets, including speech and instruments. 

Unfortunately this paper received two low-quality reviews, with little signal. The only substantial review was mildly positive, highlighting the clarity, accessibility and reproducibility of the work, and expressing concerns about the relative lack of novelty. The AC shares this assessment. The paper claims to be the first successful GAN application operating directly on wave-forms. Whereas this is certainly an important contribution, it is less clear to the AC whether this contribution belongs to a venue such as ICLR, as opposed to ICASSP or Ismir.  This is a borderline paper, and the decision is ultimately relative to other submissions with similar scores. In this context, given the mainstream popularity of GANs for image modeling, the AC feels this paper can help spark significant further research in adversarial training for audio modeling, and therefore recommends acceptance. I also encourage the authors to address the issues raised by R1.  "
iclr_2019_Bye5SiAqKX,"The method presented here adapts an SGD preconditioner by minimizing particular cost functions which are minimized by the inverse Hessian or inverse Fisher matrix. These cost functions are minimized using natural (or relative) gradient on the Lie group, as previously introduced by Amari. This can be extended to learn a Kronecker-factored preconditioner similar to K-FAC, except that the preconditioner is constrained to be upper triangular, which allows the relative gradient to be computed using backsubstitution rather than inversion. Experiments show modest speedups compared to SGD on ImageNet and language modeling.

There's a wide divergence in reviewer scores. We can disregard the extremely short review by R2. R1 and R3 each did very careful reviews (R3 even tried out the algorithm), but gave scores of 5 and 8. They agree on most of the particulars, but just emphasized different factors. Because of this, I took a careful look, and indeed I think the paper has significant strengths and weaknesses. 

The main strength is the novelty of the approach. Combining relative gradient with upper triangular preconditioners is clever, and allows for a K-FAC-like algorithm which avoids matrix inversion. I haven't seen anything similar, and this method seems potentially useful. R3 reports that (s)he tried out the algorithm and found it to work well. Contrary to R1, I think the paper does use Lie groups in a meaningful way.

Unfortunately, the writing is below the standards of an ICLR paper. The title is misleading, since the method isn't learning a preconditioner ""on"" the Lie group. The abstract and introduction don't give a clear idea of what the paper is about. While some motivation for the algorithms is given, it's expressed very tersely, and in a way that will only make sense to someone who knows the mathematical toolbox well enough to appreciate why the algorithm makes sense. As the reviewers point out, important details (such as hyperparameter tuning schemes) are left out of the experiments section.

The experiments are also somewhat problematic, as pointed out by R1. The paper compares only to SGD and Adam, even though many other second-order optimizers have been proposed (and often with code available). It's unclear how well the baselines were tuned, and at the end of the day, the performance gain is rather limited. The experiments measure only iterations, not wall clock time. 

On the plus side, the experiments include ImageNet, which is ambitious by the standards of an algorithmic paper, and as mentioned above, R3 got good results from the method.

On the whole, I would favor acceptance because of the novelty and potential usefulness of the approach. This would be a pretty solid submission of the writing were improved. (While the authors feel constrained by the 8 page limit, I'd recommend going beyond this for clarity.) However, I emphasize that it is very important to clean up the writing.
"
iclr_2019_ByeMB3Act7,"This paper introduces an approach for improving the scalability of neural network models with large output spaces, where naive soft-max inference scales linearly with the vocabulary size. The proposed approach is based on a clustering step combined with per-cluster, smaller soft-maxes. It retains differentiability with the Gumbel softmax trick. The experimental results are impressive. There are some minor flaws, however there's consensus among the reviewers the paper should be published.
"
iclr_2019_ByeSdsC9Km,"All reviewers recommend acceptance. The problem is an interesting one. THe method is interesting.
Authors were responsive in the reviewing process.

Good work. I recommend acceptance :)"
iclr_2019_ByetGn0cYX,"This paper presents a new approach for posing control as inference that leverages Sequential Monte Carlo and Bayesian smoothing. There is significant interest from the reviewers into this method, and also an active discussion about this paper, particularly with respect to the optimism bias issue. The paper is borderline and the authors are encouraged to address the desired clarifications and changes from the reviewers. 
"
iclr_2019_Byey7n05FQ,"The paper makes novel explorations into how MPC and approximate-DP / value-function approaches, with value-fn ensembles to model value-fn uncertainty, can be effectively combined.  The novelty lies in exploring their combination. The experiments are solid. The paper is clearly written.

Open issues include overall novelty, and delineating the setting in which this method is appropriate.

The reviewers and AC are in agreement on what is in the paper. The open question is whether
the combination of the ideas is interesting. 

After further reviewing the paper and results. the AC believes that the overall combination of ideas and related evaluations that make a useful and promising contribution. As evidenced in some of the reviewer discussion, there is often a
considerable schism in the community regarding what is considered fair to introduce in terms of
prior knowledge, and blurred definitions regarding planning and control. The AC discounted some of the
concerns of R2 that related more to discrete action settings and theoretical considerations; these 
often fail to translate to difficult problems in continuous action settings.  The
AC believes that R3 nicely articulates the issues of the paper that can be (and should be) addressed in the writing, i.e., to
describe and motivate the settings that the proposed framework targets, as articulated in the reviews and ensuing discussion. 
"
iclr_2019_Byf5-30qFX,"This work proposes a method for extending hindsight experience replay to the setting where the goal is not fixed, but dynamic or moving. It proceeds by amending failed episodes by searching replay memory for a compatible trajectories from which to construct a trajectory that can be productively learned from.

Reviewers were generally positive on the novelty and importance of the contribution. While noting its limitations, it was still felt that the key ideas could be useful and influential. The tasks considered are modifications of OpenAI robotics environments, adapted to the dynamic goal setting, as well as a 2D planar ""snake"" game. There were concerns about the strength of the baselines employed but reviewers seemed happy with the state of these post-revision. There were also concerns regarding clarity of presentation, particularly from AnonReviewer2, but significant progress was made on this front following discussions and revision.

Despite remaining concerns over clarity I am convinced that this is an interesting problem setting worth studying and that the proposed method makes significant progress. The method has limitations with respect to the sorts of environments where we can reasonably expect it to work (where other aspects of the environment are relatively stable both within and across episodes), but there is lots of work in the literature, particularly where robotics is concerned, that focuses on exactly these kinds of environments. This submission is therefore highly relevant to current practice and by reviewers' accounts, generally well-executed in its post-revision form. I therefore recommend acceptance."
iclr_2019_ByftGnR9KX,"Interesting and novel approach of modeling context (mainly external documents with information about the conversation content) for the conversational question answering task, demonstrating significant improvements on the newly released conversational QA datasets.
The first version of the paper was weaker on motivation and lacked a clearer presentation of the approach as mentioned by the reviewers, but the paper was updated as explained in the responses to the reviewers.
The ablation studies are useful in demonstration of the proposed FLOW approach.
A question still remains after the reviews (this was not raised by the reviewers): How does the approach perform in comparison to the state of the art for the single question and answer tasks? If each question was asked in isolation, would it still be the best?


"
iclr_2019_ByfyHh05tQ,"After a healthy discussion between reviewers and authors, the reviewers' consensus is to recommend acceptance to ICLR. The authors thoroughly addressed reviewer concerns, and all reviewers noted the quality of the paper, methodological innovations and SotA results."
iclr_2019_Byg0DsCqYQ,"The proposed method suggests a way to do robust conditional image generation with GANs. The premise is to make the image to image translation model resilient to noise by leveraging structure in the output space, with an unsupervised ""pathway"".

In general, the qualitative results seem reasonable on a a number of datasets, including those suggested by reviewers. The method appears simple, novel and easy to try.  The main concerns seem to be that the idea is maybe too simple, but I'm not particularly bothered by that. The authors showed it working well on a variety of tasks (synthetic and natural), provide SSIM numbers that look compelling (despite SSIM's short-comings) and otherwise give compelling arguments for the technical soundness of the approach.

Thus, I recommend acceptance."
iclr_2019_Byg5QhR5FQ,"This paper presents a method for building representations of logical formulae not by propagating information upwards from leaves to root and making decisions (e.g. as to whether one formula entails another) based on the root representation, but rather by propagating information down from root to leaves.

It is a somewhat curious approach, and it is interesting to see that it works so well, especially on the ""massive"" train/test split of Evans et al. (2018). This paper certainly piques my interest, and I was disappointed to see a complete absence of discussion from reviewers during the rebuttal period despite author responses. The reviewer scores are all middle-of-the-road scores lightly leaning towards accepting, so the paper is rather borderline. It would have been most helpful to hear what the reviewers thought of the rebuttal and revisions made to the paper.

Having read through the paper myself, and through the reviews and rebuttal, I am hesitantly casting an extra vote in favour of acceptance: the sort of work discussed in this paper is important and under-represented in the conference, and the results are convincing. I however, share the concerns outlined by the reviewers in their first (and only) set of comments, and invite the authors to take particular heed of the points made by AnonReviewer3, although all make excellent points. There needs to be some further analysis and explanation of these results. If not in this paper, then at least in follow up work. For now, I will recommend with medium confidence that the paper be accepted."
iclr_2019_BygANhA9tQ,"This paper studies the notion of certified cost-sensitive robustness against adversarial examples, by building from the recent [Wong & Koller'18]. Its main contribution is to adapt the robust classification objective to a 'cost-sensitive' objective, that weights labelling errors according to their potential damage. 
This paper received mixed reviews, with a clear champion and two skeptical reviewers. On the one hand, they all highlighted the clarity of the presentation and the relevance of the topic as strengths; on the other hand, they noted the relatively little novelty of the paper relative [W & K'18]. Reviewers also acknowledged the diligence of authors during the response phase. The AC mostly agrees with these assessments, and taking them all into consideration, he/she concludes that the potential practical benefits of cost-sensitive certified robustness outweight the limited scientific novelty. Therefore, he recommends acceptance as a poster. "
iclr_2019_BygfghAcYX,"I agree with the reviewers that this is a strong contribution and provides new insights, even if it doesn't quite close the problem. 

p.s.: It seems that centering the weight matrices at initialization is a key idea. The authors note that Dziugaite and Roy used  bounds that were based on the distance to initialization, but that their reported numerical generalization bounds also increase with the increasing network size. Looking back at that work, they look at networks where the size increases by a very large factor (going from e.g. 400,000 parameters roughly to over 1.2 million, so a factor of 2.5), at the same time the bound increases by a much smaller factor. The type of increase also seems much less severe than those pictured in Figures 3/5. Since Dzugate and Roy's bounds involved optimization, perhaps the increase there is merely apparent."
iclr_2019_BygqBiRcFQ,"The paper gives an extension of scattering transform to non-Euclidean domains by introducing scattering transforms on graphs using diffusion wavelet representations, and presents a stability analysis of such a representation under deformation of the underlying graph metric defined in terms of graph diffusion. 

Concerns of the reviewers are primarily with what type of graphs is the primary consideration (small world social networks or point cloud submanifold samples) and experimental studies. Technical development like deformation in the proposed graph metric is motivated by sub-manifold scenarios in computer vision, and whether the development is well suitable to social networks in experiments still needs further investigations. 

The authors make satisfied answers to the reviewers’ questions. The reviewers unanimously accept the paper for ICLR publication."
iclr_2019_Byl8BnRcYm,"AR1 asks for a clear experimental evaluation showing that capsules and dynamic routing help in the GCN setting. After rebuttal, AR1 seems satisfied that routing in CapsGNN might help generate 'more representative graph embeddings from different aspects'. AC strongly encourages the authors to improve the discussion on these 'different aspects' as currently it feels vague. AR2 is initially concerned about experimental evaluations and whether the attention mechanism works as expected, though, he/she is happy with the revised experiments. AR3 would like to see all biological datasets included in experiments. He/she is also concerned about the lack of ability to preserve fine structures by CapsGNN. The authors leave this aspect of their approach for the future work.

On balance, all reviewers felt this paper is a borderline paper. After going through all questions and responses, AC sees that many requests about aspects of the proposed method have not been clarified by the authors. However, reviewers note that the authors provided more evaluations/visualisations etc. The reviewers expressed hope (numerous times) that this initial attempt to introduce capsules into GCN will result in future developments and  improvements. While AC thinks this is an overoptimistic view, AC will give the authors the benefit of doubt and will advocate a weak accept.

The authors are asked to incorporate all modifications requested by the reviewers. Moreover, 'Graph capsule convolutional neural networks' is not a mere ArXiV work. It is an ICML workshop paper. Kindly check all ArXiV references and update with the actual conference venues."
iclr_2019_BylBr3C9K7,"All of the reviewers agree that this is a well-written paper with the novel perspective of minimizing energy consumption in neural networks, as opposed to maximizing sparsity, which does not always correlate with energy cost. There are a number of promised clarifications and additional results that have emerged from the discussion that should be put into the final draft. Namely, describing the overhead of converting from sparse to dense representations, adding the Imagenet sparsity results, and adding the time taken to run the projection step."
iclr_2019_BylE1205Fm,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

The proposed method performed well on 3 visual content transfer problems.
 
2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- The paper is hard to follow at times
- The problem being addressed is technically interesting but not well-motivated. That is, the question ""why is this of interest to the ICLR community"" was not well-answered.

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

There were no major points of contention.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be accepted.
"
iclr_2019_BylIciRcYQ,"The proposed notion of star convexity is interesting and the empirical work done to provide evidence that it is indeed present in real-world neural network training is appreciated.  The reviewers raise a number of concerns. The authors were able to convince some of the reviewers with new experiments under MSE loss and experiments showing how robust the method was to the reference point. The most serious concerns relate to novelty and the assumptions that individual functions share a global minima with respect to which the path of iterates generated by SGD satisfies the star convexity property. I'm inclined to accept the authors rebuttal, although it would have been nicer had the reviewer re-engaged. Overall, the paper is on the borderline."
iclr_2019_BylQV305YQ,The reviewers that provided extensive and technically well-justified reviews agreed that the paper is of high quality. The authors are encouraged to make sure all concerns of these reviewers are properly addressed in the paper.
iclr_2019_ByldlhAqYQ,"This paper presents a method for transferring source information via the hidden states of recurrent networks.  The transfer happens via an attention mechanism that operates between the target and the source.  Results on two tasks are strong.

I found this paper similar in spirit to Hypernetworks (David Ha, Andrew Dai, Quoc V Le, ICLR 2016) since there too there is a dynamic weight generation for network given another network, although this method did not use an attention mechanism.

However, reviewers thought that there is merit in this paper (albeit pointed the authors to other related work) and the empirical results are solid.
"
iclr_2019_ByleB2CcKm,"While the reviews of this paper were somewhat mixed (7,6,4), I ended up favoring acceptance because of the thorough author responses, and the novelty of what is being examined.

The reviewer with a score of 4, argues that this work is not a good fit for iclr, but, although tailoring new metrics may not be a common area that is explored, I don't believe that it's outside the range of iclr's interest, and therefore also more unique."
iclr_2019_Bylmkh05KX,"This paper is about unsupervised learning for ASR, by matching the acoustic distribution, learned unsupervisedly, with a prior phone-lm distribution. Overall, the results look good on TIMIT. Reviewers agree that this is a well written paper and that it has interesting results.

Strengths
- Novel formulation for unsupervised ASR, and a non-trivial extension to previously proposed unsupervised classification to segmental level.
- Well written, with strong results. Improved results and analysis based on review feedback.

Weaknesses
- Results are on TIMIT -- a small phone recognition task.
- Unclear how it extends to large vocabulary ASR tasks, and tasks that have large scale training data, and RNNs that may learn implicit LMs. The authors propose to deal with this in future work.

Overall, the reviewers agree that this is an excellent contribution with strong results. Therefore, it is recommended that the paper be accepted."
iclr_2019_Bylnx209YX," The paper proposes an method for investigating robustness of graph neural nets for node classification problem; training-time attacks for perturbing graph structure are generated using  meta-learning approach. Reviewers agree that the contribution is novel and empirical results support the validity of the approach.
 "
iclr_2019_ByloIiCqYQ,"
* Strengths

This paper applies deep learning to the domain of cybersecurity, which is non-traditional relative to more common domains such as vision and speech. I see this as a strength. Additionally, the paper curates a dataset that may be of broader interest.

* Weaknesses

While the empirical results are good, there appears to be limited conceptual novelty. However, this is fine for a paper that is providing a new task in an interesting application domain.

* Discussion

Some reviewers were concerned about whether the dataset is a substantial contribution, as it is created based on existing publicly available data. However, these concerns were addressed by the author responses and all reviewers now agree with accepting the paper."
iclr_2019_ByloJ20qtm,"This paper provides an approach to jointly localize and repair VarMisuse bugs, where a wrong variable from the context has been used. The proposed work provides an end-to-end training pipeline for jointly localizing and repairing, as opposed to independent predictions in existing work. The reviewers felt that the manuscript was very well-written and clear, with fairly strong results on a number of datasets.

The reviewers and AC note the following potential weaknesses: (1) reviewer 4 brings up related approaches from automated program repair (APR), that are much more general than the VarMisuse bugs, and the paper lacks citation and comparison to them, (2) the baselines that were compared against are fairly weak, and some recent approaches like DeepBugs and Sk_p are ignored, (3) the approach is trained and evaluated only on synthetic bugs, which look very different from the realistic ones, and (4) the contributions were found to be restricted in novelty, just uses a pointer-based LSTM for locating and fixing bugs. 

The authors provided detailed comments and a revision to address and clarify these concerns. They added an evaluation on realistic bugs, along with differences from DeepBugs and Sk_p, and differences between neural and automated program repair. They also added more detail comparisons, including separating the localization vs repair aspects by comparing against enumeration. During the discussion, the reviewers disagree on the ""weakness"" of the baseline, as reviewers 1 and 4 feel it is a reasonable baseline as it builds upon the Allamanis paper. They found, to different degrees, that the results on realistic bugs are much more convincing than the synthetic bug evaluation. Finally, all reviewers agree that the novelty of this work is limited.

Although the reviewers disagree on the strength of the baselines (a recent paper) and the evaluation benchmarks, they agreed that the results are quite strong. The paper, however, addressed many of the concerns in the response/revision, and thus, the reviewers agree that it meets the bar for acceptance."
iclr_2019_Byx83s09Km,"The paper introduces a method for using information directed sampling, by taking advantage of recent advances in computing parametric uncertainty and variance estimates for returns. These estimates are used to estimate the information gain, based on a formula from (Kirschner & Krause, 2018) for the bandit setting. This paper takes these ideas and puts them together in a reasonably easy-to-use and understandable way for the reinforcement learning setting, which is both nontrivial and useful. The work then demonstrates some successes in Atari. Though it is of course laudable that the paper runs on 57 Atari games, it would make the paper even stronger if a simpler setting (some toy domain) was investigated to more systematically understand this approach and some choices in the approach."
iclr_2019_ByxBFsRqYm,"The paper presents a new deep learning approach for combinatorial optimization
problems based on the Transformer architecture. The paper is well written
and several experiments are provided. A reviewer asked for more intuition to
the proposed approach and authors have responded accordingly. Reviewers are
also concerned with scalability and theoretical basis.
Overall, all reviewers were positives in their scores, and I recommend accepting the paper."
iclr_2019_ByxGSsR9FQ,"
* Strengths

This paper studies adversarial robustness to perturbations that are bounded in the L2 norm. It is motivated by a theoretical sufficient condition (non-expansiveness) but rather than trying to formally verify robustness, it uses this condition as inspiration, modifying standard network architectures in several ways to encourage non-expansiveness while mostly preserving computational efficiency and accuracy. This “theory-inspired practically-focused” hybrid is a rare perspective in this area and could fruitfully inspire further improvements. Finally, the paper came under substantial scrutiny during the review period (there are 65 comments on the page) and the authors have convincingly answered a number of technical criticisms.

* Weaknesses

One reviewer and some commenters were concerned that the L2 norm is not a realistic norm to measure adversarial attacks in. There were also concerns that the empirical level of robustness of the network was too weak to be meaningful. In addition, while some parts of the experiments were thorough and some parts of the paper were well-presented, the quality was not uniform throughout. Finally, while the proposed changes improve adversarial robustness, they also decrease the accuracy of the network on clean examples (this is to be expected but may be an issue in practice).

* Discussion

There was substantial disagreement on whether to accept the paper. On the one hand, there has been limited progress on robustness to adversarial examples (even under simple norms such as the L2 norm) and most methods that do work are based on formal verification and therefore quite computationally expensive. On the other hand, simple norms such as the L2 norm are somewhat contrived and mainly chosen for convenience (although doing well in the L2 norm is a necessary condition for being robust to more general attacks). Moreover, the empirical results are currently too weak to confer meaningful robustness even under the L2 norm.

* Decision

While I agree with the reviewers and commenters who are skeptical of the L2 norm model (and would very much like to see approaches that consider more realistic threat models), I decided to accept the paper for two reasons: first, doing well in L2 is a necessary condition to doing well in more general models, and the ideas and approach here are simple enough that they might provide inspiration in these more general models as well. Additionally, this was one of the strongest adversarial defense papers at ICLR this year in terms of credibility of the claims (certainly the strongest in my pile) and contains several useful ideas as well as novel empirical findings (such as the increased success of attacks up to 1 million iterations)."
iclr_2019_ByxPYjC5KQ,"The paper received unanimous accept over reviewers (7,7,6), hence proposed as definite accept. "
iclr_2019_ByxZX20qFQ,"There is a clear consensus among the reviews to accept this submission thus I am recommending acceptance. The paper makes a clear, if modest, contribution to language modeling that is likely to be valuable to many other researchers."
iclr_2019_ByxkijC5FQ,"The paper presents a topological complexity measure of neural networks based on persistence 0-homology of the weights in each layer. Some lower and upper bounds of the p-norm persistence diagram are derived that leads to normalized persistence metric. The main discovery of such a topological complexity measure is that it leads to a stability-based early stopping criterion without a statistical cross-validation, as well as distinct characterizations on random initialization, batch normalization and drop out. Experiments are conducted with simple networks and MNIST, Fashion-MNIST, CIFAR10, IMDB datasets. 

The main concerns from the reviewers are that experimental studies are still preliminary and the understanding on the observed interesting phenomenon is premature. The authors make comprehensive responses to the raised questions with new experiments and some reviewers raise the rating. 

The reviewers all agree that the paper presents a novel study on neural network from an algebraic topology perspective with interesting results that has not been seen before. The paper is thus suggested to be borderline lean accept. 
"
iclr_2019_Byxpfh0cFm,"The paper proposes several subsampling policies to achieve a clear reduction in the size of augmented data while maintaining the accuracy of using a standard data augmentation method. The paper in general is clearly written and easy to follow, and provides sufficiently convincing experimental results to support the claim. After reading the authors' response and revision, the reviewers have reached a general consensus that the paper is above the acceptance bar. "
iclr_2019_ByzcS3AcYX,"The paper proposes using GANs for disentangling style information from speech content, and thereby improve style transfer in TTS. The review and responses for this paper have been especially thorough! The authors significantly improved the paper during the review process, as pointed out by the reviewers. Inclusion of additional baselines, evaluations and ablation analysis helped improve the overall quality of the paper and helped alleviate concerns raised by the reviewers. Therefore, it is recommended that the paper be accepted for publication."
iclr_2019_H1MW72AcK7,"The paper makes progress on a problem that is still largely unexplored, presents promising results, and builds bridges with 
prior work on optimal control.  It designs input convex recurrent neural networks to capture temporal behavior of 
dynamical systems; this then allows optimal controllers to be computed by solving a convex model predictive control problem.

There were initial critiques regarding some of the claims. These have now been clarified.
Also, there is in the end a compromise between the (necessary) approximations of the input-convex model and the true dynamics, and being able to compute an optimal result. 

Overall, all reviewers and the AC are in agreement to see this paper accepted.
There was extensive and productive interaction between the reviewers and authors.
It makes contributions that will be of interest to many, and builds interesting bridges with known control methods."
iclr_2019_H1MgjoR9tQ,"This paper presents CMOW—an unsupervised sentence representation learning method that treats sentences as the product of their word matrices. This method is not entirely novel, as the authors acknowledge, but it has not been successfully applied to downstream tasks before. This paper presents methods for successfully training it, and shows results on the SentEval benchmark suite for sentence representations and an associated set of analysis tasks.

All three reviewers agree that the results are unimpressive: CMOW is no better than the faster CBOW baseline on most tasks, and the combination of the two is only marginally better than CBOW. However, CMOW does show some real advantages on the analysis tasks. No reviewer has any major correctness concerns that I can see.

As I see it, this paper is borderline, but narrowly worth accepting: As a methods paper, it presents weak results, and it's not likely that many practitioners will leap to use the method. However, the method is so appealingly simple and well known that there is some value in seeing this as an analysis paper that thoroughly evaluates it. Because it is so simple, it will likely be of interest to researchers beyond just the NLP domain in which it is tested (as CBOW-style models have been), so ICLR seems like an appropriate venue. It seems like it's in the community's best interest to see a method like this be evaluated, and since this paper appears to offer a thorough and sound evaluation, I recommend acceptance."
iclr_2019_H1eSS3CcKX,"This paper proposes a general-purpose continuous relaxation of the output of the sorting operator. This enables end-to-end training to enable more efficient stochastic optimization over the combinatorially large space of permutations.

In the submitted versions, two of the reviewers had difficulty in understanding the writing. After the rebuttal and the revised version, one of the reviewers is satisfied. I personally went through the paper and found that it could be tricky to read certain parts of the paper. For example, I am personally very familiar with the Placket-Luce model but the writing in Section 2.1 does not do a good job in explaining the model (particularly Eq 1 is not very easy to read, same with Eq. 3 for the key identity used in the paper). 

I encourage authors to improve writing and make it a bit more intuitive to read.

Overall, this is a good paper and I recommend to accept it.
"
iclr_2019_H1ebTsActm,"The paper extends the results in Yarotsky (2017) from Sobolev spaces to Besov spaces, stating that once the target function lies in certain Besov spaces, there exists some deep neural networks with ReLU activation that approximate the target in the minimax optimal rates. Such adaptive networks can be found by empirical risk minimization, which however is not yet known to be found by SGDs etc. This gap is the key weakness of applying approximation theory to the study of constructive deep neural networks of certain approximation spaces, which lacks algorithmic guarantees. The gap is hoped to be filled in future studies. 

Despite the incompleteness of approximation theory, this paper is still a good solid work. Based on fact that the majority of reviewers suggest accept (6,8,6), with some concerns on the clarity, the paper is proposed as probable accept. "
iclr_2019_H1edIiA9KQ,"The submission proposes a model to generate images where one can control the fine-grained locations of objects. This is achieved by adding an ""object pathway"" to the GAN architecture. Experiments on a number of baselines are performed, including a number of reviewer-suggested metrics that were added post-rebuttal.

The method needs bounding boxes of the objects to be placed (and labels). The proposed method is simple and likely novel and I like the evaluating done with Yolov3 to get a sense of the object detection performance on the generated images. I find the results (qual & quant) and write-up compelling and I think that the method will be of practical relevance, especially in creative applications.

Because of this, I recommend acceptance."
iclr_2019_H1emus0qF7,"Strong paper on hierarchical RL with very strong reviews from people expert in this subarea that I know well.
"
iclr_2019_H1eqjiCctX,"AR1 is concerned about lack of downstream applications which show that higher-order interactions are useful and asks why not to model higher-order interactions for all (a,b) pairs. AR2 notes that this submission is a further development of Arora et al. and is satisfied with the paper. AR3 is the most critical regarding lack of explanations, e.g. why linear addition of two word embeddings is bad and why the corrective term proposed here is a good idea. The authors suggest that linear addition is insufficient when final meaning differs from the individual meanings and show tome quantitative results to back up their corrective term.

On balance, all reviewers find the theoretical contributions sufficient which warrants an accept. The authors are asked to honestly reflect all uncertain aspects of their work in the final draft to reflect legitimate concerns of reviewers."
iclr_2019_H1ersoRqtm,"This paper examines ways of encoding structured input such as source code or parsed natural language into representations that are conducive for summarization. Specifically, the innovation is to not use only a sequence model, nor only a tree model, but both. Empirical evaluation is extensive, and it is exhaustively demonstrated that combining both models provides the best results.

The major perceived issue of the paper is the lack of methodological novelty, which the authors acknowledge. In addition, there are other existing graph-based architectures that have not been compared to.

However, given that the experimental results are informative and convincing, I think that the paper is a reasonable candidate to be accepted to the conference."
iclr_2019_H1ewdiR5tQ,"AR1 and AR3 have found this paper interesting in terms of replacing the spectral operations in GCN by wavelet operations. However, AR4 was more critical about the poor complexity of the proposed method compared to approximations in Hammond et al. AR4 was also right to find the proposed work similar to Chebyshev approximations in ChebNet and to highlight that the proposed approach is only marginally better than GCN. On balance, all reviewers find some merit in this work thus AC advocates an accept. The authors are asked to keep the contents of the final draft as agreed with AR4 (and other reviewers) during rebuttal without making any further theoretical changes/brushing over various new claims/ideas unsolicited by the reviewers (otherwise such changes would require passing the draft again through reviewers)."
iclr_2019_H1fU8iAqKX,"The overall consensus after an extended discussion of the paper is that this work should be accepted to ICLR. The back-and-forth between reviewers and authors was very productive, and resulted in substantial clarification of the work, and modification (trending positive) of the reviewer scores."
iclr_2019_H1g0Z3A9Fm,"This paper introduces a new graph convolutional neural network, called LGNN, and applied it to solve the community detection problem. The reviewers think LGNN yields a nice and useful extension of graph CNN, especially in using the line graph of edge adjacencies and a non-backtracking operator.  The empirical evaluation shows that the new method provides a useful tool for real datasets. The reviewers raised some issues in writing and reference, for which the authors have provided clarification and modified the papers accordingly.   "
iclr_2019_H1g2NhC5KQ,"The paper shows how techniques introduced in the context of unsupervised machine translation can be used to build a style transfer methods.

Pros:

-  The approach is simple and questions assumptions made by previous style transfer methods (specifically, they show that we do not need to specifically enforce disentanglement).  

-  The evaluation is thorough and shows benefits of the proposed method

-  Multi-attribute style transfer is introduced and benchmarks are created

-  Given the success of unsupervised NMT, it makes a lot of sense to see if it can be applied to the style transfer problem

Cons:

- Technical novelty is limited 

- Some findings may be somewhat trivial (e.g., we already know that offline classifiers are stronger than the adversarials, e.g., see Elazar and Goldberg, EMNLP 2018).




"
iclr_2019_H1g4k309F7,"The paper proposes a novel way to ensemble multi-class or multi-label models
based on a Wasserstein barycenter approach. The approach is theoretically
justified and obtains good results. Reviewers were concerned with time
complexity, and authors provided a clear breakdown of the complexity.
Overall, all reviewers were positives in their scores, and I recommend accepting the paper."
iclr_2019_H1g6osRcFQ,"The paper presents quite a simple idea to transfer a policy between domains by conditioning
the orginal learned policy on the physical parameter used in dynamics randomization.  CMA-ES then
finds the best parameters in the target domain. Importantly, it is shown to work well, 
for examples where the dynamics randomization parameters do not span the parameters that are
actually changed, i.e., as is likely common in reality-gap problems.

A weakness is the size of the contribution beyond UPOSI (Yu et al. 2017), the closest work.
The authors now explicitly benchmark against this, with (generally) positive results.
AC: It would be ideal to see that the method does truly help span the reality gap, by seeing working sim2real transfer.

Overall, the reviewers and AC are in agreement that this is a good idea that is likely to have impact.
Its fundamental simplicity means that it can also readily be used as a benchmark in future sim2real work.
The AC recommend it be considered for oral presentation based on its simplicity, the importance of
the sim2real problem, and particularly if it can be demonstrated to work well on actual
sim2real transfer tasks (not yet shown in the current results).
"
iclr_2019_H1gKYo09tX,"Overall this paper presents a few improvements over the code2vec model of Alon et al., applying it to seq2seq tasks. The empirical results are very good, and there is fairly extensive experimentation.

This is a relatively crowded space, so there are a few natural baselines that were not compared to, but I don't think that comparison to every single baseline is warranted or necessary, and the authors have done an admirable job. One thing that still is quite puzzling is the strength of the ""AST nodes only baseline"", which the authors have given a few explanations for (using nodes helps focus on variables, and also there is an effect of combining together things that are close together in the AST tree). Still, this result doesn't seem to mesh with the overall story of the paper all that well, and again opens up some obvious questions such as whether a Transformer model trained on only AST nodes would have done similarly, and if not why not.

This paper is very much on the borderline, so if there is space in the conference I think it would be a reasonable addition, but there could also be an argument made that the paper would be stronger in a re-submission where the above questions are answered."
iclr_2019_H1gL-2A9Ym,"There were several ambivalent reviews for this submission and one favorable one. Although this is a difficult case, I am recommending accepting the paper.

There were two main questions in my mind.
1. Did the authors justify that the limited neighborhood problem they try to fix with their method is a real problem and that they fixed it? If so, accept.

Here I believe evidence has been presented, but the case remains undecided.

2. If they have not, is the method/experiments sufficiently useful to be interesting anyway?

This question I would lean towards answering in the affirmative.

I believe the paper as a whole is sufficiently interesting and executed sufficiently well to be accepted, although I was not convinced of the first point (1) above. One review voting to reject did not find the conceptual contribution very valuable but still thought the paper was not severely flawed. I am partly down-weighting the conceptual criticism they made. I am more concerned with experimental issues. However, I did not see sufficiently severe issues raised by the reviewers to justify rejection.

Ultimately, I could go either way on this case, but I think some members of the community will benefit from reading this work enough that it should be accepted."
iclr_2019_H1gMCsAqY7,"This paper proposed a method that creates neural networks that can run under different resource constraints. The reviewers have consensus on accept. The pro is that the paper is novel and provides a practical approach to adjust model for different computation resource, and achieved performance improvement on object detection. One concern from reviewer2 and another public reviewer is the inconsistent performance impact on classification/detection (performance improvement on detection, but performance degradation on classification). Besides, the numbers reported in Table 1 should be confirmed: MobileNet v1 on Google Pixel 1 should have less than 120ms latency [1], not 296 ms. 


[1] Table 4 of https://arxiv.org/pdf/1801.04381.pdf"
iclr_2019_H1gR5iR5FX,"
Pros:
- A useful and well-structured dataset which will be of use to the community
- Well-written and clear (though see Reviewer 2's comment concerning the clarity of the model description section)
- Good methodology

Cons:
- There is a question about why a new dataset is needed rather than a combination of previous datasets and also why these datasets couldn't be harvested from school texts directly.  Presumably it would've been a lot more work but please address the issue in your rebuttal.
- Evaluation: Reviewer 3 is concerned that the evaluation should perhaps have included more mathematics-specific models (a couple of which are mentioned in the text).  On the other hand, Reviewer 2 is concerned that the specific choices (e.g. ""thinking steps"") made for the general models are non-standard in seq-2-seq models.  I haven't heard about the thinking step approach but perhaps it's out there somewhere. It would be helpful generally to have more discussion about the reasoning involved in these decisions.

I think this is a useful contribution to the community, well written and thoughtfully constructed.  I am tentatively accepting this paper with the understanding that you will engage directly with the reviewers to address their concerns about the evaluation section.  Please in particular use the rebuttal period to focus on the clarity of the model description and the motivation for the particular models chosen.  Also consider adding additional experiments to allay the concerns of the reviewers."
iclr_2019_H1gTEj09FX,"This paper builds on the recent DCFNet (Decomposed Convolutional Filters) architecture to incorporate rotation equivariance while preserving stability. The core idea is to decompose the trainable filters into a steerable representation and learn over a subset of the coefficients of that representation. 
Reviewers all agreed that this is a solid contribution that advances research into group equivariant CNNs, bringing efficiency gains and stability guarantees, albeit these appear to be incremental with respect to the techniques developed in the DCFNet work. In summary, the AC believes this to be a valuable contribution and therefore recommends acceptance. "
iclr_2019_H1gfOiAqYm,"This paper presents a system which exploits semantic information of partial programs during program synthesis, and ensembling of synthesisers. The idea is general, and admirably simple. The explanation is clear, and the results are impressive. The reviewers, some after significant discussion, agree that this paper makes an import contribution and is one of the stronger papers in the conference. While some possible improvements to the method and experiment were discussed with the reviewers, it seems these are more suitable for future research, and that the paper is clearly publishable in its current form."
iclr_2019_H1goBoR9F7,"This paper proposes a novel approach for network pruning in both training and inference. This paper received a consensus of acceptance. Compared with previous work that focus and model compression on training, this paper saves memory and accelerates both training and inference. It is activation, rather than weight that dominates the training memory. Reviewer1 posed a valid concern about the efficient implementation on GPUs, and authors agreed that practical speedup on GPU is difficult. It'll be great if the authors can give practical insights on how to achieve real speedup in the final draft. "
iclr_2019_H1gsz30cKX,"The paper explores the effect of normalization and initialization in residual networks, motivated by the need to avoid exploding and vanishing activations and gradients. Based on some theoretical analysis of stepsizes in SGD, the authors propose a sensible but effective way of initializing a network that greatly increases training stability. In a nutshell, the method comes down to initializing the residual layers such that a single step of SGD results in a change in activations that is invariant to the depth of the network. The experiments in the paper provide supporting evidence for the benefits; the authors were able to train networks of up to 10,000 layers deep. The experiments have sufficient depth to support the claims. Overall, the method seems to be a simple but effective technique for learning very deep residual networks. 

While some aspects of the network have been used in earlier work, such as initializing residual branches to output zeros, these earlier methods lacked the rescaling aspect, which seems crucial to the performance of this network.

The reviewers agree that the papers provides interesting ideas and significant theoretical and empirical contributions. The main concerns by the reviewers were addressed by the author responses. The AC finds that the remaining concerns raised by the reviewers are minor and insufficient for rejection of the paper.
"
iclr_2019_H1l7bnR5Ym,"The paper proposes a new method that builds on the Bayesian modelling framework for GANs and is supported by a theoretical analysis and an empirical evaluation that shows very promising results. All reviewers agree, that the method is interesting and the results are convincing, but that the model does not really fit in the standard Bayesian setting due to a data dependency of the priors. I would therefore encourage the authors to reflect this by adapting the title and making the differences more clear in the camera ready version."
iclr_2019_H1lJJnR5Ym,"Pros:
- novel, general idea for hard exploration domains
- multiple additional tricks
- ablations, control experiments
- well-written paper
- excellent results on Montezuma

Cons:
- low sample efficiency (2B+ frames)
- unresolved questions (non-episodic intrinsic rewards)
- could have done better apples-to-apples comparisons to baselines

The reviewers did not reach consensus on whether to accept or reject the paper. In particular, after multiple rounds of discussion, reviewer 1 remains adamant that the downsides of the paper outweigh its good points. However, given that the other three reviewers argue strongly and credibly for acceptance, I think the paper should be accepted."
iclr_2019_H1lqZhRcFm,"The paper proposes a new unsupervised learning scheme via utilizing local maxima as an indicator function.

The reviewers and AC note the novelty of this paper and good empirical justifications. Hence, AC decided to recommend acceptance.

However, AC thinks the readability of the paper can be improved."
iclr_2019_H1x-x309tm,"This paper analysis the convergence properties of a family of 'Adam-Type' optimization algorithms, such as Adam, Amsgrad and AdaGrad, in the non-convex setting. The paper provides of the first comprehensive analyses of such algorithms in the non-convex setting. In addition, the results can help practitioners with monitoring convergence in experiments. Since Adam is a widely used method, the results have a potentially large impact.

The reviewers agree that the paper is well-written, provides interesting new insights, and that is results are of sufficient interest to the ICLR community to be worthy of publication."
iclr_2019_H1xD9sR5Fm,"The reviewers agree  that the paper is worthy of publication at ICLR, hence I recommend accept.

Regarding section 4.3 of the submission and the claim that this paper presents the first insight for existing work from a divergence minimization perspective, as pointed out by R2, I went and checked the details of RAML and they have similar insights in their equations (5) and (8). Please make this clearer in the paper. Regarding evaluation using greedy search instead of beam search, please consider using beam search for reporting test performance as this is the standard setup in sequence prediction. Please take my comments and the reviews into account an prepare the final version."
iclr_2019_H1xQVn09FX,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- novel approach to audio synthesis
- strong qualitative and quantitative results
- extensive evaluation
 
2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- small grammatical issues (mostly resolved in the revision).
 
3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

No major points of contention.
 
4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be accepted.
"
iclr_2019_H1xaJn05FQ,"The paper proposed to add the sliced-Wasserstein distance between the distribution of the encoded training samples and a samplable prior distribution to the auto encoder (AE) loss, resulting in a model named sliced-Wasserstein AE. The difference compared to the Wasserstein AE (WAE) lies in using the usage of the sliced-Wasserstein distance instead of GAN or MMD-based penalties.
The idea of the paper is interesting, and a theoretical and an empirical analysis supporting the approach are presented. As reviewer 1 noticed, „the advantage of using sliced Wasserstein distance is twofold: 1)parametric-free (compared to GANs); 2) almost hyperparameter-free (compared to the MMD with RBF kernels), except setting the number of random projection bases.“ However, the empirical evaluation in the paper and concurrent ICLR submission on Cramer-World-AEs the authors refer to shows no clear practical advantage over the WAE, which leads to better results at least regarding the FID score. On the other hand, the Cramer-World-AE is based on the ideas presented in this paper (which was previously available on arxive) proving that the paper presents interesting ideas which are of value to the communty. Therefore, the paper is a bit boarderline, but I recommand to accept it. "
iclr_2019_H1xipsA5K7,"Although the paper considers a somewhat limited problem of learning a neural network with a single hidden layer, it achieves a surprisingly strong result that such a network can be learned exactly (or well approximated under sampling) under weaker assumptions than recent work.  The reviewers unanimously recommended the paper be accepted.  The paper would be more impactful if the authors could clarify the barriers to extending the technique of pure neuron detection to deeper networks, as well as the barriers to incorporating bias to eliminate the symmetry assumption."
iclr_2019_H1xsSjC9Ym,"Pros:
- The paper is well-written and clear and presented with helpful illustrations and videos.
- The  training methodology seems sound (multiple random seeds etc.)
- The results are encouraging.

Cons:
- There was some concern generally about how this work is positioned relative to related work and the completeness of the related work.  However, the authors have made this clearer in their rebuttal.

There was a considerable amount of discussion between the authors and all reviewers to pin down some unclear aspects of the paper. I believe in the end there was good convergence and I thank both the authors and reviewers for their persistence and dilligence in working through this.  The final paper is much better I think and I recommend acceptance."
iclr_2019_H1xwNhCcYm,"This paper makes the intriguing observation that a density model trained on CIFAR10 has higher likelihood on SVHN than CIFAR10, i.e., it assigns higher probability to inputs that are out of the training distribution. This phenomenon is also shown to occur for several other dataset pairs. This finding is surprising and interesting, and the exposition is generally clear. The authors provide empirical and theoretical analysis, although based on rather strong assumptions. Overall, there's consensus among the reviewers that the paper would make a valuable contribution to the proceedings, and should therefore be accepted for publication."
iclr_2019_H1z-PsR5KX,"Strong points:

-- Interesting, fairly systematic and novel analyses of recurrent NMT models, revealing individual neurons responsible for specific type of information (e.g., verb tense or gender)

-- Interesting experiments showing how these neurons can be used to manipulate translations in specific ways (e.g., specifying the gender for a pronoun when the source sentence does not reveal it)

-- The paper is well written

Weak points

-- Nothing serious (e.g., maybe interesting to test across multiple runs how stable these findings are).

There is a consensus among the reviewers that this is a strong paper and should be accepted.

"
iclr_2019_H1zeHnA9KX,"This paper presents experiments showing that a linear mapping existing between the hidden states of RNNs trained to recognise (rather than model) formal languages, in the hope of at least partially elucidating the sort of representations this class of network architectures learns. This is important and timely work, fitting into a research programme begun by CL Giles in 92.

Despite its relatively low overall score, I am concurring with the assessment made by reviewer 1, whose expertise in the topic I am aware of and respect. But more importantly, I feel the review process has failed the authors here: reviewers 2 and 3 had as chief concern that there were issues with the clarity of some aspects of the paper. The authors made a substantial and bona fide attempt in their response to address the points of concern raised by these reviewers. This is precisely what the discussion period of ICLR is for, and one would expect that clarity issues can be successfully remedied during this period. I am disappointed to have seen little timely engagement from these reviewers, or willingness to explain why they are stick by their assessment if not revisiting it. As far as I am concerned, the authors have done an appropriate job of addressing these concerns, and given reviewer 1's support for the paper, I am happy to add mine as well."
iclr_2019_H1ziPjC5Fm,"This was a difficult decision to converge to. R2 strongly champions this work, R1 is strongly critical, and R3 did not participate in the discussions (or take a stand). On the one hand, the AC can sympathize with R1's concerns -- insights developed on synthetic datasets may fail to generalize and fundamentally, the burden is not on a reviewer to be able to provide to authors a realistic dataset for the paper to experiment on. Having said that, a carefully constructed synthetic dataset is often *exactly* what the community needs as the first step to studying a difficult problem. Moreover, it is better for a proceeding to include works that generate vigorous discussions than the routine bland incremental works that typically dominate. Welcome to ICLR19. "
iclr_2019_HJE6X305Fm,The paper provides a simple method for regularising and robustifying GAN training. Always appreciated contribution to GANs. :-)
iclr_2019_HJGciiR5Y7,The reviewers are in general impressed by the results and like the idea but they also express some uncertainty about how the proposed actually is set up. The authors have made a good attempt to address the reviewers' concerns. 
iclr_2019_HJGkisCcKm,"The paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments. The model is an auto-encoder with a WaveNet-like domain-specific decoder and a shared encoder, trained with an adversarial ""domain confusion loss"". Even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance."
iclr_2019_HJGven05Y7,"This paper proposes several improvements for the MAML algorithm that improve its stability and performance.
Strengths: The improvements are useful for future researchers building upon the MAML algorithm. The results demonstrate a significant improvement over MAML. The authors revised the paper to address concerns about overstatements 
Weaknesses: The paper does not present a major conceptual advance. It would also be very helpful to present a more careful ablation study of the six individual techniques.
Overall, the significance of the results outweights the weaknesses. However, the authors are strongly encouraged to perform and include a more detailed ablation study in the final paper. I recommend accept."
iclr_2019_HJMC_iA5tm,"The submission proposes a machine learning approach to directly train a prediction system for whether a boolean sentence is satisfiable.  The strengths of the paper seem to be largely in proposing an architecture for SAT problems and the analysis of the generalization performance of the resulting classifier on classes of problems not directly seen during training.

Although the resulting system cannot be claimed to be a state of the art system, and it does not have a correctness guarantee like DPLL based approaches, the paper is a nice re-introduction of SAT in a machine learning context using deep networks.  It may be nice to mention e.g. (W. Ruml. Adaptive Tree Search. PhD thesis, Harvard University, 2002) which applied reinforcement learning techniques to SAT problems.  The empirical validation on variable sized problems, etc. is a nice contribution showing interesting generalization properties of the proposed approach.

The reviewers were unanimous in their recommendation that the paper be accepted, and the review process attracted a number of additional comments showing the broader interest of the setting."
iclr_2019_HJMCcjAcYX,"The paper proposes an architecture to learn over sets, by proposing a way
to have permutations differentiable end-to-end, hence learnable by gradient
descent. Reviewers pointed out to the computational limitation (quadratic in
the size of the set just to consider pairwise interactions, and cubic overall).
One reviewer (with low confidence) though the approach was not novel but
didn't appreciate the integration of learning-to-permute with a differentiable
setting, so I decided to down-weight their score. Overall, I found the paper
borderline but would propose to accept it if possible."
iclr_2019_HJMHpjC9Ym,This paper propose a novel CNN architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. reviewers generally arrived at a consensus on accept.
iclr_2019_HJe62s09tX,"This paper provides a simple and intuitive method for learning multilingual word embeddings that makes it possible to softly encourage the model to align the spaces of non-English language pairs. The results are better than learning just pairwise embeddings with English.

The main remaining concern (in my mind) after the author response is that the method is less accurate empirically than Chen and Cardie (2018). I think however that given that these two works are largely contemporaneous, the methods are appreciably different, and the proposed method also has advantages with respect to speed, that the paper here is still a reasonably candidate for acceptance at ICLR.

However, I would like to request that in the final version the authors feature Chen and Cardie (2018) more prominently in the introduction and discuss the theoretical and empirical differences between the two methods. This will make sure that readers get the full picture of the two works and understand their relative differences and advantages/disadvantages."
iclr_2019_HJeRkh05Km,"The authors propose an approach for visual navigation that leverages a semantic knowledge graph to ground and inform the policy of an RL agent. The agent uses a graphnet to learn relationships and support the navigation. The empirical protocol is sound and uses best practices, and the authors have added additional experiments during the revision period, in response to the reviewers' requests. However, there were some significant problems with the submission - there were no comparisons to other semantic navigation methods, the approach is somewhat convoluted and will not survive the test of time, and the authors did not conclusively show the value of their approach. The reviewers uniformly support the publication of this paper, but with a low confidence. "
iclr_2019_HJeu43ActQ,"Alternating minimization is surprisingly effective for low-rank matrix factorization and dictionary learning problems. Better theoretical characterization of these methods is well motivated. This paper fills up a gap by providing simultaneous guarantees for support recovery as well as coefficient estimates for  linearly convergence to the true factors, in the online learning setting. The reviewers are largely in agreement that the paper is well written and makes a valuable contribution.  The authors are advised to address some of the review comments around relationship to prior work highlighting novelties."
iclr_2019_HJf9ZhC9FX,"The authors give a characterization of stochastic mirror descent (SMD) as a conservation law (17) in terms of the Bregman divergence of the loss. The identity allows the authors to show that SMD converges to the optimal solution of a particular minimax filtering problem. In the special overparametrized linear case, when SMD is simply SGD, the result recovers a recent theorem due to Gunasekar et al. (2018). The consequences for the overparametrized nonlinear case are more speculative.

The main criticisms are around impact, however, I'm inclined to think that any new insight on this problem, especially one that imports results from other areas like control, are useful to incorporate into the literature. 

I will comment that the discussion of previous work is wholly inadequate. The authors essentially do not engage with previous work, and mostly make throwaway citations. This is a real pity.  I would be nice to see better scholarship."
iclr_2019_HJfSEnRqKQ,"This paper is on active deep learning in the setting where a label hierarchy is available for multiclass classification problems: a fairly natural and pervasive setting. The extension where the learner can ask for example labels as well as a series of questions to adequately descend the label hierarchy is  an interesting twist on active learning.  The paper is well written and develops several natural formulations which are then benchmarked on CIFAR10, CIFAR100, and Tiny ImageNet using a ResNet-18 architecture.  The empirical results are carefully analyzed and appear to set interesting new baselines for active learning. "
iclr_2019_HJflg30qKX,"This paper studies the behavior of weight parameters for linear networks when trained on separable data with strictly decreasing loss functions. For this setting the paper shows that the gradient descent solution converges to max margin solution and each layer converges to a rank 1 matrix with consequent layers aligned.  All reviewers agree that the paper provides novel results for understanding implicit regularization effects of gradient descent for linear networks. Despite the limitations of this paper such as studying networks with linear activation, studying gradient descent not with practical step sizes, assuming data is linearly separable, reviewers find the results useful and a good addition to existing literature."
iclr_2019_HJfwJ2A5KX,"The reviewers and AC note that the strength of the paper includes a) an interesting compression algorithm of neural networks with provable guarantees (under some assumptions), b) solid experimental comparison with the existing *matrix sparsification* algorithms. The AC's main concern of the experimental part of the paper is that it doesn't outperform or match the performance of the ""vanilla"" neural network compression algorithms such as Han et al'15. The AC decided to suggest acceptance for the paper but also strongly encourage the paper to clarify the algorithms in comparison don't include state-of-the-art compression algorithms. "
iclr_2019_HJgXsjA5tQ,"This paper introduces a class of deep neural nets that provably have no bad local valleys. By constructing a new class of network this paper avoids having to rely on unrealistic assumptions and manages to provide a relatively concise proof that the network family has no strict local minima. Furthermore, it is demonstrated that this type of network yields reasonable experimental results on some benchmarks. The reviewers identified issues such as missing measurements of the training loss, which is the actual quantity studied in the theoretical results, as well as some issues with the presentation of the results. After revisions the reviewers are satisfied that their comments have been addressed. This paper continues an interesting line of theoretical research and brings it closer to practice and so it should be of interest to the ICLR community."
iclr_2019_HJgd1nAqFX,"This paper considers the task of web navigation, i.e. given a goal expressed in natural language, the task is to navigate webs by filling up fields and clicking links. The proposed model uses reinforcement learning, introducing a novel extension where the graph embedding of the pages is incorporated into the Q-function. The results are sound, and the paper is overall well-written.

The reviewers and AC note the following potential weaknesses. The primary concern that was raised was the novelty. Since the task could potentially be framed as semantic parsing, reviewer 4 mentioned there may be readily available approaches for baselines that the authors did not consider. The comparison to semantic parsing required a more detailed discussion, pointing not only the differences but also the similarities, that would encourage the two communities to explore novel approaches to their tasks. Further, reviewer 2 was concerned about the limited novelty, given the extensive work that combines GNN and RL, such as NerveNet.

The authors provided comments and a revision to address these issues. They described why it is not trivial to formulate their setup as a semantic parsing problem, partly due to the fact that the environment is partially observable.
Similarly, the authors described the differences between the proposed approach and methods like NerveNet, such as the use of a dynamic graph and off-policy RL, making the latter not a viable baseline for the task. These changes addressed most of the concerns raised by the reviewers.

The reviewers agreed that this paper should be accepted."
iclr_2019_HJgeEh09KQ,"The paper addresess an important problem of neural net robustness verification, and presents a novel approach outperforming state of art; author provided details rebuttals which clarified their contributions over the state of art and highlighted scalability; this work appears to be a solid and useful contribution to the field.
 "
iclr_2019_HJgkx2Aqt7,"This paper discusses the promising idea of using RL for optimizing simulators’ parameters. 

The theme of this paper was very well received by the reviewers. Initial concerns about insufficient experimentation were justified, however the amendments done during the rebuttal period ameliorated this issue. The authors argue that due to considered domain and status of existing literature, extensive comparisons are difficult. The AC sympathizes with this argument, however it is still advised that the experiments are conducted in a more conclusive way, for example by disentangling the effects of the different choices made by the proposed model. For example, how would different sampling strategies for optimization perform? Are there more natural black-box optimization methods to use?

The reviewers believe that the methodology followed has a lot of space for improvement. However, the paper presents some fresh and intriguing ideas, which make it overall a relevant work for presentation at ICLR."
iclr_2019_HJlLKjR9FQ,"+ the ideas presented in the paper are quite intriguing and draw on a variety of different connections
- the presentation has a lot of room for improvement. In particular, the statement of Theorem 1, in its current form, requires rephrasing and making it more rigorous. 

Still, the general consensus is that, once these presentation shortcomings are address, this will be an interesting paper.
"
iclr_2019_HJlNpoA5YQ,"This paper provides a novel and non-trivial method for approximating the eigenvectors of the Laplacian, in large or continuous state environments. Eigenvectors of the Laplacian have been used for proto-value functions and eigenoptions, but it has remained an open problem to extend their use to the non-tabular case. This paper makes an important advance towards this goal, and will be of interest to many that would like to learn state representations based on the geometric information given by the Laplacian. 

The paper could be made stronger by including a short discussion on why the limitations of this approach. Its an important new direction, but there must still be open questions (e.g., issues with the approach used to approximate the orthogonality constraint). It will be beneficial to readers to understand these issues."
iclr_2019_HJlQfnCqKX,"The paper suggests a new measurement of layer-wise margin distributions for generalization ability. Extensive experiments are conducted. Though there lacks a solid theory to explain the phenomenon. The majority of reviewers suggest acceptance (9,6,5). Therefore, it is proposed as probable accept."
iclr_2019_HJlmHoR5tQ,"This paper proposes a regularization for IRL based on empowerment. The paper has some good results, and is generally well-written. The reviewers raised concerns about how the approach was motivated; these concerns have largely been addressed from the reframing of the algorithm from the perspective of regularization. Now, all reviewers agree that the paper is somewhat above the bar for acceptance. Hence, I also recommend accept. There are several changes that the authors are strongly encouraged to incorporate in the final version of the paper (based on discussion between the reviewers):
- The claim that empowerment acts as a regularizer in the policy update is a fairly complicated interpretation of the effect of the algorithm. It relies on an approximation derived in the appendix that relates the proposed objective with an empowerment regularized IRL formulation. The new framing makes much more sense. However, the one sentence reference to this section of the appendix in the main paper is not appropriate given that it is central to the claims of the paper's contribution. More discussion in the main text should be included.
- There are still some parts of the implemented algorithm that could introduce bias (using a target network in the shaping term which differs from the theory in Ng et al. 1999), but this concern could be remedied by a code release. The authors are strongly encouraged to link to the code in the final non-blind submission, especially since IRL implementations tend to be quite difficult to get right.
- The authors said they would change the way they bold their best numbers in their rebuttal. The current paper does not make the promised change, and actually adopts different bolding conventions in different tables which is even more confusing. The numbers should be bolded in a consistent way, bolding the numbers with the best performance up to statistical significance."
iclr_2019_HJx9EhC9tQ,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- The problem is interesting and challenging
- The proposed approach is novel and performs well.

2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- The clarity could be improved

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

Many concerns were clarified during the discussion period. One major concern had been the experimental evaluation. In particular, some reviewers felt that experiments on real images (rather than in simulation) was needed.
To strengthen this aspect, the authors added new qualitative and quantitative results on a real-world experiment with a robot arm, under 10 different scenarios, showing good performance on this challenging task. Still, one reviewer was left unconvinced that the experimental evaluation was sufficient.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

Consensus was not reached. The final decision is aligned with the positive reviews as the AC believes that the evaluation was adequate.
"
iclr_2019_HJxB5sRcFQ,"Reviewers agree the paper should be accepted.
See reviews below."
iclr_2019_HJxeWnCcF7,"This paper proposes a novel framework for tractably learning non-eucliean embeddings that are product spaces formed by hyperbolic, spherical, and Euclidean components, providing a heterogenous mix of curvature properties.  On several datasets, these product space embeddings outperform single Euclidean or hyperbolic spaces. The reviewers unanimously recommend acceptance."
iclr_2019_HJxwDiActX,"The paper proposes a novel differential way to output brush strokes, taking a few ideas from model-based learning. The method is efficient in that one can train it in an unsupervised manner and does not require paired data. The strengths of the paper are the qualitative results that demonstrate nice interpolations among other things, on a number of datasets (esp. post-rebuttal).

The weaknesses of the paper are the writing (which I think is relatively easy to improve if the authors make an honest effort) and some of the quantitative evaluation. I would encourage the authors to get in touch with the SPIRAL paper authors in order to get access to the SPIRAL generated MNIST test data and then perhaps the classification metric could be updated.

In summary, from the discussion, the major points of contention were the somewhat lacking initial evaluation (which was fixed to a large extent) and the quality of writing (which could be fixed more). I believe the submission is genuinely novel, interesting (esp. the usage of world model-like techniques) and valuable for the ICLR audience so I recommend acceptance."
iclr_2019_HJxyAjRcFX,The paper presents new loss functions (which replace the reconstruction part) for the training of conditional GANs. Theoretical considerations and an empirical analysis show that the proposed loss can better handle multimodality of the target distribution than reconstruction based losses while being competitive in terms of image quality.
iclr_2019_HJz05o0qK7,"This paper presents a method for measuring the degree to which some representation for a composed object effectively represents the pieces from which it is composed. All three authors found this to be an important topic for study, and found the paper to be a limited but original and important step toward studying this topic. However, two reviewers expressed serious concerns about clarity, and were not fully satisfied with the revisions made so far. I'm recommending acceptance, but I ask the authors to further revise the paper (especially the introduction) to make sure it includes a blunt and straightforward presentation of the problem under study and the way TRE addresses it.

I'm also somewhat concerned at R2's mention of a potential confound in one experiment. The paper has been updated with what appears to be a fix, though, and R2 has not yet responded, so I'm presuming that this issue has been resolved.

I also ask the authors to release code shortly upon de-anonymization, as promised."
iclr_2019_HJz6tiCqYm,"The reviewers have all recommended accepting this paper thus I am as well. Based on the reviews and the selectivity of the single track for oral presentations, I am only recommending acceptance as a poster."
iclr_2019_Hk4dFjR5K7,"The submission proposes a method to construct adversarial attacks based on deforming an input image rather than adding small peturbations.  Although deformations can also be characterized by the difference of the original and deformed image, it is qualitatively and quantitatively different as a small deformation can result in a large difference.

On the positive side, this paper proposes an interesting form of adversarial attack, whose success can give additional insights on the forms of existing adversarial attacks.  The experiments on MNIST and ImageNet are reasonably comprehensive and allow interesting interpretation of how the image deforms to allow the attack.  The paper is also praised for its clarity, and cleaner formulation compared to Xiao et al. (see below).  Additional experiments during rebuttal phase partially answered reviewer concerns, and provided more information e.g. about the effect of the smoothness of the deformation.

There were some concerns that the paper primarly presents one idea, and perhaps missed an opportunity for deeper analysis (R1).  R2 would have appreciated more analysis on how to defend against the attack.

A controversial point is the relation /  novelty with respect to Xiao et al., ICLR 2018.  As e.g. pointed out by R1: ""The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to ICLR 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.]).""

On the balance, all three reviewers recommended acceptance of the paper.  Regarding novelty over Xiao et al., even ignoring the arguable precedence of the current submission, the formulation is cleaner and will likely advance the analysis of adversarial attacks."
iclr_2019_Hk4fpoA5Km,"This work highlights the problem of biased rewards present in common adversarial imitation learning implementations, and proposes adding absorbing states to to fix the issue. This is combined with an off-policy training algorithm, yielding significantly improved sample efficiency, whose benefits are convincingly shown empirically. The paper is well written and clearly presents the contributions. Questions were satisfactorily answered during discussion, and resulted in an improved submission, a paper that all reviewers now agree is worth presenting at ICLR.
"
iclr_2019_HkG3e205K7,"The paper is well written and easy to follow. The experiments are adequate to justify the usefulness of an identity for improving existing multi-Monte-Carlo-sample based gradient estimators for deep generative models. The originality and significance are acceptable, as discussed below.

The proposed doubly reparameterized gradient estimators are built on an important identity shown in Equation (5). This identity appears straightforward to derive by applying both score-function gradient and reparameterization gradient to the same objective function, which is expressed as an expectation. The AC suspects that this identity might have already appeared in previous publications / implementations, though not being claimed as an important contribution / being explicitly discussed. While that identity may not be claimed as the original contribution of the paper if that suspicion is true, the paper makes another useful contribution in applying that identity to the right problem: improving three distinct training algorithms for deep generative models. The doubly reparameterized versions of IWAE and reweighted wake-sleep (RWS) further show how IWAE and RWS are related to each other and how they can be combined for potentially further improved performance. 

The AC believes that the paper makes enough contributions by well presenting the identity in (5) and applying it to the right problems. "
iclr_2019_HkNGYjR9FX,"This work proposes a simple but useful way to train RNN with binary / ternary weights for improving memory and power efficiency. The paper presented a sequence of experiments on various benchmarks and demonstrated significant improvement on memory size  with only minor decrease of accuracy. Authors' rebuttal addressed the reviewers' concern nicely. 

"
iclr_2019_Hke-JhA9Y7,"The reviewers all feel that the paper should be accepted to the conference.  The main strengths that they noted were the quality of writing, the wide applicability of the proposed method and the strength of the empirical evaluation.  It's nice to see experiments across a large number of problems (100), with corresponding code, where baselines were hyperparameter tuned as well.  This helps to give some assurance that the method will generalize to new problems and datasets.    Some weaknesses noted by the reviewers were computational cost (the method is significantly slower than the baselines) and they weren't entirely convinced that having more concise representations would directly lead to the claimed interpretability of the approach.  Nevertheless, they found it would make for a solid contribution to the conference."
iclr_2019_Hke20iA9Y7,"This paper presents methods to scale learning of embedding models estimated using neural networks. The main idea is to work with Gram matrices whose sizes depend on the length of the embedding. Building upon existing works like SAG algorithm, the paper proposes two new stochastic methods for learning using stochastic estimates of Gram matrices. 

Reviewers find the paper interesting and useful, although have given many suggestions to improve the presentation and experiments. For this reason, I recommend to accept this paper.

A small note: SAG algorithm was originally proposed in 2013. The paper only cites the 2017 version. Please include the 2013 version as well.
"
iclr_2019_Hke4l2AcKQ,"This paper proposes a solution for the well-known problem of posterior collapse in VAEs: a phenomenon where the posteriors fail to diverge from the prior, which tends to happen in situations where the decoder is overly flexible.

A downside of the proposed method is the introduction of hyper-parameters controlling the degree of regularization. The empirical results show improvements on various baselines.

The paper proposes the addition of a regularization term that penalizes pairwise similarity of posteriors in latent space. The reviewers agree that the paper is clearly written and that the method is reasonably motivated. The experiments are also sufficiently convincing."
iclr_2019_HkeGhoA5FX,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- strong qualitative and quantitative results
- a good ablative analysis of the proposed method.
 
2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- clarity could be improved (and was much improved in the revision).
- somewhat limited novelty.
 
3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

No major points of contention.
 
4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be accepted.
"
iclr_2019_HkeoOo09YX,"This paper proposes to use meta-learning to design MCMC sampling distributions based on Hamiltonian dynamics, aiming to mix faster on set of problems that are related to the training problems. The reviewers agree that the paper is well-written and the ideas are interesting and novel. The main weaknesses of the paper are that (1) there is not a clear case for using this method over SG-HMC, and (2) there are many design choices that are not validated. The authors revised the paper to address some aspects of the latter concern, but are encouraged to add additional revisions to clarify the points brought up by the reviewers.
Despite the weaknesses, the reviewers all agree that the paper exceeds the bar for acceptance. I also recommend accept."
iclr_2019_HkezXnA9YX,"This paper generated a lot of discussion. Paper presents an empirical evaluation of generalization in models for visual reasoning. All reviewers generally agree that it presents a thorough evaluation with a good set of questions. The only remaining concerns of R3 (the sole negative vote) were lack of surprise in findings and lingering questions of whether these results generalize to realistic settings. The former suffers from hindsight bias and tends to be an unreliable indicator of the impact of a paper. The latter is an open question and should be worked on, but in the opinion of the AC, does not preclude publication of this manuscript. These experiments are well done and deserve to be published. If the findings don't generalize to more complex settings, we will let the noisy process of science correct our understanding in the future. "
iclr_2019_Hkf2_sC5FX,"
Pros:
- Great work on getting rid of the need for QP and the corresponding proof of the update rule
- Mostly clear writing
- Good experimental results on relevant datasets
- Introduction of a more reasonable evaluation methodology for continual learning

Cons:
- The model is arguably a little incremental over GEM.  In the end I think all the reviewers agree though that the practical value of a considerably more efficient and easy to implement approach largely outweighs this concern.

I think this is a good contribution in this area and I recommend acceptance."
iclr_2019_HkfPSh05K7,"
pros:
- novel idea for multi-step QA which rewrites the query in embedding space
- good comparison with related work
- reasonable evaluation and improved results

cons:

There were concerns about missing training details, insufficient evaluation, and presentation.  These have been largely addressed in revision and I am recommending acceptance."
iclr_2019_HkfYOoCcYX,"The authors propose an efficient scheme for encoding sparse matrices which allow weights to be compressed efficiently. At the same time, the proposed scheme allows for fast parallelizable decompression into a dense matrix using Viterbi-based pruning. 
The reviewers noted that the techniques address an important problem relevant to deploying neural networks on resource-constrained platforms, and although the work builds on previous work, it is important from a practical standpoint. 
The reviewers noted a number of concerns on the initial draft of this work related to the experimental methodology and the absence of runtime comparison against the baseline, which the reviewers have since fixed in the revised draft. The reviewers were unanimous in recommending that the revision be accepted, and the authors are requested to incorporate the final changes that they said they would make in the camera-ready version.
"
iclr_2019_Hkg4W2AcFm,"The paper proposes a new way to tackle the trade-off between disentanglement and reconstruction, by training a teacher autoencoder that learns to disentangle, then distilling into a student model. The distillation is encouraged with a loss term that constrains the Jacobian in an interesting way. The qualitative results with image manipulation are interesting and the general idea seems to be well-liked by the reviewers (and myself).

The main weaknesses of the paper seem to be in the evaluation. Disentanglement is not exactly easy to measure as such. But overall the various ablation studies do show that the Jacobian regularization term improves meaningfully over Fader nets. Given the quality of the results and the fact that this work moves the needle in an important (albeit hard to define) area of learning disentangled representations, I think would be a good piece of work to present at ICLR so I recommend acceptance."
iclr_2019_HkgEQnRqYQ,"This paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel. The authors provide analysis to show how this model can capture symmetry/assymmetry, inversions, and composition. The authors also introduce a separate contribution of self-adversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task.

The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper.

The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation.

With the revision, the reviewers agreed that this is a worthy paper to include in the conference.
"
iclr_2019_HkgSEnA5KQ,"The paper proposes a meta-learning approach to ""language guided policy learning"" where instructions are provided in the form of natural language instructions, rather than in the form of a reward function or through demonstration. A particularly interesting novel feature of the proposed approach is that it can seamlessly incorporate natural language corrections after an initial attempt to solve the task, opening up the direction towards natural instructions through interactive dialogue. The method is empirically shown to be able to learn to navigate environments and manipulate objects more sample efficiently (on test tasks) than approaches without instructions. 

The reviewers noted several potential weaknesses: while the problem setting was considered interesting, the empirical validation was seen to be limited. Reviewers noted that only one (simple) domain was studied, and it was unclear if results would hold up in more complex domains. They also note lack of comparison to baselines based on prior work (e.g., pre-training).

The authors provided very detailed replies to the reviewer comments, and added very substantial new experiments, including an entire new domain and newly implemented baselines. Reviewers indicated that they are satisfied with the revisions. The AC reviewed the reviewer suggestions and revisions and notes that the additional experiments significantly improve the contribution of the paper. The resulting consensus is that the paper should be accepted.

The AC would like to note that several figures are very small and unreadable when the paper is printed, e.g., figure 7, and suggests that the authors increase figure size (and font size within figures) to ensure legibility."
iclr_2019_HkgTkhRcKQ,"This paper proposes a new stochastic optimization scheme similar to Adam. The authors claim that Adam can be improved upon by decorrelating the second-moment estimate v_t from gradient estimates g_t. This is done through the temporal decorrelation scheme, as well as block-wise sharing of estimates v_t.

The reviewers agree that the paper is sufficiently well-written, original and significant to be accepted for ICLR, although some unclarity remains after the reviews. A disadvantage of the method is mainly an increased computational cost (linear in 'n', however this might be negligible when sharing v_t across blocks)."
iclr_2019_HkgYmhR9KX,"The paper presents an adversarial learning framework for active visual tracking, a tracking setup where the tracker has camera control in order to follow a target object. The paper builds upon Luo et al. 2018 and proposes jointly learning  tracker and target policies (as opposed to tracker policy alone). This automatically creates a curriculum of target trajectory difficulty, as opposed to the engineer designing the target trajectories. The paper further proposes a method for preventing the target to fast outperform the tracker and thus cause his policy to plateau. Experiments presented justify the problem formulation and design choices, and outperform Luo et al. . The task considered is  very important, active surveillance with drones is just one sue case.

A downside of the paper is that certain sentences have English mistakes, such as this one:  ""The authors learn a policy that maps raw-pixel observation to control signal straightly with a Conv-LSTM network. Not only can it save
the effort in tuning an extra camera controller, but also does it outperform the..."" However, overall the manuscript is well written, well structured, and easy to follow. The authors are encouraged to correct any remaining English mistakes in the manuscript. "
iclr_2019_HkgqFiAcFm,"The paper introduces a new variance reduced policy gradient method, for directional and clipped action spaces, with provable guarantees that the gradient is lower variance. The paper is clearly written and the theory an important contribution. The experiments provide some preliminary insights that the algorithm could be beneficial in practice. "
iclr_2019_Hkl5aoR5tm,"This manuscript proposes an architectural improvement for generative adversarial network that allows the intermediate layers of a generator to be modulated by the input noise vector using conditional batch normalization. The reviewers find the paper simple and well-supported by extensive experimental results. There were some concerns about the impact of such an empirical study. However, the strength and simplicity of the technique means that the method could be of practical interest to the ICLR community."
iclr_2019_HklKui0ct7,"This is an interesting paper that shows how improved off-policy estimation (and optimization) can be improved by explicitly estimating the data logging policy.  It is remarkable that the estimation variance can be reduced over using the original logging policy for IPW, although this result depends on the (somewhat impractical) assumption that the parametric form for the true logging policy is known.  The reviewers unanimously recommended the paper be accepted.  However, there remain criticisms of the theoretical analysis that the authors should take into account in preparing a final version (namely, motivating the assumptions needed to obtain the results, and providing stronger intuitions behind the reduced variance)."
iclr_2019_HklSf3CqKm,"This paper studies non smooth and non convex optimization and provides a global analysis for orthogonal dictionary learning. The referees indicate that the analysis is highly nontrivial compared with existing work. 

The experiments fall a bit short and the relation to the loss landscape of neural networks could be described more clearly. 

The reviewers pointed out that the experiments section was too short. The revision included a few more experiments. The paper has a theoretical focus, and scores high ratings there. 

The confidence levels of the reviewers is relatively moderate, with only one confident reviewer. However, all five reviewers regard this paper positively, in particular the confident reviewer. "
iclr_2019_HklY120cYm,"The authors discuss an improved distillation scheme for parallel WaveNet using a Gaussian inverse autoregressive flow, which can be computed in closed-form, thus simplifying training. The work received favorable comments from the reviewers, along with a number of suggestions for improvement which have improved the draft considerably. The AC agrees with the reviewers that the work is a valuable contribution, particularly in the context of end-to-end neural text-to-speech systems. "
iclr_2019_HkljioCcFQ,"The paper proposes a new attentional pooling mechanism that potentially addresses the issues of simple attention-based weighted averaging (where discriminative parts/frames might get disportionately high attentions). A nice contribution of the paper is to propose an alternative mechanism with theoretical proofs, and it also presents a method for fast recurrent computation. The experimental results show that the proposed attention mechanism improves over prior methods (e.g., STPN) on THUMOS14 and ActivityNet1.3 datasets. In terms of weaknesses: (1) the computational cost may be quite significant. (2) the proposed method should be evaluated over several tasks beyond activity recognition, but it’s unclear how it would work. 

The authors provided positive proof-of-concept results on weakly supervised object localization task, improving over CAM-based methods. However, CAM baseline is a reasonable but not the strongest method and the weakly-supervised object recognition/segmentation domains are much more competitive domains, so it's unclear if the proposed method would achieve the state-of-the-art by simply replacing the weighted-averaging-attentional-pooling with the proposed attention mechanism. In addition, the description on how to perform attentional pooling over images is not clearly described (it’s not clear how the 1D sequence-based recurrent attention method can be extended to 2-D cases). However, this would not be a reason to reject the paper. 

Finally, the paper’s presentation would need improvement. I would suggest that the authors give more intuitive explanations and rationale before going into technical details. The paper starts with Figure 1 which is not really well motivated/explained, so it could be moved to a later part. Overall, there are interesting technical contributions with positive results, but there are issues to be addressed.
"
iclr_2019_HkxKH2AcFm,"The paper argues for a GAN evaluation metric that needs sufficiently large number of generated samples to evaluate. Authors propose a metric based on existing set of divergences computed with neural net representations. R2 and R3 appreciate the motivation behind the proposed method and the discussion in the paper to that end. The proposed NND based metric has some limitations as pointed out by R2/R3 and also acknowledged by the authors -- being biased towards GANs learned with the same NND metric; challenge in choosing the capacity of the metric neural network; being computationally expensive, etc. However, these points are discussed well in the paper, and R2 and R3 are in favor of accepting the paper (with R3 bumping their score up after the author response). 
R1's main concern is the lack of rigorous theoretical analysis of the proposed metric, which the AC agrees with, but is willing to overlook, given that it is nontrivial and most existing evaluation metrics in the literature also lack this. 
Overall, this is a borderline paper but falling on the accept side according to the AC. "
iclr_2019_HkxLXnAcFQ,"This paper provides a number of interesting experiments for few-shot learning using the CUB and miniImagenet datasets. One of the especially intriguing experiments is the analysis of backbone depth in the architecture, as it relates to few-shot performance. The strong performance of the baseline and baseline++ are quite surprising. Overall the reviewers agree that this paper raises a number of questions about current few-shot learning approaches, especially how they relate to architecture and dataset characteristics.

A few minor comments:
- In table 1, matching nets are mistakenly attributed to Ravi and Larochelle. Should be Vinyals et al.
- The notation for cosine similarity in section 3.2 is odd. It looks like you’re computing some cosine function of two vectors which doesn’t make sense. Please clarify this.
- There are a few results that were promised after the revision deadline, please be sure to include these in the final draft.
"
iclr_2019_HkxStoC5F7,"The paper proposes a decision-theoretic framework for meta-learning. The ideas and analysis are interesting and well-motivated, and the experiments are thorough. The primary concerns of the reviewers have been addressed in new revisions of the paper. The reviewers all agree that the paper should be accepted. Hence, I recommend acceptance."
iclr_2019_HkxaFoC9KQ,"The paper presents a family of models for relational reasoning over structured representations. The experiments show good results in learning efficiency and generalization, in Box-World (grid world) and StarCraft 2 mini-games, trained through reinforcement (IMPALA/off-policy A2C).

The final version would benefit from more qualitative and/or quantitative details in the experimental section, as noted by all reviewers. 

The reviewers all agreed that this is worthy of publication at ICLR 2019. E.g. ""The paper clearly demonstrates the utility of relational inductive biases in reinforcement learning."" (R3)"
iclr_2019_HkxjYoCqKX,This paper proposes an effective method to train neural networks with quantized reduced precision. It's fairly straight-forward idea and achieved good results and solid empirical work. reviewers have a consensus on acceptance. 
iclr_2019_HkzRQhR9YX,"This paper presents a recurrent tree-structured linear dynamical system to model the dynamics of a complex nonlinear dynamical system. All reviewers agree that the paper is interesting and useful, and is likely to have an impact in the community. Some of the doubts that reviewers had were resolved after the rebuttal period. 

Overall, this is a good paper, and I recommend an acceptance."
iclr_2019_HkzSQhCcK7,"The paper presents a generative model of sequences based on the VAE framework, where the generative model is given by CNN with causal and dilated connections. 

Novelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the VAE framework. However, knowing how well this performs is valuable the community.

The proposed method appears to have significant benefits, as shown in experiments. The result on MNIST is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better."
iclr_2019_HyEtjoCqFX,"The paper proposes a new RL algorithm (MIRL) in the control-as-inference framework that learns a state-independent action prior.  A connection is provided to mutual information regularization.  Compared to entropic regularization, this approach is expected to work better when actions have significantly different importance.    The algorithm is shown to beat baselines in 11 out of 19 Atari games.

The paper is well written.  The derivation is novel, and the resulting algorithm is interesting and has good empirical results.  A few concerns were raised in initial reviews, including certain questions about experiments and potential negative impacts of the use of nonuniform action priors in MIRL.  The author responses and the new version were quite helpful, and all reviewers agree the paper is an interesting contribution.

In a revised version, the authors are encouraged to
  (1) include a discussion of when MIRL might fail, and
  (2) improve the related work section to compare the proposed method to other entropy regularized RL (sometimes under a different name in the literature), for example the following recent works and the references therein:
    https://arxiv.org/abs/1705.07798
    http://proceedings.mlr.press/v70/asadi17a.html
    http://papers.nips.cc/paper/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning
    http://proceedings.mlr.press/v80/dai18c.html"
iclr_2019_HyGBdo0qFm,"This paper provides a theoretical analysis of the Turing completeness of popular neural network architectures, specifically Neural Transformers and the Neural GPU. The reviewers agreed that this paper provides a meaningful theoretical contribution and should be accepted to the conference. Work of a theoretical nature is, amongst other types of work, called for by the ICLR CFP, but is not a very popular category for submissions, nor is it an easy one. As such, I am happy to follow the reviewers' recommendation and support this paper."
iclr_2019_HyGEM3C9KQ,"
pros:
- Identification of several interesting problems with the original DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links
- An improved architecture which addresses the issues and shows improved performance on synthetic memory tasks and bAbI over the original model
- Clear writing

cons:
- Does not really show this modified DNC can solve a task that the original DNC could not and the bAbI tasks are effectively solved anyway.  It is still not clear whether the DNC even with these improvements will have much impact beyond these toy tasks.

Overall the reviewers found this to be a solid paper with a useful analysis and I agree.  I recommend acceptance.

"
iclr_2019_HyGIdiRqtm,"
The paper investigates mixed-integer linear programming methods for neural net robustness verification in presence of adversarial attckas. The paper addresses and important problem, is well-written, presents a novel approach and demonstrates empirical improvements; all reviewers agree that this is a solid contribution to the field."
iclr_2019_HyGcghRct7,"This paper proposes a novel method of solving inverse problems that avoids direct inversion by  first reconstructing various piecewise-constant projections of the unknown image (using a different CNN to learn each)  and then combining them via optimization to solve the final inversion. 
Two of the reviewers requested more intuitions into why this two stage process would  fight the inherent ambiguity. 
At the end of the discussion, two of the three reviewers are convinced by the derivations and empirical justification of the paper.
The authors also have significantly improved the clarity of the manuscript throughout the discussion period.
It would be interesting to see if there are any connections between such inversion via optimization with deep component analysis methods, e.g. “Deep Component Analysis via Alternating Direction Neural Networks
” of Murdock et al. , that train neural architectures to effectively carry out the second step of optimization, as opposed to learning  a feedforward mapping. 
"
iclr_2019_HyGhN2A5tm,A paper that studies two tasks: machine translation and image translation. The authors propose a new multi-agent dual learning technique that takes advantage of the symmetry of the problem. The empirical gains over a competitive baseline are quite solid. The reviewers consistently liked the paper but have in some cases fairly low confidence in their assessment.
iclr_2019_HyM7AiA5YX,"This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as ""maximizing the complement entropy."" Rather than adding the cross-entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross-entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross-entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR-10, CIFAR-100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English-Vietnamese task), and small-vocabulary isolated-word recognition (Google Commands), show that the proposed two-objective approach outperforms training only to minimize cross-entropy. Experiments on CIFAR-10 also show that models trained in this framework have somewhat better resistance to single-step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem."
iclr_2019_HyN-M2Rctm,"The paper develops an original extension/generalization of standard batchnorm (and group norm) by employing a mixture-of-experts to separate incoming data into several modes and separately normalizing each mode. The paper is well written and technically correct, and the method yields consistent accuracy improvements over basic batchnorm on standard image classification tasks and models.
Reviewers and AC noted the following potential weaknesses: a) while large on artificially mixed data, improvements are relatively small on single standard datasets (<1% on CIFAR10 and CIFAR100)  b) the paper could better motivate why multi-modality is important e.g. by showing histograms of node activations c) the important interplay between number of modes and batch size should be more thoroughly discussed
d) the closely related approach of Kalayeh & Shah 2018 should be presented and contrasted with in more details in the paper. Also comparing to it in experiments would enrich the work.
"
iclr_2019_HyNA5iRcFQ,"This work examines how to craft adversarial examples that will lead trained seq2seq models to generate undesired outputs (here defined as, assigning higher-than-average probability to undesired outputs). Making a model safe for deployment is an important unsolved problem and this work is looking at it from an interesting angle, and all reviewers agree that the paper is clear, well-presented, and offering useful observations. While the paper does not provide ways to fix the problem of egregious outputs being probable, as pointed out by reviewers, it is still a valuable study of the behavior of trained models and an interesting way to ""probe"" them, that would likely be of high interest to many people at ICLR."
iclr_2019_Hye9lnCct7,"To borrow the succinct summary from R1, ""the paper suggests a method for generating representations that are linked to goals  in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the 
policies leading to them are similar."" The reviewers and AC agree that this is a novel and worthy idea.

Concerns about the paper are primarily about the following.
(i) the method already requires good solutions as input, i.e., in the form of goal-conditioned policies, (GCPs)
and the paper claims that these are easy to learn in any case.
As R3 notes, this then begs the question as to why the actionable representations are needed.
(ii) reviewers had questions regarding the evaluations, i.e., fairness of baselines, additional comparisons, and 
additional detail. 

After much discussion, there is now a fair degree of consensus.  While R1 (the low score) still has a remaining issue with evaluation, particularly hyperparameter evaluation, they are also ok with acceptance. The AC is of the opinion that hyperparameter tuning is of course an important issue, but does not see it as the key issue for this particular paper. 
The AC is of the opinion that the key issue is issue (i), raised by R3. In the discussion, the authors reconcile the inherent contradiction in (i) based on the need of additional downstream tasks that can then benefit from the actionable representation, and as demonstrated in a number of the evaluation examples (at least in the revised version). The AC believes in this logic, but believes that this should be stated more clearly in the final paper. And it should be explained
the extent to which training for auxiliary tasks implicitly solve this problem in any case.

The AC also suggests nominating R3 for a best-reviewer award."
iclr_2019_HyeFAsRctQ,"This paper proposes verification algorithms for a class of convex-relaxable specifications to evaluate the robustness of neural networks under adversarial examples.

The reviewers were unanimous in their vote to accept the paper. Note: the remaining score of 5 belongs to a reviewer who agreed to acceptance in the discussion."
iclr_2019_HyeGBj09Fm,"This paper presents a novel method for synthesizing fluid simulations, constrained to a set of parameterized variations,
such as the size and position of a water ball that is dropped. The results are solid; there is little related
work to compare to, in terms of methods that can ""compute""/recall simulations at that speed.
The method is 2000x faster than the orginal simulations. This comes with the caveats that: 
(a) the results are specific to the given set of parameterized environments; the method is learning a 
compressed version of the original animations; (b) there is a loss of accuracy, and therefore
also a loss of visual plausibility.

The AC notes that the paper should use the ICLR format for citations, i.e., ""(foo et al.)"" rather than ""(19)"".
The AC also suggests that limitations should also be clearly documented, i.e., as seen from the 
perspective of those working in the fluid simulation domain.

The principle (and only?) contentious issue relates to the suitability of the paper for the ICLR audience,
given its focus on the specific domain of fluid simulations.  The AC is of two minds on this:
(i) the fluid simulation domain has different characteristics to other domains, and thu
understanding the ICLR audience can benefit from the specific nature of the predictive problems that
come the fluid simulation domain;  new problems can drive new methods.  There is a loose connection
between the given work and residual nets, and of course res-nets have also been recently reconceptualized as PDEs.
(ii) it's not clear how much the ICLR audience will get out of the specific solutions being described;
it requires understanding spatial transformer networks and a number of other domain-specific issues.
A problem with this type of paper in terms of graphics/SIGGRAPH is that it can also be seen as ""falling short""
there, simply because it is not yet competitive in terms of visual quality or the generality of
fluid simulators;  it really fulfills a different niche than classical fluid simulators.

The AC leans slightly in favor of acceptance, but is otherwise on the fence.

"
iclr_2019_HyePrhR5KX,"After discussion, all reviewers agree to accept this paper. Congratulations!!"
iclr_2019_HyeVtoRqtQ,"The paper proposes a novel network architecture for sequential learning, called trellis networks, which generalizes truncated RNNs and also links them to temporal convnets. The advantages of both types of nets are used to design trellis networks which appear to outperform state of art on several datasets.  The paper is well-written and the results are convincing."
iclr_2019_HyexAiA5Fm,"After revision, all reviewers agree that this paper makes an interesting contribution to ICLR by proposing a new methodology for unbalanced optimal transport using GANs and should be accepted."
iclr_2019_Hyfn2jCcKm,"The paper introduces a version of approximate policy iteration (API), called Autodidactic Iteration (ADI), designed to overcome the problem of sparse rewards.  In particular, the policy evaluation step of ADI is trained on a distribution of states that allows the reward to easily propagate from the goal state to states farther away.  ADI is applied to successfully solve the Rubik's Cube (together with other existing techniques).

This work is an interesting contribution where the ADI idea may be useful in other scenarios.  A limitation is that the whole empirical study is on the Rubik's Cube; a controlled experiment on other problems (even if simpler) can be useful to understand the pros & cons of ADI compared to others.

Minor: please update the bib entry of Bottou (2011).  It's now published in MLJ 2014."
iclr_2019_Hyg1G2AqtQ,"This paper proposes an input-dependent baseline function to reduce variance in policy gradient estimation without adding bias. The approach is novel and theoretically validated, and the experimental results are convincing. The authors addressed nearly all of the reviewer's concerns. I recommend acceptance."
iclr_2019_HygQBn0cYm,"Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"
iclr_2019_Hyg_X2C5FX,"The paper proposes an interesting framework for visualizing and understanding GANs, that will be of clear help for understanding existing models and might provide insights for developing new ones. "
iclr_2019_HygjqjR9Km,"The submission proposes two new things: a repulsive loss for MMD loss optimization and a bounded RBF kernel that stabilizes training of MMD-GAN. The submission has a number of unsupervised image modeling experiments on standard benchmarks and shows reasonable performance. All in all, this is an interesting piece of work that has a number of interesting ideas (e.g. the PICO method, which is useful to know). I agree with R2 that the RBF kernel seems somewhat hacky in its introduction, despite working well in practice.

That being said, the repulsive loss seems like something the research community would benefit from finding out more about, and I think the experiments and discussion are sufficiently extensive to warrant publication."
iclr_2019_Hygn2o0qKX,"Existing PAC Bayes analysis gives generalization bounds for stochastic networks/classifiers. This paper develops a new approach to obtain generalization bounds for the original network, by generalizing noise resilience property from training data to test data.  All reviewers agree that the techniques  developed in the paper (namely Theorem 3.1) are novel and interesting.  There was disagreement between reviewers on the usefulness of the new generalization bound (Theorem 4.1) shown in this paper using the above techniques. I believe authors have sufficiently addressed these concerns in their response and updated draft. Hence, despite the concerns of R3 on limitations of this bound and its dependence on pre-activation values, I agree with R2 and R4 that the techniques developed in the paper are of interest to the community and deserve publication. I suggest authors to keep comments of R3 in mind while preparing the final version. "
iclr_2019_HygsfnR9Ym,"The paper presents ""recall traces"", a model based approach designed to improve reinforcement learning in sparse reward settings. The approach learns a generative model of trajectories leading to high-reward states, and is subsequently used to augment the real experience collected by the agent. This novel take on combining model-based and model-free learning is conceptually well motivated and is empirically shown to improve sample efficiency on several benchmark tasks.

The reviewers noted the following potential weaknesses in their initial reviews: the paper could provide a clearer motivation of why the proposed approach is expected to lead to performance improvements, and how it relates to learning (and uses of) a forward model. Details of the method, e.g., model parameterization is unclear, and the effect of hyperparameter choices is not fully evaluated.

The authors provided detailed replies to all reviewer suggestions, and ran extensive new experiments, including experiments to address questions about hyperparameter settings, and an entirely new use of the proposed model in a learning from demonstration setting. The authors also clarified the paper as requested by the reviewers. The reviewers have not responded to the rebuttal, but in the AC's assessment their concerns have been adequately addressed. The reviewers have updated their scores in response to the rebuttal, and the consensus is to accept the paper.

The AC notes that the authors seem unaware of related work by Oh et al. ""Self Imitation Learning"" which was published at ICML 2018. The paper is based on a similar conceptual motivation but imitates high-value traces directly, instead of using a generative model. The authors should include a discussion of how their paper relates to this earlier work in their camera ready version."
iclr_2019_Hygxb2CqKm,The paper presents both theoretical analysis (based upon lambda-stability) and experimental evidence on stability of recurrent neural networks. The results are convincing but is concerns with a restricted definition of stability. Even with this restriction acceptance is recommended. 
iclr_2019_HylTBhA5tQ,"Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"
iclr_2019_HylTXn0qYX,"This paper proposes a new method for verifying whether a given point of a two layer ReLU network is a local minima or a second order stationary point and checks for descent directions. All reviewers agree that the algorithm is based on number of new techniques involving both convex and non-convex QPs, and is novel. The method proposed in the paper has significant limitations as the method is not robust to handle approximate stationary points. Given these limitations, there is a disagreement between reviewers about the significance of the result . While I share the same concerns as R4, I agree with R3 and believe that the new ideas in the paper will inspire future work to extend the proposed method towards addressing these limitations. Hence I suggest acceptance. "
iclr_2019_HylVB3AqYm,"This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance.
"
iclr_2019_Hyl_vjC5KQ,"This paper proposes a method for hierarchical reinforcement learning that aims to maximize mutual information between options and state-action pairs. The approach and empirical analysis is interesting. The initial submission had many issues with clarity. However, the new revisions of the paper have significantly improved the clarity, better describing the idea and improving the terminology. The main remaining weakness is the scope of the experimental results.
However, the reviewers agree that the paper exceeds the bar for publication at ICLR with the existing experiments."
iclr_2019_Hyx4knR9Ym,"Adversarial training has quickly become important for training robust neural networks.  However this training generally results in poor generalization behavior. This paper proposes using margin loss with adversarial training for better generalization. The paper provides generalization bounds for this adversarial training setup motivating the use of spectral regularization. The experimental results using the spectral regularization with adversarial training are very promising and all the reviewers agree that they show non-trivial improvement. Even though the spectral regularization techniques have been tried in different settings, hence of limited novelty, the experimental results in the paper are encouraging and I believe will motivate further study on this topic. Reviewers also opined that the writing in the paper is currently not that great with limited explanation of the theoretical results. More discussions interpreting the theoretical results and their significance can help the readers appreciate the paper better."
iclr_2019_Hyx6Bi0qYm,"BMIs need per-patient and per-session calibration, and this paper seeks to amend that.  Using VAEs and RNNs, it relates sEEG to sEMG, in principle a ten-year old approach, but do so using a novel adversarial approach that seems to work.

The reviewers agree the approach is nice, the statements in the paper are too strong, but publication is recommended.  Clinical evaluation is an important next step."
iclr_2019_HyxAfnA5tm,"The reviewers appreciated this contribution, particularly its ability to tackle nonstationary domains which are common in real-world tasks. 

"
iclr_2019_HyxCxhRcY7,"The paper proposes a new fine-tuning method for improving the performance of existing anomaly detectors.

The reviewers and AC note the limitation of novelty beyond existing literature.

This is quite a borader line paper, but AC decided to recommend acceptance as comprehensive experimental results (still based on empirical observation though) are interesting."
iclr_2019_HyxGB2AcY7,"The paper addresses the challenging and important problem of exploration in sparse-rewards settings. The authors propose a novel use of contingency awareness, i.e., the agent's understanding of the environment features that are under its direct control, in combination with a count-based approach to exploration. The model is trained using an inverse dynamics model and attention mechanism and is shown to be able to identify the controllable character. The resulting exploration approach achieves strong empirical results compared to alternative count-based exploration techniques. The reviewers note that the novel approach has potential for opening up potential fruitful directions for follow-up research. The obtained strong empirical results are another strong indication of the value of the proposed idea.


The reviewers mention several potential weaknesses. First, while the proposed idea is general, the specific implementation seems targetted specifically towards Atari games. While Atari is a popular benchmark domain, this raises questions as to whether insights can be more generally applied. Second, several questions were raised regarding the motivation for some of the presented modeling choices (e.g., loss terms) as well as their impact on the empirical results. Ablation studies were recommended as a step to resolving these questions Reviewer 3 questioned whether the learned state representation could be directly used as an additional input to the agent, and if it would improve performance. Finally, several related works were suggested that should be included in the discussion of related work.

The authors carefully addressed the issues raised by the reviewers, running additional comparisons and adding to the original empirical insights. Several issues of clarity were resolved in the paper and in the discussion. Reviewer 3 engaged with the authors and confirmed that they are satisfied with the resulting submission. The AC judges that the suggestions of reviewer 1 have been addressed to a satisfactory level. A remaining issue regarding results reporting was raised anonymously towards the end of the review period, and the AC encourages the authors to address this issue in their camera ready version."
iclr_2019_HyxKIiAqYQ,"This paper proposes an algorithm for end-to-end image compression outperforming previously proposed ANN-based techniques and typical image compression standards like JPEG.

Strengths
- All reviewers agreed that this a well written paper, with careful analysis and results.

Weaknesses
- One of the points raised during the review process was that 2 very recent publications propose very similar algorithms. Since these works appeared very close to ICLR paper submission deadline (within 30 days), the program committee decided to treat this as concurrent work.

The authors also clarified the differences and similarities with prior work, and included additional experiments to clarify some of the concerns raised during the review process. Overall the paper is a solid contribution towards improving image compression, and is therefore recommended to be accepted.
"
iclr_2019_HyxPx3R9tm,"The paper proposes a simple and general technique based on the information bottleneck to constrain the information flow in the discriminator of adversarial models. It helps to train by maintaining informative gradients. While the information bottleneck is not novel, its application in adversarial learning to my knowledge is, and the empirical evaluation demonstrates impressive performance on a broad range of applications. Therefore, the paper should clearly be accepted.
"
iclr_2019_HyxnZh0ct7,"The reviewers disagree strongly on this paper. Reviewer 2 was the most positive, believing it to be an interesting contribution with strong results. Reviewer 3 however, was underwhelmed by the results. Reviewer 1 does not believe that the contribution is sufficiently novel, seeing it as too close to existing multi-task learning approaches.

After considering all of the discussion so far, I have to agree with reviewer 2 on their assessment. Much of the meta learning literature involves changing the base learner *for a fixed architecture* and seeing how it affects performance. There is a temptation to chase performance by changing the architecture, adding new regularizers, etc., and while this is important for practical reasons, it does not help to shed light on the underlying fundamentals. This is best done by considering carefully controlled and well understood experimental settings. Even still, the performance is quite good relative to popular base learners.

Regarding novelty, I agree it is a simple change to the base learner, using a technique that has been tried before in other settings (linear regression as opposed to classification), however its use in a meta learning setup is novel in my opinion, and the new experimental comparison regression on top of pre-trained CNN features helps to demonstrate the utility of its use in the meta-learning settings.

While the novelty can certainly be debated, I want to highlight two reasons why I am opting to accept this paper: 1) simple and effective ideas are often some of the most impactful. 2) sometimes taking ideas from one area (e.g., multi-task learning) and demonstrating that they can be effective in other settings (e.g., meta-learning) can itself be a valuable contribution. I believe that the meta-learning community would benefit from reading this paper.
"
iclr_2019_HyxzRsR9Y7,"This paper proposes a reinforcement learning approach that better handles sparse reward environments, by using previously-experienced roll-outs that achieve high reward. The approach is intuitive, and the results in the paper are convincing. The authors addressed nearly all of the reviewer's concerns. The reviewers all agree that the paper should be accepted."
iclr_2019_HyzMyhCcK7,"A novel  approach for quantized deep neural nets is proposed,  which is more principled than commonly used  straight-through gradient method. A theoretical analysis of the algorithm's converegence  is presented, and empirical results show advantages of the proposed approach. "
iclr_2019_HyzdRiR9Y7,"This paper presents Universal Transformers that generalizes Transformers with recurrent connections. The goal of Universal Transformers is to combine the strength of feed-forward convolutional architectures (parallelizability and global receptive fields) with the strength of recurrent neural networks (sequential inductive bias). In addition, the paper investigates a dynamic halting scheme (by adapting Adaptive Computation Time (ACT) of Graves 2016) to allow each individual subsequence to stop recurrent computation dynamically.

Pros: 
The paper presents a new generalized architecture that brings a reasonable novelty over the previous Transformers when combined with the dynamic halting scheme. Empirical results are reasonably comprehensive and the codebase is publicly available.

Cons:
Unlike RNNs, the network recurs T times over the entire sequence of length M, thus it is not a literal combination of Transformers with RNNs, but only inspired by RNNs. Thus the proposed architecture does not precisely replicate the sequential inductive bias of RNNs. Furthermore, depending on how one views it, the network architecture is not entirely novel in that it is reminiscent of the previous memory network extensions with multi-hop reasoning (--- a point raised by R1 and R2). While several datasets are covered in the empirical study, the selected datasets may be biased toward simpler/easier tasks (--- R1). 

Verdict:
While key ideas might not be entirely novel (R1/R2), the novelty comes from the fact that these ideas have not been combined and experimented in this exact form of Universal Transformers (with optional dynamic halting/ACT), and that the empirical results are reasonably broad and strong, while not entirely impressive (R1). Sufficient novelty and substance overall, and no issues that are dealbreakers. "
iclr_2019_HyztsoC5Y7,"The authors consider the use of MAML with model based RL and applied this to robotics tasks with very encouraging results. There was definite interest in the paper, but also some concerns over how the results were situated, particularly with respect to the related research in the robotics community. The authors are strongly encouraged to carefully consider this feedback, as they have been doing in their responses, and address this as well as possible in the final version.
"
iclr_2019_S1E3Ko09F7,"The paper presents two new methods for model-agnostic interpretation of instance-wise feature importance. 

Pros:
Unlike previous approaches based on the Shapley value, which had an exponential complexity in the number of features, the proposed methods have a linear-complexity when the data have a graph structure, which allows approximation based on graph-structured factorization. The proposed methods present solid technical novelty to study the important challenge of instance-wise, model-agnostic, linear-complexity interpretation of features. 

Cons:
All reviewers wanted to see more extensive experimental results. Authors responded with most experiments requested. One issue raised by R3 was the need for comparing the proposed model-agnostic methods to existing model-specific methods. The proposed linear-complexity algorithm relies on the markov assumption, which some reviewers commented to be a potentially invalid assumption to make, but this does not seem to be a deal breaker since it is a relatively common assumption to make when deriving a polynomial-complexity approximation algorithm. Overall, the rebuttal addressed the reviewers' concerns well enough, leading to increased scores.

Verdict:
Accept. Solid technical novelty with convincing empirical results."
iclr_2019_S1EERs09YQ,"Important problem (making NN more transparent); reasonable approach for identifying which linguistic concepts different neurons are sensitive to; rigorous experiments. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance."
iclr_2019_S1EHOsC9tX,"The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. 

Strengths:

- The resulting model offers good robustness guarantees for a wide range of norm-bounded perturbations

- The authors put a lot of care into the robustness evaluation

Weaknesses: 

- Some of the ""shortcomings"" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about

Overall, this looks like a valuable and interesting contribution.
"
iclr_2019_S1GkToR5tm,"The paper proposes a discriminator dependent rejection sampling scheme for improving the quality of samples from a trained GAN. The paper is clearly written, presents an interesting idea and the authors extended and improved the experimental analyses as suggested by the reviewers."
iclr_2019_S1M6Z2Cctm,"The proposed method introduces a method for unsupervised image-to-image mapping, using a new term into the objective function that enforces consistency in similarity between image patches across domains. Reviewers left constructive and detailed comments, which, the authors have made substantial efforts to address.

Reviewers have ranked paper as borderline, and in Area Chair's opinion, most major issued have been addressed:

- R3&R2: Novelty compared to DistanceGAN/CRF limited: authors have clarified contributions in reference to DistanceGAN/CRF and demonstrated improved performance relative to several datasets.
- R3&R1: Evaluation on additional datasets required: authors added evaluation on 4 more tasks
- R3&R1: Details missing: authors added details. 

"
iclr_2019_S1VWjiRcKX,"This paper addresses an importnant and more realistic setting of multi-task RL where the reward function changes; the approach is elegant, and empirical results are convincing. The paper presents an importnant contribution to the challenging multi-task RL problem."
iclr_2019_S1eK3i09YQ,"This paper proves that gradient descent with random initialization converges to global minima for a squared loss penalty over a two layer ReLU network and arbitrarily labeled data. The paper has several weakness such as, 1) assuming top layer is fixed, 2) large number of hidden units 'm', 3) analysis is for squared loss. Despite these weaknesses the paper makes a novel contribution to a relatively challenging problem, and is able to show convergence results without strong assumptions on the input data or the model. Reviewers find the results mostly interesting and have some concerns about the \lambda_0 requirement. I believe the authors have sufficiently addressed this issue in their response and  I suggest acceptance. "
iclr_2019_S1eOHo09KX,"This paper presents a reinforcement learning approach for online cost-aware feature acquisition. The utility of each feature is measured in terms of expected variations of the model uncertainty (using MC dropout sampling as an estimate of certainty) which is subsequently used as a reward function in the reinforcement learning formulation. The empirical evaluations show improvements over prior approaches in terms of accuracy-cost trade-off on three datasets. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.
 
Initially, R1 and R2 raised important concerns regarding low technical novelty. R1 requested an ablation study to understand which of the following components gives the most improvement: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Pleased to report that the authors addressed the ablation study in their rebuttal and confirmed that MC-dropout certainty plays a crucial rule in the performance of the proposed method. R1 subsequently increased the assigned score to 6. R2 raised concerns about related prior work Contardo et al 2016, which similarly evaluates the most informative features given budget constraints with a recurrent neural network approach. After a long discussion and a detailed rebuttal, R2 upgraded the rating from below the threshold to 7, albeit acknowledging an incremental technical contribution. R3 raised important concerns regarding presentation clarity that were subsequently addressed by the authors. In conclusion, all three reviewers were convinced by the authors rebuttal and have upgraded their initial rating, and AC recommends acceptance of this paper – congratulations to the authors!
"
iclr_2019_S1eYHoC5FX,"This paper introduces a very simple but effective method for the neural architecture search problem. The key idea of the method is a particular continuous relaxation of the architecture representation to enable gradient descent-like differentiable optimization. Results are quite good. Source code is also available. A concern of the approach is the (possibly large) integrality gap between the continuous solution and the discretized architecture. The solution provided in the paper is a heuristic without guarantees.  Overall, this is a good paper. I recommend acceptance."
iclr_2019_S1ecm2C9K7,"The authors identify a source of bias that occurs when a model overestimates the importance of weak features in the regime where sufficient training data is not available. The bias is characterized theoretically, and demonstrated on synthetic and real datasets. The authors then present two algorithms to mitigate this bias, and demonstrate that they are effective in experimental evaluations.  
As noted by the reviewers, the work is well-motivated and clearly presented. Given the generally positive reviews, the AC recommends that the work be accepted. The authors should consider adding additional text describing the details concerning Figure 3 in the appendix.
"
iclr_2019_S1erHoR5t7,"All authors agree that the relativistic discriminator is an interesting idea, and a useful proposal to improve the stability and sample quality of GANs. In earlier drafts there were some clarity issues and missing details, but those have been fixed to the satisfaction of the reviewers. Both R1 and R3 expressed a desire for a more theoretical justification of why the relativistic discriminator should work better, but the empirical results are strong enough that this can be left for future work."
iclr_2019_S1fQSiCcYm,The reviewers have reached a consensus that this paper is very interesting and add insights into interpolation in autoencoders.
iclr_2019_S1fUpoR5FQ,"This paper presents quasi-hyperbolic momentum, a generalization of Nesterov Accelerated Gradient. The method can be seen as adding an additional hyperparameter to NAG corresponding to the weighting of the direct gradient term in the update. The contribution is pretty simple, but the paper has good discussion of the relationships with other momentum methods, careful theoretical analysis, and fairly strong experimental results. All the reviewers believe this is a strong paper and should be accepted, and I concur.
"
iclr_2019_S1g2JnRcFX,"This paper analyzes local SGD optimization for strongly convex functions, and proves that local SGD enjoys a linear speedup (in the number of workers and minibatch size) over vanilla SGD, while also communicating less than distributed mini-batch SGD. A similar analysis is also provided for the asynchronous case, and limited empirical confirmation of the theory is provided. The main weakness of the current revision is that it does not yet properly relate this work to two prior publications: Dekel et al., 2012 (https://arxiv.org/pdf/1012.1367.pdf) and Jain et al., 2016 (https://arxiv.org/abs/1610.03774). It is critical that these references and suitable discussion be added in the camera-ready paper, since this issue was the subject of considerable discussion and the authors promised to include the references and discussion in the final paper."
iclr_2019_S1gOpsCctm,"The paper addresses the problem of interpreting recurrent neural networks by quantizing their states an mapping them onto a Moore Machine. The paper presents some interesting results on reinforcement learning and other tasks. I believe the experiments could have been more informative if the proposed technique was compared against a simple quantization baseline (e.g. based on k-means) so that one can get a better understanding of the difficulty of these task.

This paper is clearly above the acceptance threshold at ICLR. "
iclr_2019_S1gUsoR9YX,This paper presents good empirical results on an important and interesting task (translation between several language pairs with a single model). There was solid communication between the authors and the reviewers leading to an improved updated version and consensus among the reviewers about the merits of the paper.
iclr_2019_S1lDV3RcKm,"The paper proposes an adversarial framework that learns a generative model along with a mask generator to model missing data and by this enables a GAN to learn from incomplete data.
The method builds on AmbientGAN but it is a novel and clever adjustment to the specific problem setting of learning from incomplete data, that is of high practical interest."
iclr_2019_S1lIMn05F7,"The paper proposed a GAN approach to robust learning against adversarial examples, where a generator produces adversarial examples as perturbations and a discriminator is used to distinguish between adversarial and raw images. The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task. 

The architecture of GANs used in the paper is standard, yet the defensive performance seems good. The reviewers wonder the reason behind this good mechanism and the novelty compared with other works in similar spirits. In response, the authors add some insights on discussing the mechanism as well as comparisons with other works mentioned by the reviewers. 

The reviewers all think that the paper presents a simple scheme for robust deep learning based on GANs, which shows its effectiveness in experiments. The understanding on why it works may need further explorations.  Thus the paper is proposed to be borderline lean accept. 

"
iclr_2019_S1lTEh09FQ,The paper provides a novel attack method and contributes to evaluating the robustness of neural networks with recently proposed defenses. The evaluation is convincing overall and the authors have answered most questions from the reviewers. We recommend acceptance. 
iclr_2019_S1lTg3RqYQ,"This paper proposes an image to image translation technique which decomposes into style and content transfer using a semantic consistency loss to encourage corresponding semantics (using feature masks) before and after translation. Performance is evaluated on a set of MNIST variants as well as from simulated to real world driving imagery. 

All reviewers found this paper well written with clear contribution compared to related work by focusing on the problem when one-to-one mappings are not available across two domains which also have multimodal content or sub-style. 

The main weakness as discussed by the reviewers relates to the experiments and whether or not the set provided does effectively validate the proposed approach. The authors argue their use of MNIST as a toy problem but with full control to clearly validate their approach. Their semantic segmentation experiment shows modest performance improvement. Based on the experiments as is and the relative novelty of the proposed approach, the AC recommends poster and encourages the authors to extend their analysis of the current results in a final version."
iclr_2019_S1lg0jAcYm,"This paper introduces a new way to estimate gradients of expectations of discrete random variables by introducing antithetic noise samples for use in a control variate.

Quality:  The experiments are mostly appropriate, although I disagree with the choice to present validation and test-set results instead of training-time results.  If the goal of the method is to reduce variance, then checking whether optimization is improved (training loss) is the most direct measure.  However reasonable people can disagree about this.

I also think the toy experiment (copied from the REBAR and RELAX paper) is a bit too easy for this method, since it relies on taking two antithetic samples.  I would have liked to see a categorical extension of the same experiment.

Clarity:  I think that this method will not have the impact it otherwise could because of the authors' fearless use of long equations and heavy notation throughout.  This is unavoidable to some degree, but
1) The title of the paper isn't very descriptive
2) Why not follow previous work and use \theta instead of \phi for the parameters being optimized?
The presentation has come a long way, but I fear that few besides our intrepid reviewers will have the stomach.  I recommend providing more intuition throughout.

Originality:  The use of antithetic samples to reduce variance is old, but this seems like a well-thought-through and non-trivial application of the idea to this setting.

Significance:  Ultimately I think this is a new direction in gradient estimators for discrete RVs.  I don't think this is the last word in this direction but it's both an empirical improvement, and will inspire further work."
iclr_2019_S1lhbnRqF7,"This paper investigates a new approach to machine reading for procedural text, where the task of reading comprehension is formulated as dynamic construction of a procedural knowledge graph. The proposed model constructs a recurrent knowledge graph (as a bipartite graph between entities and location nodes) and tracks the entity states for two domains: scientific processes and recipes.

Pros:
The idea of formulating reading comprehension as dynamic construction of a knowledge graph is novel and interesting. The proposed model is tested on two different domains: scientific processes (ProPara) and cooking recipes.

Cons:
The initial submission didn't have the experimental results on the full recipe dataset and also had several clarity issues, all of which have been resolved through the rebuttal. 

Verdict:
Accept. An interesting task & models with solid empirical results.
"
iclr_2019_S1lqMn05Ym,"Strengths

The paper introduces a promising and novel idea, i.e., regularizing RL via an informationally asymmetric default policy 
The paper is well written.  It has solid and extensive experimental results.

Weaknesses


There is a lack of benefit on dense-reward problems as a limitation, which the authors further
acknowledge as a limitation. There also some similarities to HRL approaches. 
A lack of theoretical results is also suggested. To be fair, the paper makes a number of connections
with various bits of theory, although it perhaps does not directly result in any new theoretical analysis.
A concern of one reviewer is the need for extensive compute, and making comparisons to stronger (maxent) baselines.
The authors provide a convincing reply on these issues.

Points of Contention

While the scores are non-uniform (7,7,5), the most critical review, R1(5), is in fact quite positive on many
aspects of the paper, i.e., ""this paper would have good impact in coming up with new 
learning algorithms which are inspired from cognitive science literature as well as mathematically grounded.""
The specific critiques of R1 were covered in detail by the authors.

Overall

The paper presents a novel and fairly intuitive idea, with very solid experimental results.  
While the methods has theoretical results, the results themselves are more experimental than theoretic.
The reviewers are largely enthused about the paper.  The AC recommends acceptance as a poster.
"
iclr_2019_S1lvm305YQ,"Strengths: This paper is ""thorough and well written"", exploring the timbre transfer problem in a novel way. There is a video accompanying the work and some reviewers assessed the quality of the results as being good relative to other approaches. Two of the reviewers were quite positive about the work.

Weaknesses: Reviewer 2 (the lowest scoring reviewer) felt that the paper was a little too far from solving the problem to be of high significance and that there was:
 - too much focus on STFT vs. CQT
 - too little focus on getting WaveNet synthesis right
 - too limited experimental validation (too restricted choice of instruments)
 - poor resulting audio quality
 - feels too much of combining black boxes

AMT listening tests were performed, but better baselines could have been used.
The author response addressed some of these points.

Contention: 
An anonymous commenter noted that the revised manuscript added some names in the acknowledgements, thereby violating double blind review guidelines. However, the aggregated initial scores for this work were past the threshold for acceptance. Reviewer 2 was the most critical of the work but did not engage in dialog or comment on the author response. 

Consensus:
The two positive reviewers felt that this work is worth of presentation at ICLR. The AC recommends accept as poster unless the PC feel the issue of names in the Acknowledgements in an updated draft is too serious of an issue.
 "
iclr_2019_S1x2Fj0qKQ,"The paper addresses normalisation and conditioning of GANs. The authors propose to replace class-conditional batch norm with whitening and class-conditional coloring. Evaluation demonstrates that the method performs very well, and the ablation studies confirm the design choices. After extensive discussion, all reviewers agreed that this is a solid contribution, and the paper should be accepted. "
iclr_2019_S1xLN3C9YX,"The authors propose a method to learn a neural network architecture which achieves the same accuracy as a reference network, with fewer parameters through Bayesian Optimization. The search is carried out on embeddings of the neural network architecture using a train bi-directional LSTM. The reviewers generally found the work to be clearly written, and well motivated, with thorough experimentation, particularly in the revised version. Given the generally positive reviews from the authors, the AC recommends that the paper be accepted.
"
iclr_2019_S1xNEhR9KX,"This paper studies an interesting phenomenon related to adversarial training -- that adversarial robustness is quite sensitive to semantically lossless shifts in input data distribution. 

Strengths
- Characterizes a previously unobserved phenomenon in adversarial training, which is quite relevant to ongoing research in the area.
- Interesting and novel theoretical analysis that motivates the relationship between adversarial robustness and the shape of input distribution.

Weaknesses
- Reviewers pointed out some shortcomings in experiments, and analysis of causes and remedies to adversarial robustness. The authors agree that given the current state of understanding, these are hard questions to pose good answers for. The result and observations by themselves are interesting and useful for the community.

The weakness that the paper does not propose a solution for the observed phenomenon remains, but all reviewers agree that the observation in itself is interesting. Therefore, I recommend that the paper be accepted.
"
iclr_2019_S1xNb2A9YX,"This paper characterizes a particular kind of fragility in the image classification ability of deep networks: minimal image regions which are classified correctly, but for which neighboring regions shifted by one row or column of pixels are classified incorrectly. Comparisons are made to human vision. All three reviewers recommend acceptance. AnonReviewer1 places the paper marginally above threshold, due to limited originality over Ullman et al. 2016, and concerns about overall significance.
"
iclr_2019_S1xcx3C5FX,"* Strengths

The paper addresses an important topic: how to bound the probability that a given “bad” event occurs for a neural network under some distribution of inputs. This could be relevant, for instance, in autonomous robotics settings where there is some environment model and we would like to bound the probability of an adverse outcome (e.g. for an autonomous aircraft, the time to crash under a given turbulence model). The desired failure probabilities are often low enough that direct Monte Carlo simulation is too expensive. The present work provides some preliminary but meaningful progress towards better methods of estimating such low-probability events, and provides some evidence that the methods can scale up to larger networks. It is well-written and of high technical quality.

* Weaknesses

In the initial submission, one reviewer was concerned that the term “verification” was misleading, as the methods had no formal guarantees that the estimated probability was correct. The authors proposed to revise the paper to remove reference to verification in the title and the text, and afterwards all reviewers agreed the work should be accepted. The paper also may slightly overstate the generality of the method. For instance, the claim that this can be used to show that adversarial examples do not exist is probably wrong---adversarial examples often occupy a negligibly small portion of the input space. There was also concern that most comparisons were limited to naive Monte Carlo.

* Discussion

While there was initial disagreement among reviewers, after the discussion all reviewers agree the paper should be accepted. However, we remind the authors to implement the changes promised during the discussion period."
iclr_2019_S1xtAjR5tX,"The paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences. Strong empirical results are presented which support the use of optimal transport in conjunction with log-likelihood for training sequence models. I appreciate the improvements to the manuscript during the review process, and I encourage the authors to address the rest of the comments in the final version."
iclr_2019_S1zk9iRqF7,"This paper improves upon the PATE-GAN framework for differentially-private synthetic data generation. They eliminate the need for public data samples for training the GAN, by providing a distribution which can be sampled from instead.

The authors were unanimous in their vote to accept."
iclr_2019_S1zz2i0cY7,"This paper addresses the issue of numerical rounding-off errors that can arise when using latent variable models for data compression,  e.g., because of differences in floating point arithmetic across different platforms (sender and receiver). The authors propose using neural networks that perform integer arithmetic (integer networks) to mitigate this issue. The problem statement is well described, and the presentation is generally OK, although it could be improved in certain aspects as pointed out by the reviewers. The experiments are properly carried out, and the experimental results are good.
Thank you for addressing the questions raised by the reviewers. After taking into account the author's responds, there is consensus that the paper is worthy of publication. I therefore recommend acceptance. "
iclr_2019_SJG6G2RqtX,"
Interesting idea, reviewers were positive and indicated presentation should be improved.
"
iclr_2019_SJGvns0qK7,"The paper proposed a deep, Bayesian optimization approach to RL with model uncertainty (BAMDP).  The algorithm is a variant of policy gradient, which in each iteration uses a Bayes filter on sampled MDPs to update the posterior belief distribution of the parameters.  An extension is also made to POMDPs.

The work is a combination of existing techniques, and the algorithmic novelty is a bit low.  Initial reviews suggested the empirical study could be improved with better baselines, and the main idea of the proposed method could be expended.  The revised version moves towards this direction, and the author responses were helpful.  Overall, the paper is a useful contribution."
iclr_2019_SJVmjjR9FX,"The reviewers lean to accept, and the authors clearly put a significant amount of time into their response. I will also lean to accept. However, the comments of reviewer 2 should be taken seriously, and addressed if possible, including an attempt to cut the paper length down."
iclr_2019_SJe3HiC5KX,"This paper proposes a new approach to domain adaptation based on sub-spacing, such that outliers are filtered out. While similar ideas have been used e.g. in multi-view learning, their application to domain adaptation makes it a novel and interesting approach. 

While the above is considered by the AC an adequate contribution to ICLR, the authors are encouraged to investigate further the implications of the assumptions made, in a way that the derived criteria seem less heuristic, as R1 pointed out.

There had been some concerns regarding the experiments, but the authors have been very active in the rebuttal period and addressed these concerns satisfactorily.
"
iclr_2019_SJe9rh0cFX,"This paper addresses a well motivated problem and provides new insight on the theoretical analysis of representational power in quantized networks. The results contribute towards a better understanding of quantized networks in a way that has not been treated in the past. 

The most moderate rating (marginally above acceptance threshold) explains that while the paper is technically quite simple, it gives an interesting study and blends well into recent literature on an important topic. 

A criticism is that the approach uses modules to approximate the basic operations of non quantized networks. As such it not compatible with quantizing the weights of a given network structure, but rather with choosing the network structure under a given level of quantization. However, reviewers consider that this issue is discussed directly and clearly in the paper. 

The reviewers report to be only fairly confident about their assessment, but they all give a positive or very positive evaluation of the paper. "
iclr_2019_SJeXSo09FQ,"All reviewers gave an accept rating: 9, 7 &6.
A clear accept -- just not strong enough reviewer support for an oral."
iclr_2019_SJfPFjA9Fm,"The main criticisms were around novelty: that the analysis is rather standard. Given that all the reviewers agreed the paper is well written, I'm inclined to think the paper will be a useful contribution to the literature. The authors also highlight the analysis of the discretization, which seems to be missed by the most critical reviewer. I would suggest to the reviewers that they use the criticisms to rework the paper's introduction, to better explain which parts of the work are novel and which parts are standard. I would also suggest that standard background be moved to the appendix so that it is there for the nonexpert, while making the body of the work more focused on the novel aspects."
iclr_2019_SJfZKiC5FX,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.
 
- The approach is novel
- The experimental results are convincing.

2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- The authors didn't show results with non-Gaussian noise
- Some details that could help the understanding of the method are missing. 

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

No major points of contention.
 
4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be accepted.
"
iclr_2019_SJfb5jCqKm,"The paper proposes an improved method for uncertainty estimation in deep neural networks.

Reviewer 2 and AC note that the paper is a bit isolated in terms of comparing the literature.

However, as all of reviewers and AC found, the paper is well written and the proposed idea is clearly new/interesting."
iclr_2019_SJgEl3A5tm,"This work develops a method for learning camouflage patterns that could be painted onto a 3d object in order to reliably fool an image-based object detector.  Experiments are conducted in a simulated environment.

All reviewers agree that the problem and approach are interesting.  Reviewers 1 and 3 are highly positive, while Reviewer 2 believes that real-world experiments are necessary to substantiate the claims of the paper.  While such experiments would certainly enhance the impact of the work, I agree with Reviewers 1 and 3 that the current approach is sufficiently interesting and well-developed on its own."
iclr_2019_SJgNwi09Km,"A well-written paper that proposes an original approach for leaning a structured prior for VAEs, as a latent tree model whose structure and parameters are simultaneously learned. It describes a well-principled approach to learning a multifaceted clustering, and is shown empirically to be competitive with other unsupervised clustering models. 
Reviewers noted that the approach reached a worse log-likelihood than regular VAE (which it should be able to find as a special case), hinting towards potential optimization difficulties (local minimum?). This would benefit form a more in-depth analysis. 
But reviewers appreciated the gain in interpretability and insights from the model, and unanimously agreed that the paper was an interesting novel contribution worth publishing.
"
iclr_2019_SJggZnRcFQ,"This paper considers the problem of learning symbolic representations from raw data. The reviewers are split on the importance of the paper. The main argument in favor of acceptance is that bridges neural and symbolic approaches in the reinforcement learning problem domain, whereas most previous work that have attempted to bridge this gap have been in inverse graphics or physical dynamics settings. Hence, it makes for a contribution that is relevant to the ICLR community. The main downside is that the paper does not provide particularly surprising insights, and could become much stronger with more complex experimental domains.
It seems like the benefits slightly outweigh the weaknesses. Hence, I recommend accept."
iclr_2019_SJgsCjCqt7,"Strengths:
This paper develops a method for learning the structure of discrete latent variables in a VAE.  The overall approach is well-explained and reasonable.

Weaknesses:
Ultimately, this is done using the usual style of discrete relaxations, which come with tradeoffs and inconsistencies.

Consensus:
The reviewers all agreed that the paper is above the bar."
iclr_2019_SJgw_sRqFQ,"This work analyses the use of parameter averaging in GANs. It can mainly be seen as an empirical study (while also a convergence analysis of EMA for a concrete example provides some minor theoretical result) but experimental results are very convincing and could promote using parameter averaging in the GAN community. Therefore, even if the technical novelty is limited, the insights brought by the paper are intesting. "
iclr_2019_SJl2niR9KQ,"The paper describes the use of differentiable physics based rendering schemes to generate adversarial perturbations that are constrained by physics of image formation.

The paper puts forth a fairly novel approach to tackle an interesting question. However, some of the claims made regarding the ""believability"" of the adversarial examples produced by existing techniques are not fully supported. Also, the adversarial examples produced by the proposed techniques are not fully ""physical"" at least compared to how ""physical"" adversarial examples presented in some of the prior work were.

Overall though this paper constitutes a valuable contribution. "
iclr_2019_SJx63jRqFm,There is consensus among the reviewer that this is a good paper. It is a bit incremental compared to Gregor et al 2016. This paper show quite better empirical results.
iclr_2019_SJxTroR9F7,"The paper presents an interesting technique for constrained policy optimization, which is applicable to existing RL algorithms such as TRPO and PPO. All of the reviewers agree that the paper is above the bar and the authors have improved the exposition during the review process. I encourage the authors to address all of the comments in the final version."
iclr_2019_SJxsV2R5FQ,"pros:
- the paper is well-written and precise
- the proposed method is novel
- valuable for real-world problems

cons:
- Reviewer 2 expresses some concern about the organization of the paper and over-generality in the exposition
- There could be more discussion of scalability"
iclr_2019_SJxu5iR9KQ,"The authors present a learnt scheduling mechanism for managing communications in bandwidth-constrained, contentious multi-agent RL domains. This is well-positioned in the rapidly advancing field of MARL and the contribution of the paper is both novel, interesting, and effective. The agents learn how to schedule themselves, how to encode messages, and how to select actions. The approach is evaluated against several other methods and achieves a good performance increase. The reviewers had concerns regarding the difficulty of evaluating the overall performance and also about how it would fare in more real-world scenarios, but all agree that this paper should be accepted."
iclr_2019_SJz1x20cFQ,"Strengths 

The paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well.
The challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the
separation of internal state from external state is a clean principle that can potentially be broadly employed. 
The method does well in outperforming the alternative baselines.

Weaknesses

There is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses 
a policy ensemble;  phase info is used in DeepLoco/DeepMimic; methods such as ""Virtual Windup Toys for Animation"" 
exploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al. would help. 
The separation of internal and external state is an assumption that may not hold in many cases.
The results are locomotion focussed. There are only two timescales.

Decision

The reviewers are largely in agreement to accept the paper. 
There are fairly-simple-but-useful lessons to be found in the paper
for those working on HRL problems, particularly those for movement and locomotion. 
The AC sees the novely with respect to different pieces of related work is the weakest point of the paper.  
The reviews contain good suggestions for revisions and improvements;  the latest version may take care
of these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution
to ICLR 2019.
"
iclr_2019_SJzR2iRcK7,"This paper provides a technique to learn multi-class classifiers without multi-class labels, by modeling the multi-class labels as hidden variables and optimizing the likelihood of the input variables and the binary similarity labels. 

The majority of reviewers voted to accept."
iclr_2019_SJzSgnRcKX,"Pros

- Thorough analysis on a large number of diverse tasks
- Extending the probing technique typically applied to individual encoder states to testing for presence of certain (linguistic) information based on pairs of encoders states (corresponding to pairs of words)
- The comparison can be useful when deciding which representations to use for a given task

Cons

- Nothing serious, it is solid and important empirical study

The reviewers are in consensus."
iclr_2019_SJzqpj09YQ,"The paper proposes a deep learning framework to solve large-scale spectral decomposition.

The reviewers and AC note that the paper is quite weak from presentation. However, technically, the proposed ideas make sense, as Reviewer 1 and Reviewer 2 mentioned. In particular, as Reviewer 1 pointed out, the paper has high practical value as it aims for solving the problem at a scale larger than any existing method. Reviewer 3 pointed out no comparison with existing algorithms, but this is understandable due to the new goal.

In overall, AC thinks this is quite a boarderline paper. But, AC tends to suggest acceptance since the paper can be interested for a broad range of readers if presentation is improved."
iclr_2019_Sk4jFoA9K7,"The paper presents a novel with compelling experiments. Good paper, accept. 
"
iclr_2019_SkE6PjC9KX,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- The paper is clear and well-motivated.
- The experimental results indicate that the proposed method outperforms the SOTA
 
2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.
 
- The novelty is somewhat minor.
- An interesting (but not essential) ablation study is missing (but the authors promised to include it in the final version).

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.
 
There were no major points of contention.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be accepted.
"
iclr_2019_SkEYojRqtm,"although i (ac) believe the contribution is fairly limited (e.g., (1) only looking at the word embedding which goes through many nonlinear layers, in which case it's not even clear whether how word vectors are distributed matters much, (2) only considering the case of tied embeddings, which is not necessarily the most common setting, ...), all the reviewers found the execution of the submission (motivation, analysis and experimentation) to be done well, and i'll go with the reviewers' opinion."
iclr_2019_SkEqro0ctQ,"The paper receives a unanimous accept over reviewers, though some concerns on novelty exist. So it is suggested to be a probable accept. "
iclr_2019_SkGuG2R5tm,". Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- The proposed method is novel and effective
- The paper is clear and the experiments and literature review are sufficient (especially after revision).
 
2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

The original weaknesses (mainly clarity and missing details) were adequately addressed in the revisions.

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

No major points of contention.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be accepted."
iclr_2019_SkMQg3C5K7,This is a well written paper that contributes a clear advance to the understanding of how gradient descent behaves when training deep linear models.  Reviewers were unanimously supportive.
iclr_2019_SkMuPjRcKQ,"Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"
iclr_2019_SkMwpiR9Y7,"This paper proposes to regularize neural network in function space rather than in parameter space, a proposal which makes sense and is also different than the natural gradient approach.

After discussion and considering the rebuttal, all reviewers argue for acceptance. The AC does agree that this direction of research is an important one for deep learning, and while the paper could benefit from revision and tightening the story (and stronger experiments); these do not preclude publishing in its current state.

Side comment: the visualization of neural networks in function space was done profusely when the effect of unsupervised pre-training on neural networks was investigated (among others). See e.g. Figure 7 in Erhan et al. AISTATS 2009 ""The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training"". This literature should be cited (and it seems that tSNE might be a more appropriate visualization techniques for non-linear functions than MDS)."
iclr_2019_SkNksoRctQ,"The paper presents interesting idea, but the reviewers ask for improving further paper clarity - that includes, but is not limited to, providing in-depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand."
iclr_2019_Ske5r3AqK7,Word vectors are well studied but this paper adds yet another interesting dimension to the field.
iclr_2019_SkeK3s0qKQ,"
The authors present a novel method for tackling exploration and exploitation that yields promising results on some hard navigation-like domains. The reviewers were impressed by the contribution and had some suggestions for improvement that should be addressed in the camera ready version.
"
iclr_2019_SkeRTsAcYm,"The authors propose an algorithm for enhancing noisy speech by also accounting for the phase information. This is done by adapting UNets to handle features defined in the complex space, and by adapting the loss function to improve an appropriate evaluation metric.

Strengths
- Modifies existing techniques well to better suit the domain for which the algorithm is being proposed. Modifications like extending UNet to complex Unet to deal with phase, redefining the mask and loss are all interesting improvements.
- Extensive results and analysis.

Weaknesses
- The work is centered around speech enhancement, and hence has limited focus. 

Even though the paper is limited to speech enhancement, the reviewers agreed that the contributions made by the paper are significant and can help improve related applications like ASR. The paper is well written with interesting results and analysis. Therefore, it is recommended that the paper be accepted.
"
iclr_2019_SkeVsiAcYm,"This paper proposes to estimate the predecessor state dynamics for more sample-efficient imitation learning. While backward models have been used in the past in reinforcement learning, the application to imitation learning has not been previously studied. The paper is well-written and the results are good, showing clear improvements over GAIL in the presented experiments. The primary weakness of the paper is the lack of comparisons to the baselines suggested by reviewer 1 (a jumpy forward model and a single step predecessor model) to fully evaluate the contribution, and to SAIL and AIRL. Despite these weaknesses, the paper slightly exceeds the bar for acceptance at ICLR.
The authors are strongly encouraged to include these comparisons in the final version."
iclr_2019_SkeZisA5t7,"This paper suggests that noise-regularized estimators of mutual information in deep neural networks should be adaptive, in the sense that the variance of the regularization noise should be proportional to the range of the hidden activity. Two adaptive estimators are proposed: (1) an entropy-based adaptive binning (EBAB) estimator that chooses the bin boundaries such that each bin contains the same number of unique observed activation levels, and (2) an adaptive kernel density estimator (aKDE) that adds isotropic Gaussian noise, where the variance of the noise is proportional to the maximum activity value in a given layer. These estimators are then used to show that (1) ReLU networks can compress, but that compression may or may not occur depending on the specific weight initialization; (2) different nonsaturating noninearities exhibit different information plane behaviors over the course of training; and (3) L2 regularization in ReLU networks encourages compression. The paper also finds that only compression in the last (softmax) layer correlates with generalization performance. The reviewers liked the range of experiments and found the observations in the paper interesting, but had reservations about the lack of rigor in the paper (no theoretical analysis of the convergence of the proposed estimator), were worried that post-hoc addition of noise distorts the function of the network, and felt that there wasn't much insight provided on the cause of compression in deep neural networks. The AC shares these concerns, and considers them to be more significant than the reviewers do, but doesn't wish to override the reviewers' recommendation that the paper be accepted."
iclr_2019_Skeke3C5Fm,"although some may find the proposed approach as incremental over e.g. gu et al. (2018) and kiela et al. (2018), i believe the authors' clear motivation, formulation, experimentation and analysis are solid enough to warrant the presentation at the conference. the relative simplicity and successful empirical result show that the proposed approach could be one of the standard toolkits in deep learning for multilingual processing.


J Gu, H Hassan, J Devlin, VOK Li. Universal Neural Machine Translation for Extremely Low Resource Languages. NAACL 2018.
D Kiela, C Wang, K Cho. Context-Attentive Embeddings for Improved Sentence Representations. EMNLP 2018."
iclr_2019_SkfMWhAqYQ,"This paper presents an approach that relies on DNNs and bags of features that are fed into them, towards object recognition.  The strength of the papers lie in the strong performance of these simple and interpretable models compared to more complex architectures.  The authors stress on the interpretability of the results that is indeed a strength of this paper.

There is plenty of discussion between the first reviewer and the authors regarding the novelty of the work as the former point out to several related papers;  however, the authors provide relatively convincing rebuttal of the concerns.

Overall, after the long discussion, there is enough consensus for this paper to be accepted to the conference."
iclr_2019_SkfrvsA9FX,"This work is novel, reasonably clearly written with a thorough literature survey. The proposed approach also empirically seems promising. The paper could be improved with a bit more discussion about the sensitivity, particularly as a two-timescale approach can be more difficult to tune."
iclr_2019_SkgEaj05t7,The reviewers found the paper insightful and the authors explanations well-provided. However the paper would benefit from more systematic empirical evaluation and corresponding theoretical intuition.
iclr_2019_SkgQBn0cF7,"This paper explores the use of multi-step latent variable models of the dynamics in imitation learning, planning, and finding sub-goals. The reviewers found the approach to be interesting. The initial experiments were a main weakpoint in the initial submission. However, the authors updated the experimental results to address these concerns to a significant degree. The reviewers all agree that the paper is above the bar for acceptance. I recommend accept."
iclr_2019_Skh4jRcKQ,"The paper contributes to the understanding of straight-through estimation for single hidden layer neural networks, revealing advantages for ReLU and clipped ReLU over identity activations.  A thorough and convincing theoretical analysis is provided to support these findings.  After resolving various issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Valid criticisms of the presentation quality were raised during the review and response period, and the authors would be well served by continuing to improve the paper's clarity."
iclr_2019_SklEEnC5tQ,"This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy.

The reviewers found the contribution interesting for the ICLR community. R3 initially found the paper lacked clarity, but the authors took the feedback in consideration and made significant improvements in their revision. The reviewers all agreed that the updated paper should be accepted."
iclr_2019_SkloDjAqYm,This paper is about representation learning for calcium imaging and thus a bit different in scope that most ICLR submissions. But the paper is well-executed with good choices for the various parts of the model making it relevant for other similar domains.
iclr_2019_Sklsm20ctX,"The paper proposes a new method to improve exploration in sparse reward problems, by having two agents competing with each other to generate shaping reward that relies on how novel a newly visited state is.

The idea is nice and simple, and the results are promising.  The authors implemented more baselines suggested in initial reviews, which was also helpful.  On the other hand, the approach appears somewhat ad hoc.  It is not always clear why (and when) the method works, although some intuitions are given.  One reviewer gave a nice suggestion of obtaining further insights by running experiments in less complex environments.  Overall, this work is an interesting contribution."
iclr_2019_Sklv5iRqYX,"This paper extends the single source H-divergence theory for domain adaptation to the case of multiple domains. Thus, drawing on the known connection between H-divergence and learning the domain classifier for adversarial adaptation, the authors propose a multi-domain adversarial learning algorithm. The approach builds upon the gradient reversal version of adversarial adaptation proposed by Ganin et al 2016. 

Overall, multi-domain learning and limiting the worst case performance on any single domain is an interesting problem which has been relatively underexplored. Though this work does not have the highest performance on all datasets across competing methods, as noted by reviewers, it proposes a useful theoretical result which future research may build on. I would encourage the reviewers to compare against and discuss the missing prior work cited by Rev 3. "
iclr_2019_SkxXCi0qFX,"The paper studies the credit assignment problem in meta-RL, proposes a new algorithm that computes the right gradient, and demonstrates its superior empirical performance over others.  The paper is well written, and all reviewers agree the work is a solid contribution to an important problem."
iclr_2019_SkxXg2C5FX,"This paper presents new generalized methods for representing sentences and measuring their similarities based on word vectors. More specifically, the paper presents Fuzzy Bag-of-Words (FBoW), a generalized approach to composing sentence embeddings by combining word embeddings with different degrees of membership, which generalize more commonly used average or max-pooled vector representations. In addition, the paper presents DynaMax, an unsupervised and non-parametric similarity measure that can dynamically extract and max-pool features from a sentence pair. 

Pros:
The proposed methods are natural generalization of exiting average and max-pooled vectors. The proposed methods are elegant, simple, easy to implement, and demonstrate strong performance on STS tasks.

Cons:
The paper is solid, no significant con other than that the proposed methods are not groundbreaking innovations per say. 

Verdict:
The simplicity is what makes the proposed methods elegant. The empirical results are strong. The paper is worthy of acceptance."
iclr_2019_SyGjjsC5tQ,"This paper provides interesting results on convergence and stability in general differentiable games. The theory appears to be correct, and the paper reasonably well written. The main concern is in connections to an area of related work that has been omitted, with overly strong statements in the paper that there has been little work for general game dynamics. This is a serious omission, since it calls into question some of the novelty of the results because they have not been adequately placed relative to this work. The authors should incorporate a thorough discussion on relations to this work, and adjust claims about novelty (and potentially even results) based on that literature."
iclr_2019_SyMDXnCcF7,"This paper provides a mean-field-theory analysis of batch normalization. First there is a negative result as to the necessity of gradient explosion when using batch normalization in a fully connected network. They then provide further insights as to what can be done about this, along with experiments to confirm their theoretical predictions.

The reviewers (and random commenters) found this paper very interesting. The reviewers were unanimous in their vote to accept."
iclr_2019_SyMWn05F7,"The authors have proposed an approach for directly learning a spatial exploration policy which is effective in unseen environments. Rather than use external task rewards, the proposed approach uses an internally computed coverage reward derived from on-board sensors. The authors use imitation learning to bootstrap the training and then fine-tune using the intrinsic coverage reward. Multiple experiments and ablations are given to support and understand the approach. The paper is well-written and interesting. The experiments are appropriate, although further evaluations in real-world settings really ought to be done to fully explore the significance of the approach. The reviewers were divided, with one reviewer finding fault with the paper in terms of the claims made, the positioning against prior art, and the chosen baselines. The other two reviewers supported publication even after considering the opposition of R1, noting that they believe that the baselines are sufficient, and the contribution is novel. After reviewing the long exchange and discussion, the AC sides with accepting the paper. Although R1 raises some valid concerns, the authors defend themselves convincingly and the arguments do not, in any case, detract substantially from what is a solid submission."
iclr_2019_SyMhLo0qKQ,"All the reviewers and AC agrees that the main strength of the paper that it studies a rather important question of the validity of using linear interpolation in evaluating GANs. The paper gives concrete examples and theoretical and empirical analysis that shows linear interpolation is not a great idea. The potential weakness is that the paper doesn't provide a very convincing new evaluation to replace the linear interpolation. However, given that it's largely unclear what are the right evaluations for GANs, the AC thinks the ""negative result"" about linear interpolation already deserves an ICLR paper. "
iclr_2019_SyNPk2R9K7,"This paper presents a dataset and method for training a model to infer, from a visual scene, the program that would generate/describe it. In doing so, it produces abstract disentangled representations of the scene which could be used by agents, models, and other ML methods to reason about the scene.

This is yet another paper where the reviewers disappointingly did not interact. The first round of reviews were mediocre-to-acceptable. The authors, I think, did a good job of responding to the concerns raised by the reviewers and edited their paper accordingly. Unfortunately, not one of the reviewers took the time to consider author responses.

In light of my reading of the responses and the revisions in the paper, I am leaning towards treating this as a paper where the review process has failed the authors, and recommending acceptance. The paper presents a novel method and dataset, and the experiments are reasonably convincing. The paper has flaws and the authors are advised to carefully take into account the concerns flagged by reviewers—many of which they have responded to—in producing their final manuscript."
iclr_2019_SyNvti09KQ,"The paper considers the problem of incorporating human physiological feedback into an autonomous driving system, where minimization of a predicted arousal response is used as an additional source of reward signal, with the intuition that this could be used as a proxy for training a policy that is risk-averse.
 
Reviewers were generally positive about the novelty and relevance of the approach but had methodological concerns. In particular, concerns about the weighting of the intrinsic vs. extrinsic reward (why under different settings the optimal tradeoff parameter was different, how this affects the optimal policy if the influence of the intrinsic reward is not decreased with time). Additional baseline experiments were requested and performed, and the paper was modified to significantly incorporate other feedback such as drawing connections to imitation learning. A title change was proposed and accepted to reflect the focus on the application of risk aversion (I'd ask that the authors update the paper OpenReview metadata to reflect this).

At a high level, I believe this is an original and interesting contribution to the literature. I have not heard from two of three reviewers regarding whether their concerns were addressed, but given that their concerns appear to me to have been addressed (and their initial scores indicated that the work met the bar for acceptance, if only marginally), I am inclined to recommend acceptance."
iclr_2019_SyVU6s05K7,"The paper was judged by the reviewers as providing interesting ideas, well-written and potentially having impact on future research on NN optimization.  The authors are asked to make sure they addressed reviewers comments clearly in the paper."
iclr_2019_SyVuRiC5K7,"As far as I know, this is the first paper to combine transductive learning with few-shot classification. The proposed algorithm, TPN, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. The reviewers liked the idea, however there were concerns of novelty and clarity. I think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since September. It is therefore recommended that the authors improve the clarity based on the reviewer feedback. In particular, clarifying the details around learning \sigma_i and graph construction. It would also be useful to include the discussion of timing complexity in the final draft."
iclr_2019_SyfIfnC5Ym,"The paper presents an interesting idea for increasing the robustness of adversarial defenses by combining with existing domain adaptation approaches. All reviewers agree that the paper is well written and clearly articulates the approach and contribution.

The main areas of weakness is that the experiments focus on small datasets, namely CiFAR and MNIST.  That being said, the algorithm is reasonably ablated on the data explored and the authors provided valuable new experimental evidence during the rebuttal phase and in response to the public comment. "
iclr_2019_SygD-hCcF7,"This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The authors provide an extensive theoretical and empirical analysis, showing performance improvements in zero shot learning and finite sample error upper bounds. The reviewers generally agree this is a good paper that should be published."
iclr_2019_SygLehCqtm,"The reviewers and authors had a productive conversation, leading to an improvement in the paper quality. The strengths of the paper highlighted by reviewers are a novel learning set-up and new loss functions that seem to help in the task of protein contact prediction and protein structural similarity prediction. The reviewers characterize the work as constituting an advance in an exciting application space, as well as containing a new configuration of methods to address the problem.

Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work."
iclr_2019_SygQvs0cFQ,"as r1 and r2 have pointed out, this work presents an interesting and potentially more generalizable extension of the earlier work on introducing noise as regularization in autoregressive language modelling. although it would have been better with more extensive evaluation that goes beyond unsupervised language modelling and toward conditional language modelling, but i believe this is all fine for this further work to be left as follow-up.

r3's concern is definitely valid, but i believe the existing evaluation set as well as exposition merit presentation and discussion at the conference, which was shared by the other reviewers as well as a programme chair."
iclr_2019_SygvZ209F7,"This heavily disputed paper discusses a biologically motivated alternative to back-propagation learning.   In particular, methods focussing on sign-symmetry rather than weight-symmetry are investigated and, importantly, scaled to large problems.  The paper demonstrates the viability of the approach.  If nothing else, it instigates a wonderful platform for debate.

The results are convincing and the paper is well-presented.  But the biological plausibility of the methods needed for these algorithms can be disputed.  In my opinion, these are best tackled in a poster session, following the good practice at neuroscience meetings.

On an aside note, the use of the approach to ResNet should be questioned.  The skip-connections in ResNet may be all but biologically relevant."
iclr_2019_Syl7OsRqY7,"The paper presents a method for coarse and fine inference for question answering.  It originally measured performance only on WikiHop and then later added experiments on TriviaQA.  The results are good.

One of the concerns regarding the paper was the novelty of the work, and lack of enough experiments.  However, the addition of TriviaQA results allays some of that concern.  I'd suggest citing the paper by Swayamdipta et al from last year that attempted coarse to fine inference for TriviaQA:

Multi-Mention Learning for Reading Comprehension with Neural Cascades. 
Swabha Swayamdipta, Ankur P. Parikh and Tom Kwiatkowski. 
Proceedings of ICLR 2018.

Overall, there is relative consensus that the paper is good with a new method and some strong results."
iclr_2019_Syl8Sn0cK7,"This paper presents an RL agent which progressively synthesis programs according to syntactic constraints, and can learn to solve problems with different DSLs, demonstrating some degree of transfer across program synthesis problems. Reviewers agreed that this was an exciting and important development in program synthesis and meta-learning (if that word still has any meaning to it), and were impressed with both the clarity of the paper and its evaluation. There were some concerns about missing baselines and benchmarks, some of which were resolved during the discussion period, although it would still be good to compare to out-of-the-box MCTS.

Overall, everyone agrees this is a strong paper and that it belongs in the conference, so I have no hesitation in recommending it."
iclr_2019_SylCrnCcFX,"The paper aims to encourage deep networks to have stable derivatives over larger regions under networks with piecewise linear activation functions.

All reviewers and AC note the significance of the paper. AC also thinks this is also a very timely work and potentially of broader interest of ICLR audience."
iclr_2019_SylKoo0cKm,"This paper proposes a new measure to quantify the contribution of an individual neuron within a deep neural network. Interpretability and better understanding of the inner workings of neural networks are important questions, and all reviewers agree that this work is contributing an interesting approach and results."
iclr_2019_SylLYsCcFm,"
pros:
- The paper is well-written and includes a lot of interesting connections to cog sci (though see specific clarity concerns)
- The tasks considered (visual and symbolic) provide a nice opportunity to study analogy making in different settings.

cons:
- There was some concerns about baselines and novelty that I think the authors have largely addressed in revision

This is an intriguing paper and an exciting direction and I think it merits acceptance."
iclr_2019_SylPMnR9Ym,The reviewers had some concerns regarding clarity and evaluation but in general liked various aspects of the paper. The authors did a good job of addressing the reviewers' concerns so acceptance is recommended.
iclr_2019_Syx0Mh05YQ,"The authors have presented a simple yet elegant model to learn grid-like responses to encode spatial position, relying only on relative Euclidean distances to train the model, and achieving a good path integration accuracy. The model is simpler than recent related work and uses a structure of 'disentangled blocks' to achieve multi-scale grids rather than requiring dropout or injected noise. The paper is clearly written and it is intriguing to get down to the fundamentals of the grid code. On the negative side, the section on planning does not hold up as well and makes unverifiable claims, and one reviewer suggests that this section be replaced altogether by additional analysis of the grid model. Another reviewer points out that the authors have missed an opportunity to give a theoretical perspective on their model. Although there are aspects of the work which could be improved, the AC and all reviewers are in favor of acceptance of this paper."
iclr_2019_Syx5V2CcFm,"This paper develops a stagewise optimization framework for solving non smooth and non convex problems. The idea is to use  standard convex solvers to iteratively optimize a regularized objective with penalty centered at previous iterates - which is standard in many proximal methods. The paper combines this with the analysis for non-smooth functions giving a more general convergence results. Reviewers agree on the usefulness and novelty of the contribution. Initially there were concerns about lack of comparison with current results, but updated version have addressed this issue.  The main weakness is that the results only holds for \mu weekly convex functions and the algorithm depends on the knowledge of \mu. Despite this limitations, reviewers believe that the paper has enough new material and I suggest for publication. I suggest authors to address these issues in the final version. "
iclr_2019_Syx72jC9tm,"The paper provides a comprehensive study and generalisations of previous results on linear permutation invariant and equivariant operators / layers for the case of hypergraph data on multiple node sets. Reviewers indicate that the paper makes a particularly interesting and important contribution, with applications to graphs and hyper-graphs, as demonstrated in experiments. 

A concern was raised that the paper could be overstating its scope. A point is that the model might not actually give a complete characterization, since the analysis considers permutation action only. The authors have rephrased the claim. Following comments of the reviewer, the authors have also revised the paper to include a discussion of how the model is capable of approximating message passing networks. 

Two referees give the paper a strong support. One referee considers the paper ok, but not good enough. The authors have made convincing efforts to improve issues and address the concerns. 




 "
iclr_2019_SyxAb30cY7,"This paper provides interesting discussions on the trade-off between model accuracy and robustness to adversarial examples. All reviewers found that both empirical studies and theoretical results are solid. The paper is very well written. The visualization results are very intuitive. I recommend acceptance.
"
iclr_2019_SyxZJn05YX,"The paper proposes an interesting idea (using ""reliable"" samples to guide the learning of ""less reliable"" samples). The experimental results and detailed analysis show clear improvement in object detection, especially small objects.

On the weak side, the paper seems to focus quite heavily on the object detection problem, and how to divide the data into reliable/less-reliable samples is domain-specific (it makes sense for object detection tasks, but it's unclear how to do this for general scenarios). As the authors promise, it will make more sense to change the title to ""Feature Intertwiner for Object Detection"" to alleviate such criticisms. 

Given this said, I think this paper is over the acceptance threshold and would be of interest to many researchers. "
iclr_2019_Syx_Ss05tm,"Reviewers mostly recommended to accept after engaging with the authors. I have decided to reduce the weight of AnonReviewer3 because of the short review. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"
iclr_2019_SyxfEn09Y7,This paper proposes a new optimization method for ReLU networks that optimizes in a scale-invariant vector space in the hopes of facilitating learning. The proposed method is novel and is validated by some experiments on CIFAR-10 and CIFAR-100. The reviewers find the analysis of the invariance group informative but have raised questions about the computational cost of the method. These concerns were addressed by the authors in the revision. The method could be of practical interest to the community and so acceptance is recommended.
iclr_2019_Syxt2jC5FX,"Dear authors,

All reviewers liked your work. However, they also noted that the paper was hard to read, whether because of the notation or the lack of visualization.

I strongly encourage you to spend the extra effort making your work more accessible for the final version."
iclr_2019_Syxt5oC5YQ,"Dear authors,

Reviewers liked the idea of your new optimizer and found the experiments convincing. However, they also would have liked to get better insights on the place of AggMo in the existing optimization literature. Given that the related work section is quite small, I encourage you to expand it based on the works mentioned in the reviews."
iclr_2019_SyxtJh0qYm,"This paper proposes a VAE model with arbitrary conditioning. It is a novel idea, and the model derivation and training approach are technically sound. Experiments are thoughtfully designed and include comparison with latest related works.

R1 and R3 suggested the original version of the paper was lack of comparison with relevant work and the authors provided new experiments in the revision. The rebuttal also addressed a few other concerns about the novelty and clarity raised by R3.

Based on the novel contribution in handling missing feature imputation with VAE, I would recommend to accept the paper. It is worth noticing that there is another submission to ICLR (https://openreview.net/forum?id=ByxLl309Ym) that shares a similar idea of constructing the inference network with binary masking, although it is designed for a pre-trained VAE model.

There are still two weaknesses pointed out by R3 that would help improve the paper by addressing them:
1. The paper does not handle different kinds of missingness beyond missing at random.
2. VAE model makes the trade-off between computational complexity and accuracy.
Point 1 would be a good direction for future research, and point 2 is a common problem for all VAE approaches. While the latter should not become a reason to reject the paper, I encourage the authors to take MCMC methods into account in the evaluation section.
"
iclr_2019_SyzVb3CcFX,"The paper introduces a new and convincing method for video frame prediction, by adding prediction uncertainty through VAEs.  The results are convincing, and the reviewers are convinced.

It's unfortunate however that the method is only evaluated on simulated data.  Letting it loose on real data would cement the results and merit oral representation; in the current form, poster presentation is recommended."
iclr_2019_r14EOsCqKX,"The presented method uses mode connectivity to help illustrate the surfaces of parameter space between various selections of models (either through changes of parameters, learning methods, or epochs), and canonical correlation analysis (CCA) to visualize the similarity of model layers across two different selected models.  These analyses are then used to study 3 forms of learning heuristics: stochastic gradient descent with restart (SGDR), warmup, and distillation. 

Reviews tend to be leaning toward acceptance. 

Pros:
+ R1: Well-written
+ R1: Papers that analyze learning strategies are generally informative to the larger community. These experiments haven't been previously performed.
+ R1: Thorough experiments
+ R3: Results brought into context of prior hypotheses

Cons:
- R3: Batch normalization not studied, but authors have added experiments in response.
- R3 & R2: Practical implications not clear, but authors have added a discussion. 
"
iclr_2019_r1GAsjC5Fm,"The authors have described a navigation method that uses co-grounding between language and vision as well as an explicit self-assessment of progress. The method is used for room 2 room navigation and is tested in unseen environments. On the positive side, the approach is well-analyzed, with multiple ablations and baseline comparisons. The method is interesting and could be a good starting point for a more ambitious grounded language-vision agent. The approach seems to work well and achieves a high score using the metric of successful goal acquisition. On the negative side, the method relies on beam search, which is certainly unrealistic for real-world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community. There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers' conclusion is that the paper should be accepted. "
iclr_2019_r1GbfhRqF7,"This paper proposes a new kernel learning framework for change point detection by using a generative model. The reviewers agree that the paper is interesting and useful for the community. One of the reviewer had some issues with the paper but those were resolved after the rebuttal. The other two reviewers have short reviews and somewhat low confidence, so it is difficult to tell how this paper stands among other that exist in the literature. Overall, given the consistent ratings from all the reviewers, I believe this paper can be accepted. "
iclr_2019_r1My6sR9tX,Reviewers largely agree that the paper proposes a novel and interesting idea for unsupervised learning through meta learning and the empirical evaluation does a convincing job in demonstrating its effectiveness. There were some concerns on clarity/readability of the paper which seem to have been addressed by the authors. I recommend acceptance. 
iclr_2019_r1NJqsRctX,"The reviewers all argued for acceptance citing the novelty and potential of the work as strengths.  They all found the experiments a little underwhelming and asked for more exciting empirical evaluation.  The authors have addressed this somewhat by including multi-modal experiments in the discussion period.  The paper would be more impactful if the authors could demonstrate significant improvements on really challenging problems where MCMC is currently prohibitively expensive, such as improving over HMC for highly parameterized deep neural networks.  Overall, however, this is a very nice paper and warrants acceptance to the conference."
iclr_2019_r1e13s05YX,"The paper focuses on hybrid pipelines that contain black-boxes and neural networks, making it difficult to train the neural components due to non-differentiability. As a solution, this paper proposes to replace black-box functions with neural modules that approximate them during training, so that end-to-end training can be used, but at test time use the original black box modules. The authors propose a number of variations: offline, online, and hybrid of the two, to train the intermediate auxiliary networks. The proposed model is shown to be effective on a number of synthetic datasets.

The reviewers and AC note the following potential weaknesses: (1) the reviewers found some of the experiment details to be scattered, (2) It was unclear what happens if there is a mismatch between the auxiliary network and the black box function it is approximating, especially if the function is one, like sorting, that is difficult for neural models to approximate, and (3) the text lacked description of real-world tasks for which such a hybrid pipeline would be useful.

The authors provide comments and a revision to address these concerns. They added a section that described the experiment setup to aid reproducibility, and incorporated more details in the results and related work, as suggested by the reviewers. Although these changes go a long way, some of the concerns, especially regarding the mismatch between neural and black box function, still remain.

Overall, the reviewers agreed that the issues had been addressed to a sufficient degree, and the paper should be accepted."
iclr_2019_r1eEG20qKQ,"The paper proposes an approach to hyperparameter tuning based on bilevel optimization, and demonstrates promising empirical results. Reviewer's concerns seem to be addressed well in rebuttals and extended version of the paper."
iclr_2019_r1eVMnA9K7,"This paper introduces an unsupervised algorithm to learn a goal-conditioned policy and the reward function by formulating a mutual information maximization problem. The idea is interesting, but the experimental studies seem not rigorous enough. In the final version, I would like to see some more detailed analysis of the results obtained by the baselines (pixel approaches), as well as careful discussion on the relationship with other related work, such as Variational Intrinsic Control."
iclr_2019_r1efr3C9Ym,"After much discussion, all reviewers agree that this paper should be accepted. Congratulations!!"
iclr_2019_r1eiqi09K7,"Dear authors,

All reviewers agreed that your work sheds new light on a popular class of algorithms and should thus be presented at ICLR.

Please make sure to implement all their comments in the final version."
iclr_2019_r1f0YiCctm,"This paper proposes a novel coding scheme for compressing neural network weights using Shannon-style coding and a variational distribution over weights.  This approach is shown to improve over existing schemes for LeNet-5 on MNIST and VGG-16 on CIFAR-10, strictly dominating them in terms of compression/error rate tradeoffs. Comparing to more baselines would have been helpful. Theoretical analysis based on non-trivial extensions of prior work by Harsha et al. (2010) and  Chatterjee & Diaconis (2018) is also presented. Overall, there was consensus among the reviewers that the paper makes a solid contribution and should be published.
"
iclr_2019_r1g4E3C9t7,"The authors present a study characterizing adversarial examples in the audio domain. They highlight the importance of temporal dependency when defining defense against adversarial attacks.

Strengths
- The work presents an interesting analysis of properties of audio adversarial examples, and contrasts it with those in vision literature.
- Proposes a novel defense mechanism that is based on the idea of temporal dependency.

Weaknesses
- The technique identifies adversarial examples but is not able to make the correct prediction.
- The reviewers raised issue around clarity, but the authors took the effort to improve the section during the revision process. 

The reviewers agree that the contribution is significant and useful for the community. There are still some concerns about clarity, which the authors should consider improving in the final version. Overall, the paper received positive reviews and therefore, is recommended to be accepted to the conference."
iclr_2019_r1gEqiC9FX,"The proposed ENorm procedure is a normalization scheme for neural nets whereby the weights are rescaled in a way that minimizes the sum of L_p norms while maintaining functional equivalence. An algorithm is given which provably converges to the globally optimal solution. Experiments show it is complementary to, and perhaps slightly better than, other normalization schemes.

Normalization issues are important for DNN training, and normalization schemes like batch norm, weight norm, etc. have the unsatisfying property that they entangle multiple issues such as normalization, stochastic regularization, and effective learning rates. ENorm is a conceptually cleaner (if more algorithmically complicated) approach. It's a nice addition to the set of normalization schemes, and possibly complementary to the existing ones.

After a revision which included various new experiments, the reviewers are generally happy with the paper. While there's still some controversy over whether it's really better than things like batch norm, I think the paper would be worth publishing even if the results came out negative, since it is a very natural idea which took some algorithmic insight in order to actually execute.

"
iclr_2019_r1gNni0qtm,"AR1 finds that extension of the previously presented ICLR'18 paper are interesting and sufficient due to the provided analysis of universality and depth efficiency. AR2 is concerned with the lack of any concrete toy example between the proposed architecture and RNNs. Kindly make an effort to add such a basic step-by-step illustration for a simple chosen architecture e.g. in the supplementary material. AR3 is the most critical (the analysis TT-RNN based on the product non-linearity done before, particular case of rectifier non-linearity is used, etc.)

Despite the authors cannot guarantee the existence of corresponding weight tensor W in less trivial cases, the overall analysis is very interesting and it is the starting point for further modeling. Thus, AC advocates acceptance of this paper. The review scores do not indicate this can be an oral paper, e.g. it currently is unlikely to be in top few percent of accepted papers. Nonetheless, this is a valuable and solid work.

Moreover, for the camera-ready paper, kindly refresh your list of citations as a mere 1 page of citations feels rather too conservative. This makes the background of the paper and related work obscure to average reader unfamiliar with this topic, tensors, tensor outer products etc. There are numerous works on tensor decompositions that can be acknowledged:
- Multilinear Analysis of Image Ensembles: TensorFaces by Vasilescou et al.
- Multilinear Projection for Face Recognition via Canonical Decomposition by Vasilescou et al.
- Tensor decompositions for learning latent variable models by Anandkumar et al.
- Fast and guaranteed tensor decomposition via sketching by Anandkumar et al.

One good example of the use of the outer product (sums over rank one outer products of higher-order) is paper from 2013. They perform higher-order pooling on encoded feature vectors (although this seems to be the shallow setting) similar to Eq. 2 and 3 (this submission):
- Higher-order occurrence pooling on mid-and low-level features: Visual concept detection by Koniusz et al. (e.g. equations equations 49 and 50 or 1, 16 and 17 realize Eq. 3 and 13 in this submission)
- Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection (similar follow-up work)

Other related papers include:
- Long-term Forecasting using Tensor-Train RNNs by Anandkumar et al.
- Tensor Regression Networks with various Low-Rank Tensor Approximations by Cao et al.

Of course, the authors are encouraged to cite even more related works."
iclr_2019_r1l73iRqKm,"The paper proposes a new dataset for studying knowledge grounded conversations, that would be very useful in advancing this field. In addition to the details of the dataset and its collection, the paper also includes a framework for advancing the research in this area, that includes evaluation methods and baselines with a relatively new approach.
The proposed approach for dialogue generation however is a simple extension of previous work by (Zhang et al) to user transformers, hence is not very interesting. The proposed approach is also not compared to many previous studies in the experimental results.
One of the reviewers highlighted the weakness of the human evaluation performed in the paper. Moving on, it would be useful if further approaches are considered and included in the task evaluation. 

A poster presentation of the work would enable participants to ask detailed questions about the proposed dataset and evaluation, and hence may be more appropriate.
"
iclr_2019_r1lWUoA9FQ,"There's precious little work asking existential questions about adversarial examples, and so this work is most welcome. The work connects with deep results in probability to make simple and transparent claims about the inevitability of adversarial examples under some assumptions. The authors have addressed the key criticisms of the authors around clarity."
iclr_2019_r1laEnA5Ym,"The paper presents a variational inequality perspective on the optimization problem arising in GANs. Convergence of stochastic gradient descent methods (averaging and extragradient variants) is given under monotonicity (or convex) assumptions. In particular, binlinear saddle point problem is carefully studied with batch and stochastic algorithms. Experiments on CIFAR10 with WGAN etc. show that the proposed averaging and extrapolation techniques improve the GAN training in such a nonconvex optimization practices.

General convergence results in the context of general non-monotone VIPs is still an open problem for future exploration. The questions raised by the reviewers are well answered. The reviewers unanimously accept the paper for ICLR publication."
iclr_2019_r1lohoCqY7,"The paper conveys interesting ideas but reviewers are concern about an incremental nature of results, choice of comparators, and in general empirical and analytical novelty."
iclr_2019_r1lq1hRqYQ,"This paper generated a lot of discussion (not all of it visible to the authors or the public). 

R1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful. Despite several requests for clarification, we could not converge on a specific problem with the manuscript. Ungrounded gut feelings are not grounds for rejection. 

After an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees. Paper makes interesting contributions and will be a welcome addition to the literature. "
iclr_2019_r1lrAiA5Ym,"The authors consider the problem of active plasticity in the mammalian brain, seen as being a means to enable lifelong learning. Building on the recent paper on differentiable plasticity, the authors propose a learnt, neuro-modulated differentiable plasticity that can be trained with gradient descent but is more flexible than fixed plasticity. The paper is clearly motivated and written, and the tasks are constructed to validate the method by demonstrating clear cases where non-modulated plasticity fails completely but where the proposed approach succeeds. On a large, general language modeling task (PTB) there is a small but consistent improvement over LSTMS. The reviewers were very split on this submission, with two reviewers focusing on the lack of large improvements on large benchmarks, and the other reviewer focusing on the novelty and success of the method on simple tasks. The AC tends to side with the positive review because of the following observations: the method is novel and potentially will have long term impact on the field, the language modeling task seems like a poor fit to demonstrate the advantages of the dynamic plasticity, so focusing on that benchmark overly much is misleading, and the paper is high-quality and interesting to the community. "
iclr_2019_r1lyTjAqYX,"The paper proposes a new distributed DQN algorithm that combines recurrent neural networks with distributed prioritized replay memory. The authors systematically compare three types of initialization strategies for training the recurrent models. The thorough investigation is cited as a valuable contribution by all reviewers, with reviewer 1 noting that the study would be of interest to ""anyone using recurrent networks on RL tasks"". Empirical results on Atari and DMLab are impressive.

The reviewers noted several weaknesses in their original reviews. These included issues of clarity, a need for more detailed ablation studies, and need to more carefully document the empirical setup. A further question was raised on whether the empirical results could be complemented with theoretical or conceptual insights.

The authors carefully addressed all concerns raised during the reviewing and rebuttal period. They took exceptional care to clarify their writing, document experiment details, and ran a large set of additional experiments as suggested by the reviewers. The AC feels that the review period for the paper was particularly productive and would like to thank the reviewers and authors.

The reviewers and AC agree that the paper makes a significant contribution to the field and should be accepted."
iclr_2019_r1x4BnCqKX,"The paper presents a graph neural network that represents the movements of electrons during chemical reactions, trained from a dataset to predict reactions outcomes.

The paper is clearly written, the comparisons are sensical. There are some concerns by reviewer 3 about the experimental results: in particular the lack of a simpler baseline, and the experimental variance. I think the some of the important concerns from reviewer 3 were addressed in the rebuttal, and I hope the authors will update the manuscript accordingly.

Overall, this is fitting for publication at ICLR 2019."
iclr_2019_r1xQQhAqKX,"This work presents a method to model embeddings as distributions, instead of points, to better quantify uncertainty. Evaluations are carried out on a new dataset created from mixtures of MNIST digits, including noise (certain probability of occlusions), that introduce ambiguity, using a small ""toy"" neural network that is incapable of perfectly fitting the data, because authors mention that performance difference lessens when the network is complex enough to almost perfectly fit the data. 

Reviewer assessment is unanimously accept, with the following points:

Pros:
+ ""The topic of injecting uncertainty in neural networks should be of broad interest to the ICLR community.""
+ ""The paper is generally clear.""
+ ""The qualitative evaluation provides intuitive results.""

Cons:
- Requirement of drawing samples may add complexity. Authors reply that alternatives should be studied in future work.
- No comparison to other uncertainty methods, such as dropout. Authors reply that dropout represents model uncertainty and not data uncertainty, but do not carry out an experiment to compare (i.e. sample from model leaving dropout activated during evaluation).
- No evaluation in larger scale/dimensionality datasets. Authors mention method scales linearly, but how practical or effective this method is to use on, say, face recognition datasets, is unclear.  

As the general reviewer consensus is accept, Area Chair is recommending Accept; However, Area Chair has strong reservations because the method is evaluated on a very limited dataset, with a toy model designed to exaggerate differences between techniques. Essentially, the toy evaluation was designed to get the results the authors were looking for. A more thorough investigation would use more realistic sized network models on true datasets.  "
iclr_2019_r1xX42R5Fm,"The paper presents a novel perspective on optimizing lists of documents (""slates"") in a recommendation setting. The proposed approach builds on progress in variational auto-encoders, and proposes an approach that generates slates of the desired quality, conditioned on user responses. 

The paper presents an interesting and promising novel idea that is expected to motivate follow-up work. Conceptually, the proposed model can learn complex relationships between documents and account for these when generating slates. The paper is clearly written. The empirical results show clear improvements over competitive baselines in synthetic and semi-synthetic experiments (real users and clicks, learned user model).

The reviewers and AC also note several potential shortcomings. The reviewers asked for additional baselines that reflect current state of the art approaches, and for comparisons in terms of prediction times. There are also concerns about the model's ability to generalize to (responses on) slates unseen during training, as well as concerns about the realism of the simulated user model in the evaluation. There were questions regarding the presentation, including model details / formalism.

In the rebuttal phase, the authors addressed the above as follows. They added new baselines that reflect sequential document selection (auto-regressive MLP and LSTM) and demonstrate that these perform on par with greedy approaches. They provide details on an experiment to test generalization, showing both when the model succeeds and where it fails - which is valuable for understanding the advantages and limitations of the proposed approach. The authors clarified modeling and evaluation choices. 

Through the rebuttal and discussion phase, the reviewers reached consensus on a borderline / lean to accept decision. The AC suggests accepting the paper, based on the innovative approach and potential directions for follow up work. 
"
iclr_2019_r1xdH3CcKX,"This paper proposes a unified approach for performing state estimation and future forecasting for agents interacting within a multi-agent system. The method relies on a graph-structured recurrent neural network trained on temporal and visual (pixel) information. 

The paper is well-written, with a convincing motivation and a set of novel ideas. 

The reviewers pointed to a few caveats in the methodology, such as quality of trajectories (AnonReviewer2) and expensive learning of states (AnonReviewer3). However, these issues do not discount much of the papers' quality. Besides, the authors have rebutted satisfactorily some of those comments.

More importantly, all three reviewers were not convinced by the experimental evaluation. AnonReviewer1 believes that the idea has a lot of potential, but is hindered by the insufficient exposition of the experiments. AnonReviewer3 similarly asks for more consistency in the experiments.

Overall, all reviewers agree on a score ""marginally above the threshold"". While this is not a particularly strong score, the AC weighted all opinions that, despite some caveats, indicate that the developed model and considered application fit nicely in a coherent and convincing story. The authors are strongly advised to work further on the experimental section (which they already started doing as is evident from the rebuttal) to further improve their paper."
iclr_2019_r1xwKoR9Y7,"This paper provides an RL environment defined over Coq, allowing for RL agents and other such systems to to be trained to propose tactics during the running of an ITP. I really like this general line of work, and the reviewers broadly speaking did as well. The one holdout is reviewer 3, who raises important concerns about the need for further evaluation. I understand and appreciate their points, and I think the authors should be careful to incorporate their feedback not only in final revisions to the paper, but in deciding what follow-on work to focus on. Nonetheless, and with all due respect to reviewer 3, who provided a review of acceptable quality, I am unsure the substance of their review merits a score as low as they have given. Considering the support the other reviews offer for the paper, I recommend acceptance for what the majority of reviewers believes is a good first step towards one day proving substantial new theorems using ITP-ML hybrids."
iclr_2019_rJ4km2R5t7,"This paper provides an interesting benchmark for multitask learning in NLP.
I wish the dataset included language generation tasks instead of just classification but it's still a step in the right direction.
"
iclr_2019_rJNH6sAqY7,"All the reviewers agree that the paper has an interesting idea on regularizing the spectral norm of the weight matrices in GANs, and a generalization bound has been shown. The empirical result shows that indeed regularization improves the performance of the GANs. Based on these the AC suggested acceptance. "
iclr_2019_rJNwDjAqYX,"The authors have extended previous publications on curiosity driven, intrinsically motivated RL with this broad empirical study on the effectiveness of the curiosity algorithm on many game environments, the merits of different feature sets, and limitations of the approach. The paper is well-written and should be of interest to the community. The experiments are well conceived and seem to validate the general effectiveness of curiosity. However, the paper does not actually have any novel contribution compared against prior work, and there are no great insights or takeaways from the empirical study. Therefore, the reviewers were somewhat divided on how confident they were that the paper should be accepted. Overall, the AC agrees that it is a valuable paper that should be accepted even though it does not deliver any algorithmic novelty."
iclr_2019_rJe10iC5K7,"The paper proposes a novel method that learns decompositions of an image over parts, their hierarchical structure  and their motion dynamics given temporal image pairs. The problem tackled is of great importance for unsupervised learning from videos. One downside of the paper is the simple datasets used to demonstrate the effectiveness of the method.  All reviewers though agree on it being a valuable contribution for ICLR.

In the related work section the paper mentions ""...Some systems emphasize
learning from pixels but without an explicitly object-based representation (Fragkiadaki et al., 2016 ..."". The paper you cite in fact emphasized the importance of having object-centric predictive models and the generalization that comes from this design choice, thus, it may be potentially not the right citation. "
iclr_2019_rJe4ShAcF7,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- improvements to a transformer model originally designed for machine translation
- application of this model to a different task: music generation
- compelling generated samples and user study.

2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- lack of clarity at times (much improved in the revised version)

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

The main contention was novelty. Some reviewers felt that adapting an existing transformer model to music generation and achieving SOTA results and minute-long music sequences was not sufficient novelty. The final decision aligns with the reviewers who felt that the novelty was sufficient.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

A consensus was not reached. The final decision is aligned with the positive reviews for the reason mentioned above.
"
iclr_2019_rJeXCo0cYX,"This paper presents ""BabyAI"", a research platform to support grounded language learning. The platform supports a suite of 19 levels, based on *synthetic* natural language of increasing difficulties. The platform uniquely supports simulated ""human-in-the-loop"" learning, where a human teacher is simulated as a heuristic expert agent speaking in synthetic language.   

Pros:
A new platform to support grounded natural language learning with 19 levels of increasing difficulties. The platform also supports a heuristic expert agent to simulate a human teacher, which aims to mimic ""human-in-the-loop"" learning. The platform seems to be the result of a substantial amount of engineering, thus nontrivial to develop. While not representing the real communication or true natural language, the platform is likely to be useful for DL/RL researchers to perform prototype research on interactive and grounded language learning. 

Cons:
Everything in the presented platform is based on synthetic natural language. While the use of synthetic language is not entirely satisfactory, such limit is relatively common among the simulation environments available today, and lifting that limitation is not straightforward. The primary contribution of the paper is a new platform (resource). There are no insights or methods.

Verdict:
Potential weak accept. The potential impact of this work is that the platform will likely be useful for DL/RL research on interactive and grounded language learning."
iclr_2019_rJed6j0cKX,"This paper proposes a framework for using invertible neural networks to study inverse problems, e.g., recover hidden states or parameters of a system from measurements. This is an important and well-motivated topic, and the solution proposed is novel although somewhat incremental. The paper is generally well written. Some theoretical analysis is provided, giving conditions under which the proposed approach recovers the true posterior. Empirically, the approach is tested on synthetic data and real world problems from medicine and astronomy, where it is shown to compared favorably to ABC and conditional VAEs. Adding additional baselines (Bayesian MCMC and Stein methods) would be good. There are some potential issues regarding MMD scalability to high dimensional spaces, but overall the paper makes a solid contribution and all the reviewers agree it should be accepted for publication."
iclr_2019_rJedV3R5tm,"
pros:
- well-written and clear
- good evaluation with convincing ablations
- moderately novel

cons:
- Reviewers 1 and 3 feel the paper is somewhat incremental over previous work, combining previously proposed ideas.

(Reviewer 2 originally had concerns about the testing methodology but feels that the paper has improved in revision)
(Reviewer 3 suggests an additional comparison to related work which was addressed in revision)

I appreciate the authors' revisions and engagement during the discussion period.  Overall the paper is good and I'm recommending acceptance."
iclr_2019_rJevYoA9Fm,"This paper proposes an efficient method to compute the singular values of the linear map represented by a convolutional layer. It makes uses of the special block-matrix form of convolutional layers to construct their more efficient method. Furthermore, it shows that this method can be used to devise new regularization schemes for DNNs. The reviewers did note that the diversity of the experiments could be improved, and R2 raised concerns that the wrong singular values were being computed. The authors should add a section clarifying why the singular values of a convolutional linear map are not found directly by performing SVD on the reshaped kernel - indeed the number of singular values would be wrong. A contrast with the singular values obtained by simple reshaping of the kernel would also be helpful."
iclr_2019_rJfUCoR5KX,"The paper summarizes existing work on binary neural network optimization and performs an empirical study across a few datasets and neural network architectures. I agree with the reviewers that this is a valuable study and it can establish a benchmark to help practitioners develop better binary neural network optimization techniques.

PS: How about ""An empirical study of binary neural network optimization"" as the title?
"
iclr_2019_rJfW5oA5KQ,"The paper presents an interesting theoretical analysis by deriving polynomial sample complexity bounds for the training of GANs that depend on the approximator properties of the discriminator.
Even if it is not clear if the theory will help to pick suitable discriminators in practice, it provides
new and interesting theoretical insights on the properties of GAN training.
"
iclr_2019_rJg4J3CqFm,"
+ An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low-dimensional space
+ As the space is low-dimensional (2D), it can be directly visualized. 
+ I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t-SNE plots of high-dimensional embeddings
+ Though not the first method to embed words as densities but seemingly the first one which shows that multi-modality  / multiple senses are captured (except for models which capture discrete senses)
+ The paper is very well written

-  The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset)
-  The approach is not very scalable (hence evaluation on 17M corpus)
-  The method cannot be used to deal with data sparsity, though (very) interesting for visualization
-  This is mostly an empirical paper (i.e. an interesting application of an existing method)

The reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting. 




"
iclr_2019_rJg6ssC5Y7,"The field of deep learning optimization suffers from a lack of standard benchmarks, and every paper reports results on a different set of models and architectures, likely with different protocols for tuning the baselines. This paper takes the useful step of providing a single benchmark suite for neural net optimizers. 

The set of benchmarks seems well-designed, and covers the range of baselines with a variety of representative architectures. It seems like a useful contribution that will improve the rigor of neural net optimizer evaluation. 

One reviewer had a long back-and-forth with the authors about whether to provide a standard protocol for hyperparameter tuning. I side with the authors on this one: it seems like a bad idea to force a one-size-fits-all protocol here. 

As a lesser point, I'm a little concerned about the strength of some of the baselines. As reviewers point out, some of the baseline results are weaker than typical implementations of those methods. One explanation might be the lack of learning rate schedules, something that's critical to get reasonable performance on some of these tasks. I get that using a fixed learning rate simplifies the grid search protocol, but I'm worried it will hurt the baselines enough that effective learning rate schedules and normalization issues come to dominate the comparisons.

Still, the benchmark suite seems well constructed on the whole, and will probably be useful for evaluation of neural net optimizers. I recommend acceptance.

"
iclr_2019_rJg8yhAqKm,"The paper presents the use of information bottlenecks as a way to identify key ""decision states"" in exploration, in a goal-conditioned model. The concept of ""decision states"" is actually common in RL, states where exploring can lead to very diverse/new states. The implementation of the ""information bottleneck"" is done by adding a regularizing term, the conditional mutual information I(A;G|S).

The main weaknesses of the paper were its lack of clarity and the experimental section. It seems to me that the rebuttals, and the additional experiments and details, made the paper worthy of publication. The authors cleared enough of the gray areas and showcased the relative merits of the methods."
iclr_2019_rJgTTjA9tX,"This paper makes a substantial contribution to the understanding of the approximation ability of deep networks in comparison to classical approximation classes, such as polynomials.  Strong results are given that show fundamental advantages for neural network function approximators in the presence of a natural form of latent structure.  The analysis techniques required to achieve these results are novel and worth reporting to the community.  The reviewers are uniformly supportive."
iclr_2019_rJgYxn09Fm,This paper proposed an interesting approach to weight sharing among CNN layers via shared weight templates to save parameters. It's well written with convincing results. Reviewers have a consensus on accept.
iclr_2019_rJgbSn09Ym,"The paper proposes a particle based framework for learning object dynamics. A scene is represented by a hierarchical graph over particles, edges between particles are established dynamically based on Euclidean distance. The model is used for model predictive control, and there is also one experiment with a particle graph built from a real scene as opposed to simulation.

All reviewers agree that the architectural changes over previous relational networks  are worthwhile and merit publication. They also suggest to tone down the ``dynamic” part of the graph construction by stating that edges are determined based on a radius. In particular, previous works also consider similar addition of edges during collisions, quoting Mrowca et al. ""Collisions between objects are handled by dynamically defining pairwise collision relations ... between leaf particles..."" which suggests that comparison against a baseline for Mrowca et al. that uses a static graph is not entirely fair. The authors are encouraged to repeat the experiment without disabling such dynamic addition of edges. 

"
iclr_2019_rJl0r3R9KX,"The paper gives a novel algorithm for transfer learning with label distribution shift with provably guarantees. As the reviewers pointed out, the pros include: 1) a solid and motivated algorithm for a understudied problem 2) the algorithm is implemented empirically and gives good performance. The drawback includes incomplete/unclear comparison with previous work. The authors claimed that the code of the previous work cannot be completed within a reasonable amount of time. The AC decided that the paper could be accepted without such a comparison, but the authors are strongly urged to clarify this point or include the comparison for a smaller dataset in the final revision if possible. "
iclr_2019_rJlDnoA5Y7,"this is a meta-review with the recommendation, but i will ultimately leave the final call to the programme chairs, as this submission has a number of valid concerns.

the proposed approach is one of the early, principled one to using (fixed) dense vectors for computing the predictive probability without resorting to softmax, that scales better than and work almost as well as softmax in neural sequence modelling. the reviewers as well as public commentators have noticed some (potentially significant) short comings, such as instability of learning due to numerical precision and the inability of using beam search (perhaps due to the sub-optimal calibration of probabilities under vMF.) however, i believe these two issues should be addressed as separate follow-up work not necessarily by the authors themselves but by a broader community who would find this approach appealing for their own work, which would only be possible if the authors presented this work and had a chance to discuss it with the community at the conference. therefore, i recommend it be accepted. "
iclr_2019_rJlEojAqFm,"
pros:
- interesting application of graph networks for relational inference in MARL, allowing interpretability and, as the results show, increasing performance
- better learning curves in several games
- somewhat better forward prediction than baselines

cons:
- perhaps some lingering confusion about the amount of improvement over the LSTM+MLP baseline

Many of the reviewer's other issues have been addressed in revision and I recommend acceptance."
iclr_2019_rJlWOj0qF7,"The authors provide an interesting method to infuse hierarchical information into existing word vectors. This could help with a variety of tasks that require both knowledge base information and textual co-occurrence counts.
Despite some of the shortcomings that the reviewers point out, I believe this could be one missing puzzle piece of connecting symbolic information/sets/logic/KBs with neural nets and hence I recommend acceptance of this paper."
iclr_2019_rJleN20qK7,"The paper proposes a new method to approximate the nonlinear value function by estimating it as a sum of linear and nonlinear terms. The nonlinear term is updated much slower than the linear term, and the paper proposes to use a 
fast least-square algorithm to update the linear term. Convergence results are also discussed and empirical evidence is provided.


As reviewers have pointed out, the novelty of the paper is limited, but the ideas are interesting and could be useful for the community. I strongly recommend taking reviewers comments into account for the camera ready and also add a discussion on the relationship with the existing work.
 
Overall, I think this paper is interesting and I recommend acceptance.
"
iclr_2019_rJliMh09F7,"The paper proposes a regularization term on the generator's gradient that increases sensitivity of the generator to the input noise variable in conditional and unconditional Generative Adversarial networks, and results in multimodal predictions. All reviewers agree that this is a simple and useful addition to current GANs. Experiments that demonstrate the trade off between diversity and generation quality would be important to include, as well as the experiment on using the proposed method on unconditional GANs, which was conducted during the discussion period. "
iclr_2019_rJlk6iRqKX,The reviewers liked the clarity of the material and agreed the experimental study is convincing. Accept.
iclr_2019_rJlnB3C5Ym,"The paper presents a lot of empirical evidence that fine tuning pruned networks is inferior to training them from scratch. These results seem unsurprising in retrospect, but hindsight is 20-20.  The reviewers raised a wide range of issues, some of which were addressed and some which were not. I recommend to the authors that they make sure that any claims they draw from their experiments are sufficiently prescribed. E.g., the lottery ticket experiments done by Anonymous in response to this paper show that the random initialization does poorer than restarting with the initial weights (other than in resnet, though this seems possibly due to the learning rate). There is something different in their setting, and so your claims should be properly circumscribed. I don't think the ""standard"" versus ""nonstandard"" terminology is appropriate until the actual boundary between these two behaviors is identified. I would recommend the authors make guarded claims here."
iclr_2019_rJxHsjRqFQ,"Reviewers all agree that this is a strong submission.
I also believe it is interesting that only by changing the geometry of embeddings, they can use the space more efficiently without increasing the number of parameters."
iclr_2019_rJzLciCqKm,"This manuscript proposes a new algorithm for learning from positive and unlabeled data. The motivation for this work includes cases of selection bias, where the positive label is correlated with observation. The resulting procedure is shown to learn a scoring function that preserves the class-posterior ordering, and can thus be thresholded to obtain a classifier.

The problem addressed is interesting, and the approach sounds reasonable. The writing seems to be well done, particularly after the rebuttal when the work was better placed in context.

The reviewers and AC note issues with the evaluation of the proposed method. In particular, the authors do not provide a sufficiently convincing empirical evaluation on real data. "
iclr_2019_rk4Qso0cKm,"Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"
iclr_2019_rkMW1hRqKX,"This paper proposes an algorithm for training sequence-to-sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids the exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. Experiments on Librispeech and Wall Street Journal show that OCD improves test performance over both maximum likelihood and scheduled sampling, yielding state-of-the-art results. The primary concerns expressed by the reviewers pertained to the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers. The revision addresses the problem with a substantially larger number of references and discussion relating OCD to the previous work. Some issues of clarity were also well addressed by the revision."
iclr_2019_rke4HiAcY7,"This paper considers the information bottleneck Lagrangian as a tool for studying deep networks in the common case of supervised learning (predicting label Y from features X) with a deterministic model, and identifies a number of troublesome issues. (1) The information bottleneck curve cannot be recovered by optimizing the Lagrangian for different values of β because in the deterministic case, the IB curve is piecewise linear, not strictly concave. (2) Uninteresting representations can lie on the IB curve, so information bottleneck optimality does not imply that a representation is useful. (3) In a multilayer model with a low probability of error, the only tradeoff that successive layers can make between compression and prediction is that deeper layers may compress more. Experiments on MNIST illustrate these issues, and supplementary material shows that these issues also apply to the deterministic information bottleneck and to stochastic models that are nearly deterministic. There was a substantial degree of disagreement between the reviewers of this paper. One reviewer (R3) suggested that all the conclusions of the paper are the consequence of P(X,Y) being degenerate. The authors responded to this criticism in their response and revision quite effectively, in the opinion of the AC. Because R3 failed to participate in the discussion, this review has been discounted in the final decision. The other two reviewers were considerably more positive about the paper, with one (R1) having basically no criticisms and the other (R2) expression some doubts about the novelty of the observations being made in the paper and their importance for practical machine learning scenarios.  Following the revision and discussion, R2 expressed general satisfaction with the paper, so the AC is recommending acceptance. The AC thinks that the final paper would be clearer if the authors were to carefully distinguish between ground-truth labels used in training and the labels estimated by the model for a given input.  At the moment, the symbol Y appears to be overloaded, standing for both.  Perhaps the authors should place a hat over Y when it is standing for estimated labels?"
iclr_2019_rkeSiiA5Fm,"Strengths:
Well written paper on a new kind of spherical convolution for use in spherical CNNs.
Evaluated on rigid and non-rigid 3D shape recognition and retrieval problems.
Paper provides solid strategy for efficient GPU implementation.

Weaknesses: There was some misunderstanding about the properties of the alt-az convolution detected by one of the reviewers along with some points needing clarifications. However, discussion of these issues appears to have led to a resolution of the issues.

Contention: The weaknesses above were discussed in some detail, but the procedure was not particularly contentious and the discussion unfolded well.

All reviewers rate the paper as accept, the paper clearly provides value to the community and therefore should be accepted.
"
iclr_2019_rke_YiRct7,"This is an interesting paper that develops new techniques for analyzing the loss surface of deep networks, allowing the existence of spurious local minima to be established under fairly general conditions.  The reviewers responded with uniformly positive opinions."
iclr_2019_rkemqsC9Fm,"Strengths:  This paper gives a detailed treatment of the connections between rate distortion theory and variational lower bounds, culminating in a practical diagnostic tool.  The paper is well-written.

Weaknesses:  Many of the theoretical results existed in older work.

Points of contention:  Most of the discussion was about the novelty of the lower bound.

Consensus:  R3 and R2 both appear to recommend acceptance (R2 in a comment), and have both clearly given the paper detailed thought."
iclr_2019_rkevMnRqYQ,"The paper proposes to take advantage of implicit preferential information in a single state, to design auxiliary reward functions that can be combined with the standard RL reward function.  The motivation is to use the implicit information to infer signals that might not have been included in the reward function.  The paper has some nice ideas and is quite novel.  A new algorithm is developed, and is supported by proof-of-concept experiments.

Overall, the paper is a nice and novel contribution.  But reviewers point out several limitations.  The biggest one seems to be related to the problem setup: how to combine inferred reward and the given reward, especially when they are in conflict with each other.  A discussion of multi-objective RL might be in place."
iclr_2019_rkgBHoCqYX,"The manuscript studies a random matrix approach to recover sparse principal components. This work extends prior work using soft thresholding of the sample covariance matrix to enable sparse PCA. In this light, the main contribution of the paper is a study of generalizing soft thresholding to a broader class of functions and showing that this improves performance. The contributions of this paper are primarily theoretical.

The reviewers and AC note issues with the discussion that can be further improved to better illustrate contributions, and place this work in context. In particular, multiple reviewers assumed that ""kernel"" referred to the covariance matrix. The authors provide a satisfactory rebuttal addressing these issues.

While not unanimous, overall the reviewers and AC have a positive opinion of this paper and recommend acceptance."
iclr_2019_rkgK3oC5Fm,"This paper proposes a method to encourage diversity of Bayesian dropout method. A discriminator is used to facilitate diversity, which the method deal with multi-modality. Empirical results show good improvement over existing methods. This is a good paper and should be accepted.
"
iclr_2019_rkgKBhA5Y7,All reviewers appreciate the empirical analysis and insights provided in the paper. The paper also reports impressive results on SSL. It will be a good addition to the ICLR program. 
iclr_2019_rkgT3jRct7,"Important problem (visually grounded dialog); incremental (but not in a negative sense of the word) extension of prior work to an important new setting (GuessWhich); well-executed. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance. 
"
iclr_2019_rkgW0oA9FX,"The paper proposes an architecture search method based on graph hypernetworks (GHN). The core idea is that given a candidate architecture, GHN predicts its weights (similar to SMASH), which allows for fast evaluation w/o training the architecture from scratch. Unlike SMASH, GHN can operate on an arbitrary directed acyclic graph. Architecture search using GHN is fast and achieves competitive performance. Overall, this is a relevant contribution backed up by solid experiments, and should be accepted."
iclr_2019_rkgbwsAcYm,"This paper argues that each layer of a network may have some channels useful for and some not useful for transfer learning. The main contribution is an approach which identifies the useful channels through an attention based mechanism. The reviewers agree that this work offers a valuable new approach that offers modest improvements over prior work. 

The authors should take care to refine their definition of behavior regularization, including/expanding on the discussion from the rebuttal phase. The authors are also encouraged to experiment with other architecture backbones and report both overall performance as well as run time for learning with the larger models. 
"
iclr_2019_rkgoyn09KQ,"This paper presents an extension of an existing topic model, DocNADE. Compared to DocNADE and other existing bag-of-word topic models, the primary contribution of this work is to integrate neural language models into the topic model in order to address two limitations of the bag-of-word topic models: expressiveness and interpretability. In addtion, the paper presents an approach to integrate external knowledge into the neural topic models to address the empirical challenges of the application scenarios where there might be only a small training corpus or limited context available. 

Pros: 
The paper presents strong and extensive empirical results. The authors went above and beyond to strengthen their paper during the rebuttal and address all the reviewers' questions and suggestions (e.g., the submitted version had 7 baselines, and the revised version has 6 additional baselines per reviewers' requests).

Cons:
The paper builds on an earlier paper that introduced the DocNADE model. Thus, the modeling contribution is relatively marginal. On the other hand, the extended model, albeit based on a relatively simple idea, is still new and demonstrates strong empirical results.

Verdict:
Probably accept. While not groundbreaking, the proposed model is new and the empirical results are strong. "
iclr_2019_rkgpy3C5tX,"This paper combines two ideas: MAML, and the hierarchical Bayesian inference approach of Amit and Meir (2018). The idea is fairly straightforward but well-motivated, and it seems to work well in practice.  The paper is well-written and includes good discussion of the relevant literature. The experiments show improvements on various tests of Bayesian inference, and include some good analysis beyond simply reporting better numbers.

On the whole, the reviewers are fairly positive about the paper. (While the numerical scores are slightly below the cutoff, the reviewers are more positive in the discussion.) The reviewers' main complaint is the lack of comparisons against recently published methods, especially Gordon et al. (2018). The lack of comparison to this paper doesn't strike me as a big problem; the preprint was released only a few months before the deadline, their approach was very different from the proposed one, and the proposed approach has some plausible advantages (simplicity, computational efficiency), so I don't think a direct comparison is required for acceptance.

Overall, I recommend acceptance.
"
iclr_2019_rkl6As0cF7,"Pros:
- novel idea of endowing RL agents with recursive reasoning
- clear, well presented paper
- thorough rebuttal and revision with new results

Cons:
- small-scale experiments

The reviewers agree that the paper should be accepted."
iclr_2019_rklaWn0qK7,"Quality: The overall quality of the work is high.  The main idea and technical choices are well-motivated, and the method is about as simple as it could be while achieving its stated objectives.

Clarity:  The writing is clear, with the exception of using alternative scripts for some letters in definitions.

Originality:  The biggest weakness of this work is originality, in that there is a lot of closely related work, and similar ideas without convergence guarantees have begun to be explored.  For example, the (very natural) U-net architecture was explored in previous work.

Significance:  This seems like an example of work that will be of interest both to the machine learning community, and also the numerics community, because it also achieves the properties that the numerics community has historically cared about.  It is significant on its own as an improved method, but also as a demonstration that using deep learning doesn't require scrapping existing frameworks but can instead augment them."
iclr_2019_rkluJ2R9KQ,"This paper is concerned with solving Online Combinatorial Optimization (OCO) problems using reinforcement learning (RL). There is a well-established traditional family of approaches to solving OCO problems, therefore the attempt itself to solve them with RL is very intriguing, as this provides insights about the capabilities of RL in a new but at the same time well understood class of problems. 

The reviewers agree that this approach is not entirely new. While past similar efforts take away some of the novelty of this paper, the reviewers and AC believe that still the setting considered here contains novel and interesting elements. 

All reviewers were unconvinced that this work can provide strong claims about using RL to learn any primal-dual algorithm. This takes away some of the paper’s impact, but thanks to discussion the authors managed to clarify some “hand-wavy” claims and toned-down the claims that were not convincing. Therefore, it was agreed that the new revision still provides some useful insight into the RL and primal-dual connection, even without a complete formal connection. 
"
iclr_2019_rklz9iAcKQ,"Because of strong support from two of the reviewers I am recommending accepting this paper. However, I believe reviewer 1's concerns should be taken seriously. Although I disagree with the reviewer that a general ""framework"" method is a bad thing, I agree with them that additional experiments would be valuable."
iclr_2019_rkxQ-nA9FX,"This paper conducted theoretical analysis of the effect of batch normalisation to auto rate-tuning. It provides an explanation for the empirical success of BN. The assumptions for the analysis is also closer to the common practice of batch normalization compared to a related work of Wu et al. 2018.

One of the concerns raised by the reviewer is that the analysis does not immediately apply to practical uses of BN, but the authors already discussed how to fill the gap with a slight change of the activation function. Another concern is about the lack of empirical evaluation of the theory, and the authors provide additional experiments in the revision. R1 also points out a few weaknesses in the theoretical analysis, which I think would help improve the paper further if the authors could clarify and provide discussion in their revision.

Overall, it is a good paper that will help improve our theoretical understanding about the power tool of batch normalization."
iclr_2019_rkxaNjA9Ym,"The paper investigates a detailed analysis of reduced precision training for a feedforward network, that accounts for both the forward and backward passes in detail.  It is shown that precision can be greatly reduced throughout the network computations while largely preserving training quality.  The analysis is thorough and carefully executed.

The technical presentation, including the motivation for some of the specific choices should be made clearer.  Also, the requirement that the network first be trained to convergence at full 32 bit precision is a significant limitation of the proposed approach (a weakness that is shared with other work in this area).  It would be highly desirable to find ways to bypass or at least mitigate this requirement, which would provide a real breakthrough rather than merely a solid improvement over competing work.

The reviewer disagreement revolves primarily around the clarity of the main technical exposition: there appears to be consensus that the paper is sound and provides a serious contribution to this area.

Although the persistent reviewer disagreement left this paper rated at the borderline, I am recommending acceptance, with the understanding that the authors will not disregard the dissenting review and strive to further improve the clarity of the presentation."
iclr_2019_rkxacs0qY7,"This paper shows a promising new variational objective for Bayesian neural networks. The new objective is obtained by effectively considering a functional prior on the parameters. The paper is well-motivated and the mathematics are supported by theoretical justifications. 

There has been some discussion regarding the experimental section. On one hand, it contains several real and synthetic data which show the good performance of the proposed method. On the other hand, the reviewers requested deeper comparisons with state-of-the art (deep) GP models and more general problem settings. The AC decided that the paper can be accepted with the experiments contained in the new revision, although the authors would be strongly encouraged to address the reviewers’ comments in a “non-cosmetic manner (as R2 put it). 
"
iclr_2019_rkxciiC9tm,"The authors have proposed a new method for exploration that is related to parameter noise, but instead uses Gaussian dropout across entire episodes, thus allowing for temporally consistent exploration. The method is evaluated in sparsely rewarded continuous control domains such as half-cheetah and humanoid, and compared against PPO and other variants. The method is novel and does seem to work stably across the tested tasks, and simple exploration methods are important for the RL field. However, the paper is poorly and confusingly written and really really needs to be thoroughly edited before the camera ready deadline. There are many approaches which are referred to without any summary or description, which makes it difficult to read the paper. The three reviewers all had low confidence in their understanding of the paper, which makes this a very borderline submission even though the reviewers gave relatively high scores. "
iclr_2019_rkxoNnC5FQ,"The paper proposes an unsupervised domain adaptation solution applied for semantic segmentation from simulated to real world driving scenes. The main contribution consists of introducing an auxiliary loss based on depth information from the simulator. All reviewers agree that the solution offers a new idea and contribution to the adaptation literature. The ablations provided effectively address the concern that the privileged information does in fact aid in transfer. The additional ablation on the perceptual loss done during rebuttal is also valuable and should be included in the final version. 

The work would benefit from application of the method across other sim2real dataset tasks so as to be compared to the recent approaches mentioned by the reviewers, but the current evaluation is sufficient to demonstrate the effectiveness of the approach over baseline solutions. "
iclr_2019_rkxw-hAcFQ,"The paper presents generative models to produce multi-agent trajectories. The approach of  using a simple heuristic labeling function that labels variables that would otherwise be latent in training data is novel and and results in higher quality than the previously proposed baselines.
In response to reviewer suggestions, authors included further results with models that share parameters across agents as well as agent-specific parameters and further clarifications were made for other main comments (i.e., baselines that train the hierarchical model by maximizing an ELBO on the marginal likelihood?)."
iclr_2019_rkxwShA9Ym,"This paper formulates a method for training deep networks to produce high-resolution semantic segmentation output using only low-resolution ground-truth labels. Reviewers agree that this is a useful contribution, but with the limitation that joint distribution between low- and high-resolution labels must be known. Experimental results are convincing. The technique introduced by the paper could be applicable to many semantic segmentation problems and is likely to be of general interest.
"
iclr_2019_rkzDIiA5YQ,The reviewers that provided extensive reviews agree that the paper is well-written and contains solid technical material. The paper however should be edited to address specific concerns regarding theoretical and empirical aspects of this work. 
iclr_2019_rkzjUoAcFX,"The paper benchmarks three strategies to adapt an existing TTS system (based on WaveNet) to new speakers.

The paper is clearly written. The models and adaptation strategies are not very novel, but still a scientific contribution. Overall, the experimental results are detailed and convincing. The rebuttals addressed some of the concerns.

This is a welcomed contribution to ICLR 2019."
iclr_2019_ryE98iR5tm,"The paper proposes a novel  lossless compression scheme that leverages latent-variable models such as VAEs. Its main original contribution is to improve the bits back coding scheme [B. Frey 1997] through the use of asymmetric numeral systems (ANS) instead of arithmetic coding. The developed practical algorithm is also able to use continuous latents. The paper is well written but the reader will benefit from prior familiarity with compression schemes. Resulting message bit-length is shown empirically to be close to ELBO on MNIST. The main weakness pointed out by reviewers is that the empirical evaluation is limited to MNIST and to a simple VAE, while applicability to other models (autoregressive) and data (PixelVAE on ImageNet) is only hinted to and expected bit-length merely extrapolated from previously reported log-likelihood. The work could be much more convincing if its compression was empirically demonstrated on larger and better models and larger scale data. Nevertheless reviewers agreed that it sufficiently advanced the field to warrant acceptance."
iclr_2019_ryGfnoC5KQ,"this submission follows on a line of work on online learning of a recurrent net, which is an important problem both in theory and in practice. it would have been better to see even more realistic experiments, but already with the set of experiments the authors have conducted the merit of the proposed approach shines. "
iclr_2019_ryGgSsAcFQ,"The paper shows limitations on the types of functions that can be represented by deep skinny networks for certain classes of activation functions, independently of the number of layers. With many other works discussing capabilities but not limitations, the paper contributes to a relatively underexplored topic. 

The settings capture a large family of activation functions, but exclude others, such as polynomial activations, for which the considered type of obstructions would not apply. Also a concern is raised about it not being clear how this theoretical result can shed insight on the empirical study of neural networks. 

The authors have responded to some of the comments of the reviewers, but not to all comments, in particular comments of reviewer 1, who's positive review is conditional on the authors addressing some points. 

The reviewers are all confident and are moderately positive, positive, or very positive about this paper. "
iclr_2019_ryGkSo0qYm,"The paper is proposed as probable accept based on current ratings with a majority accept (7,7,5)."
iclr_2019_ryGvcoA5YX,"This paper presents a promising model to avoid catastrophic forgetting in continual learning. The model consists of a) a data generator to be used at training time to replay past examples (and removes the need for storage of data or labels), b) a dynamic parameter generator that given a test input produces the parameters of a classification model, and c) a solver (the actual classifier). The advantages of such combination is that no parameter increase or network expansion is needed to learn a new task, and no previous data needs to be stored for memory replay.

There is reviewer disagreement on this paper. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.

All three reviewers and AC note the following potential weaknesses: (1) presentation clarity needed substantial improvement. Notably, the authors revised the paper several times while incorporating the reviewers suggestions regarding presentation clarity. R2 has raised the final rating from 4 to 5 while retaining doubts about clarity. 
(2) weak empirical evidence: evaluation with more than three tasks and using more recent/stronger baseline methods would substantially strengthen the evaluation (R2, R3). AC would like to report the authors added an experiment with five tasks and provided a verbal comparison with ""Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence"", ECCV-2018 by reporting the authors results on the MNIST dataset. 
(3) as noted by R2, an ablation study of different model components could strengthen the evaluation. The authors included such ablation study in Table 4 of the revised paper. 
(4) reproducibility of the model could be difficult (R1). In their response, the authors promised to make the code publicly available.

AC can confirm that all three reviewers have contributed to the final discussion. Given the effort of the reviewers and authors in revising this work and its potential novelty, the AC decided that the paper could be accepted, but the authors are strongly urged to further improve presentation clarity in the final revision if possible.
"
iclr_2019_ryM_IoAqYX,"This paper provides the first convergence analysis for convex model distributed training with quantized weights and gradients. It is well written and organized. Extensive experiments are carried out beyond the assumption of convex models in the theoretical study.

Analysis with weight and gradient quantization has been separately studied, and this paper provides a combined analysis, which renders the contribution incremental. 

As pointed out by R2 and R3, it is somewhat unclear under which problem setting, the proposed quantized training would help improve the convergence. The authors provide clarification in the feedback. It is important to include those, together with other explanations in the feedback, in the future revision.

Another limitation pointed out by R3 is that the theoretical analysis applies to convex models only. Nevertheless, it is nice to show in experiments that deep networks training is benefitted from the gradient quantization empirically."
iclr_2019_rye4g3AqFm,"Dear authors,

There was some disagreement among reviewers on the significance of your results, in particular because of the limited experimental section.

Despite this issues, which is not minor, your work adds yet another piece of the generalization puzzle. However, I would encourage the authors to make sure they do not oversell their results, either in the title or in their text, for the final version."
iclr_2019_rye7knCqK7,All reviewers agree that the proposed is interesting and innovative. One reviewer argues that  some additional baseline comparisons could be beneficial and the other two suggest inclusion of additional explanations and discussions of the results. The authors’ rebuttal alleviated most of the concerns. All reviewers are very appreciative of the quality of the work overall and recommend probable acceptance. I agree with this score and recommend this work for poster presentation at ICLR.
iclr_2019_ryeOSnAqYm,"This paper analyzes existing approaches to program induction from I/O pairs, and demonstrates that naively generating  I/O pairs results in a non-uniform sampling of salient variables, leading to poor performance. The paper convincingly shows, via strong evaluation, that uniform sampling of these variables can much result in much better models, both for explicit DSL and implicit, neural models. The reviewers feel the observation is an important one, and the paper does a good job providing sufficiently convincing evidence for it.

The reviewers and AC note the following potential weaknesses: (1) the paper does not propose a new model, but instead a different data generation strategy, somewhat limiting the novelty, (2) Salient variables that need to be uniformly sampled are still user specified, (3) there were a number of notation and clarity issues that make it difficult to understand the details of the approach, and finally, (4) there are concerns with the use of rejection sampling.

The authors provided major revisions that address the clarity issues, including an addition of new proofs, cleaner notation, and removal of unnecessary text. The authors also included additional results, such as KL divergence evaluation to show how uniform the distribution is. The authors also described the need for rejection sampling, especially for Karel dataset, and clarified why the Calculator domain, even though is not ""program synthesis"", still faces similar challenges. The reviewers agreed that not having a new model is not a chief concern, and that using rejection sampling is a reasonable first step, with more efficient techniques left for others for future work.

Overall, the reviewers agreed that the paper should be accepted. As reviewer 1 said it best, this paper ""is a timely contribution and I think it is important for future program synthesis papers to take the results and message here to heart""."
iclr_2019_ryeYHi0ctQ,"A deep neural network pipeline for multiview stereo is presented. After rebuttal and discussion, all reviewers learn toward accepting the paper. Reviewer3 points to good results, but is concerned that the technical aspects are somewhat straightforward, and thus the contribution in this area is limited. The AC concurs with the reviewers."
iclr_2019_ryepUj0qtX,"The conditional network embedding approach proposed in the paper seems nice and novel, and consistently outperforms state-of-art on variety of datasets; scalability demonstration was added during rebuttals, as well as multiple other improvements; although  the reviewers did not respond by changing the scores, this paper with augmentations provided during the rebuttal appears to be a useful contribution  worthy of publishing at ICLR. "
iclr_2019_ryetZ20ctX,The reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization.  The experiments are convincing experiments. We encourage the authors to incorporate additional references suggested in the reviews. We recommend acceptance. 
iclr_2019_ryf6Fs09YX,"This clearly written paper develops a novel, sound and comprehensive mathematical framework for computing low variance gradients of expectation-based objectives. The approach generalizes and encompasses several previous approaches for continuous random variables (reparametrization trick, Implicit Rep, pathwise gradients), and conveys novel insights. 
Importantly, and originally, it extends to discrete random variables, and to chains of continuous random variables with optionally discrete terminal variables. These contributions are well exposed, and supported by convincing experiments.
Questions from reviewers were well addressed in the rebuttal and helped significantly clarify and improve the paper, in particular for delineating the novel contribution against prior related work.
"
iclr_2019_ryf7ioRqFX,"This paper presents a method for preventing exploding and vanishing gradients in LSTMs by stochastically blocking some paths of the information flow (but not others). Experiments show improved training speed and robustness to hyperparameter settings.

I'm concerned about the quality of R2, since (as the authors point out) some of the text is copied verbatim from the paper. The other two reviewers are generally positive about the paper, with scores of 6 and 7, and R1 in particular points out that this work has already had noticeable impact in the field. While the reviewers pointed out some minor concerns with the experiments, there don't seem to be any major flaws. I think the paper is above the bar for acceptance.
"
iclr_2019_ryfMLoCqtQ,"The authors provide a new analysis of generalization in deep linear networks, provide new insight through the role of ""task structure"". Empirical findings are used to cast light on the general case. This work seems interesting and worthy of publication."
iclr_2019_ryggIs0cYQ,"This paper proposes Switchable Normalization (SN) that leans how to combine three existing normalization techniques for improved performance. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN."
iclr_2019_rygjcsR9Y7,"This paper combines probabilistic models, VAEs, and self-organizing maps to learn interpretable representations on time series. The proposed contributions are a novel and interesting combination of existing ideas, in particular, the extension to time-series data by modeling the cluster dynamics. The empirical results show improved unsupervised clustering performance, on both synthetic and real datasets, compared to a number of baselines. The resulting 2D embedding also provides an interpretable visualization.

The reviewers and the AC identified a number of potential weaknesses in the presentation in the original submission: (1) there was insufficient background on SOMs, leaving the readers unable to comprehend the contributions, (2) some of the details about the experiments were missing, such as how the baselines were constructed, (3) additional experiments were needed in regards to the hyper-parameters, such as number of clusters and the weighting in the loss, and (4) Figure 4d required a description of the results.

The revision and the comments by the authors addressed most of these comments, and the reviewers felt that their concerns had been alleviated.

Thus, the reviewers felt the paper should be accepted."
iclr_2019_rygkk305YQ,"This is an ambitious paper tackling the important and timely problem of controlling non-annotated attributes in generated speech. 

The reviewers had mixed opinions about the results. R1 asks for more convincing exposition of results but, nevertheless, acknowledging that it is difficult to evaluate TTS systems systematically. Besides, R2 and R3 find the results good. 

Judging from the reviews and previous work, this paper does not seem to be very novel, although it certainly has intriguing new elements. Furthermore, it constitutes a mature piece of work.  
"
iclr_2019_rygqqsA9KX,"This paper offers a novel perspective for learning latent multimodal representations. The idea of segmenting the information into multimodal discriminative and modality-specific generating factors is found to be intriguing by all reviewers and the AC. The technical derivations allow for an efficient implementation of this idea.

There have been some concerns regarding the experimental section, but they have all been addressed adequately during the rebuttal period. Therefore the AC suggests this paper for acceptance. It is an overall nice and well-thought work. 
"
iclr_2019_rygrBhC5tQ,"Strengths: The paper tackles a novel, well-motivated problem related to options & HRL.
The problem is that of learning transition policies, and the paper proposes
a novel and simple solution to that problem, using learned proximity predictors and transition
policies that can leverage those. Solid evaluations are done on simulated locomotion and
manipulation tasks. The paper is well written.

Weaknesses: Limitations were not originally discussed in any depth. 
There is related work related to sub-goal generation in HRL.
AC: The physics of the 2D walker simulations looks to be unrealistic;
the character seems to move in a low-gravity environment, and can lean
forwards at extreme angles without falling. It would be good to see this explained.

There is a consensus among reviewers and AC that the paper would make an excellent ICLR contribution.
AC: I suggest a poster presentation; it could also be considered for oral presentation based
on the very positive reception by reviewers."
iclr_2019_ryl5khRcKm,"The reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score. The highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty. However, because the ICLR call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was well-written and executed, I would recommend it for acceptance."
iclr_2019_ryl8-3AcFX,"This paper proposes an approach for probing an environment to quickly identify the dynamics. The problem is relevant to the ICLR community. The paper is well-written, and provides a detailed empirical evaluation. The main weakness of the paper is the somewhat small originality over prior methods on online system identification. Despite this, the reviewer's agreed that the paper exceeds the bar for publication at ICLR. Hence, I recommend accept.

Beyond the related work mentioned by the reviewers, the approach is similar to work in meta-learning. Meta-RL and multi-task learning has typically been considered in settings where the reward is changing (e.g. see [1],[2],[3],[4], where [4] also uses an embedding-based approach). However, there is some more recent work on meta-RL across varying dynamics, e.g. see [5],[6]. The authors are encouraged to make a conceptual connection between this approach and the line of work in model-based meta-RL (particularly [5] and [6]) in the final version of the paper.

[1] Duan et al. https://arxiv.org/abs/1611.02779
[2] Wang et al. CogSci '17 https://arxiv.org/abs/1611.05763
[3] Finn et al. ICML '17 https://arxiv.org/abs/1703.03400
[4] Hausman et al. ICLR '17: https://openreview.net/forum?id=rk07ZXZRb
[5] Sæmundsson et al. https://arxiv.org/abs/1803.07551
[6] Nagabandi et al. https://arxiv.org/abs/1803.11347
"
iclr_2019_rylDfnCqF7,"This paper introduces a method that aims to solve the problem of 'posterior collapse' in variational autoencoders (VAEs). The problem of posterior collapse is well-documented in the VAE literature, and various solutions have been proposed. Existing proposed solutions, however, aim to solve the problem by either changing the objective function (e.g. beta-VAE) or by changing the prior and/or approximate posterior models. The proposed method, in contrast, aims to solve the problem by bringing the VAE optimization procedure closer to the EM optimization procedure. Every iteration in optimization consists of SGD updates to the inference model (E-step), performed until the approximate posterior converges. This is followed by a single SGD update of the generative model. The multi-update E-step makes sure that the M-step optimizes something closer to the marginal log-likelihood, compared to what we would normaly do in VAEs (joint optimization of both inference model and generative model).

The experiments are relatively small-scale, but convincing.

The reviewers agree that the method is clearly described, and that the proposed technique is well supported by the experiments. We think that this work will probably be of high interest to the ICLR community."
iclr_2019_rylIAsCqYm,The reviewers all agreed that this paper makes a strong contribution to ICLR by providing the first asynchronous analysis of a Nesterov-accelerated coordinate descent method.
iclr_2019_rylNH20qFQ,"This paper presents a method whereby a model learns to describe 3D shapes as programs which generate said shapes. Beyond introducing some new techniques in neural program synthesis through the use of loops, this method also produces disentangled representations of the shapes by deconstructing them into the program that produced them, thereby introducing an interesting and useful level of abstraction that could be exploited by models, agents, and other learning algorithms.

Despite some slightly aggressive anonymous comments by a third party, the reviewers agree that this paper is solid and publishable, and I have no qualms in recommending it from inclusion in the proceedings."
iclr_2019_rylV-2C9KQ,"In this work, the authors propose a simple, under parameterized network architecture which can fit natural images well, when fed with a fixed random input signal. This allows the model to be used for a number of tasks without requiring that the model be trained on a dataset. Further, unlike a recently proposed related method (DIP; [Ulyanov et al., 18]), the method does not require regularization such as early-stopping as with DIP. 
The reviewers noted the simplicity and experimental validation, and were unanimous in recommending acceptance.
"
iclr_2019_rylqooRqK7,"This paper provides an alternative way to enable differentiable optimization to the neural architecture search problem.  Different from DARTS, SNAS reformulates the problem and employs Gumbel random variables to directly optimize the NAS objective. In addition, the resource-constrained regularization is interesting. The major cons of the paper is that the empirical results are not quite impressive, especially when compared to DARTS, in terms of both accuracy and convergence. I think this is a borderline paper but maybe good enough for acceptance.

"
iclr_2019_ryxSrhC9KX,"The reviewers viewed the work favorably, with only one reviewer providing a score slightly below acceptance. The authors thoroughly addressed the reviewer's original concerns, and they adjusted their score upwards afterwards. The low-rating reviewer remains skeptical of the significance of the work, but the other two reviewers make firm cases for the appeal of the work to the ICLR audience. In follow-up discussion after the author's responses were submitted and discussed, the low-rating reviewer did not make a clear case for rejecting the paper, and further, the higher-rating reviewers' arguments for the impact of the paper were convincing. Therefore, I recommend accepting this paper."
iclr_2019_ryxepo0cFX,"The paper presents a novel idea with a compelling experimental study. Good paper, accept."
iclr_2019_ryxnHhRqFm,"Interesting paper applying memory networks that encode external knowledge (represented in the form of triples) and conversation context for task oriented dialogues. Experiments demonstrate improvements over the state of the art on two public datasets. 
Notation and presentation in the first version of the paper were not very clear, hence many question and answers were exchanged during the reviews. 
"
iclr_2019_ryxwJhC9YX,"This paper addresses a promising method for unpaired cross-domain image-to-image translation that can accommodate multi-instance images. It extends the previously proposed CycleGAN model by taking into account per-instance segmentation masks. All three reviewers and AC agree that performing such transformation in general is a hard problem when significant changes in shape or appearance of the object have to be made, and that the proposed approach is sound and shows promising results. As rightly acknowledged by R1 ‘The formulation is intuitive and well done!’

There are several potential weaknesses and suggestions to further strengthen this work: 
(1) R1 and R2 raised important concerns about the absence of baselines such as crop & attach simple baseline and CycleGAN+Seg. Pleased to report that the authors showed and discussed in their response some preliminary qualitative results regarding these baselines. In considering the author response and reviewer comments, the AC decided that the paper could be accepted given the comparison in the revised version, but the authors are strongly urged to include more results and evaluations on crop & attach baseline in the final revision if possible.
(2) more quantitative results are needed for assessing the benefits of this approach (R3). The authors discussed in their response to R3 that more quantitative results such as the segmentation accuracy of the synthesized images are not possible since no ground-truth segmentation labels are available. This is true in general for unpaired image-to-image translation, however collecting annotations and performing such quantitative evaluation could have a substantial impact for assessing the significance of this work and can be seen as a recommendation for further improvement. 
(3) the proposed model performs translation for a pair of domains; extending the work to multi-domain translation like StarGAN by Choi et al 2018 or GANimation by Pumarola 2018 would strengthen the significance of the work. The authors discussed in their response to R3 that this is indeed possible. 
"
iclr_2019_ryxxCiRqYX,"This paper relates deep learning to convex optimization by showing that the forward pass though a dropout layer, linear layer (either convolutional or fully connected), and a nonlinear activation function is equivalent to taking one τ-nice proximal gradient descent step on a a convex optimization objective. The paper shows (1) how different activation functions correspond to different proximal operators, (2) that replacing Bernoulli dropout with additive dropout corresponds to replacing the τ-nice proximal gradient descent method with a variance-reduced proximal method, and (3) how to compute the Lipschitz constant required to set the optimal step size in the proximal step. The practical value of this perspective is illustrated in experiments that replace various layers in ConvNet architectures with proximal solvers, leading to performance improvements on CIFAR-10 and CIFAR-100. The reviewers felt that most of their concerns were adequately addressed in the discussion and revision, and that the paper should be accepted."
iclr_2019_ryzECoAcY7,"As per R3: This paper presents a novel approach for doing hierarchical deep RL (HRL) on UMDPs by:
(a) use of hindsight experience replay at multiple levels;  combined with (b) max T timesteps
at each level. By effectively learning from missed goals at multiple levels, it allows for fairly 
data-efficient learning and can (in principle) work for an arbitrary number of levels.
HRL is an important open problem.

The weaknesses described reviewers include limited comparisons to other HRL methods; its applicaiton to fairly simple domain;
its still unclear what the benefit of >=4 levels is, and what the diminishing returns are wrt to the claim of working
for an arbitrary number of levels.  R1(5) and R3(7)  stand by their scores. R1(5) still has some remaining concerns
regarding some experiments not being done across all tasks, an older version of the HAL algo baseline being used, and 
lack of insight regarding >= 4 levels.

Based on the balance of the reviewers comments and the AC's own reading of the paper and results, 
and the importance of the problem, the AC leans towards accept.  Using Hindsight Exp Replay across multiple levels
is a simple-but-interesting idea, and the terminate-after-T steps is an interesting heuristic to make this effective.
While the paper does not give insight for large (>=4) levels, it does make for an interesting framework that
will inspire further work.  The AC recommends that the claims regarding an ""arbitrary number of levels"" be significantly toned down.
"
iclr_2019_B14ejsA5YQ,"Granger Causality is a beautiful operational definition of causality, that reduces causal modeling to the past-to-future predictive strength. The combination of classical granger causality with deep learning is very well motivated as a research problem. As such the continuation of the effort in this paper is strongly encouraged. However, the review process did uncover possible flaws in some of the main, original results of this paper. The reviewers also expressed concerns that the experiments were unconvincing due to very small data sizes. The paper will benefit from a revision and resubmission to another venue, and is not ready for acceptance at ICLR-2019."
iclr_2019_B14rPj0qY7,"The paper presents a unified system for perception and control that is trained in a step-wise fashion, with visual decoders to inspect scene parsing and understanding. Results demonstrate improved performance under certain conditions. But reviewers raise several concerns that must be addressed before the work is accepted.

Reviewer Pros:
+ simple elegant design, easy to understand
+ provides some insight behind system function during failure conditions (error in perception vs control)
+ improves performance under a subset of tested conditions 

Reviewer Cons:
- Concern about lack of novelty
- Evaluation is limited in scope
- References incomplete
- Missing implementation details, hard to reproduce
- Paper still contains many writing errors"
iclr_2019_B1EjKsRqtQ,"The authors propose a hierarchical attention layer which combines intermediate layers of multi-level attention. While this is a simple idea, and the authors show some improvements over the baselines, the authors raised a number of concerns about the validity of the chosen baselines, and the lack of more detailed evaluations on additional tasks and analysis of the results. Given the incremental nature of the work, and the significant concerns raised by the reviewers, the AC is recommending that this paper be rejected."
iclr_2019_B1GHJ3R9tQ,"All of the reviewers find this paper to contain interesting ideas. Originally, clarity was a major issue, although a few issues remain (see the comments of reviewer 3). The reviewers believe that the paper has been substantially improved from its original form, however there is still room for improvement: more comprehensive comparisons to existing work (reviewer 1), careful ablations (reviewer 3), etc. With a little bit of polish, this paper is likely to be accepted at another venue.

I am certainly not penalizing you for anonymously sharing your code on Github, as this was specifically requested by reviewer 1.
"
iclr_2019_B1GIB3A9YX,"The paper presents an explicit memory that directly contributes to more efficient exploration. It stores trajectories to novel states, that serve as training data to learn to reach those states again (through iterative sub-goals). 

The description of the method is quite clear, the method is not completely novel but has some merit. Most weaknesses of the paper come from the experimental section: too specific environments/solutions, lack of points of comparisons, lacking some details.

We strongly encourage the authors to add additional experimental evidence, and details. In its current form, the paper is not sufficient for publication at ICLR 2019.

Reviewers wanted to note that the blog post from Uber (""Go-Explore"") did _not_ affect their evaluation of this paper."
iclr_2019_B1GIQhCcYm,"The paper formulates the problem of unsupervised one-to-many image translation and addresses the problem by minimizing  the mutual information.

The reviewers and AC note the critical limitation of novelty and comparison of this paper to meet the high standard of ICLR. 

AC decided that the authors need more works to publish."
iclr_2019_B1GSBsRcFX,"The paper proposes an interesting data-dependent regularization method for orthogonal-low-rank embedding (OLE). Despite the novelty of the method, the reviewers and AC note that it's unclear whether the approach can extend other settings with multi-class or continuous labels or other loss functions. "
iclr_2019_B1M9FjC5FQ,Reviewers are in a consensus and recommended to reject. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
iclr_2019_B1MAJhR5YX,"The paper seeks to obtain faster means to count or approximately count of the number of linear regions of a neural network. The paper improves bounds and makes an interesting contribution to a long line of work. 

A consistent concern of the reviewers is the limited applicability of the method. The empirical evaluation can serve to better assess the accuracy of theoretical bounds that have been obtained in previous works, but the practical utility is not as clear yet. 

This is a borderline case. The reviewers lean towards a positive rating of the paper, but are not particularly enthusiastic about the paper. The paper makes good contributions, but is just not convincing enough. 

I think that the work program that the authors suggest in their responses could lead to a stronger paper in the future. In particular, the exploration of necessary and sufficient conditions for different neural networks to be equivalent and the use of number of linear regions when analyzing neural networks, seem to be very promising directions. "
iclr_2019_B1MB5oRqtQ,"The reviewers raise an important issue about the parameters in the proposed gradient in Theorem 1. There could be different parameters for each policy in the gradient (though some parameter sharing could be possible), and computing this gradient would be prohibitive. The solution is to just use the most recent parameters, but then the gradients become off-policy again without motivation for why this is acceptable. This approximation needs to be better justified. 

As an additional point, there are other off-policy policy gradient methods, than just DPG. The authors could consider comparing to these strategies (which can use replay buffers) and explain why the proposed strategy provides benefits beyond these. What is inadequate about these methods? Further motivation is needed for the proposed strategy. This is additionally true because the proposed strategy requires entire sampled trajectories for a fixed policy (to make the policy gradient sound, with weighting dpi_n(s)), whereas DPG and other off-policy AC methods do not need that."
iclr_2019_B1MIBs05F7,"All reviewers agreed that this paper addresses an important question in deep learning (why doesn't SVRG help for deep learning)? But the paper still has some issues that need to be addressed before publication, thus the AC recommends ""revise and resubmit""."
iclr_2019_B1MUroRct7,"The paper investigates an incremental form of Sliced Inverse Regression (SIR) for supervised dimensionality reduction.  Unfortunately, the experimental evaluation is insufficient as a serious evaluation of the proposed techniques.  More importantly, the paper does not appear to contribute a significant advance over the extensive literature on fast generalized eigenvalue decompositions in machine learning.  No responses were offered to counter such an opinion."
iclr_2019_B1MX5j0cFX,"The topic of universal adversarial perturbation is quite intriguing and fairly poorly studied and the paper provides a mix of new insights, both theoretical and empirical in nature. However, the significant presentation issues make it hard to properly understand and evaluate them. In particular, the theoretical part feels rushed and not sufficiently rigorous, and it is unclear why focusing on the case of equivariant network is crucial. Also, it would be useful if the authors put more effort in explaining how their contributions fit into the context of prior work in the area.

Overall, this paper has a potential of becoming a solid contribution, once the above shortcomings are addressed."
iclr_2019_B1MbDj0ctQ,The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
iclr_2019_B1MhpiRqFm,"Pros:
- a method that obtains convergence results using a using time-dependent (not fixed or state-dependent) softmax temperature.

Cons:
- theoretical contribution is not very novel
- some theoretical results are dubious
- mismatch of Boltzmann updates and epsilon-greedy exploration
- the authors seem to have intended to upload a revised version of the paper, but unfortunately, they changed only title and abstract, not the pdf -- and consequently the reviewers did not change their scores.

The reviewers agree that the paper should be rejected in the submitted form."
iclr_2019_B1VWtsA5tQ,"This paper proposes to improve the exploration in the PPO algorithm by applying CMA-ES. Major concerns of the paper include: paper editing can be improved; the choices of baselines used in the paper may be not reasonable; flaws in comparisons with SOTA. It is also not quite clear why CMA can improve exploration, further justification required. Overall, this paper cannot be published in its current form."
iclr_2019_B1e0KsRcYQ,"AR1 is is concerned that the only contribution of this work is  combining second-order pooling with with a codebook style assignments. After discussions, AR1 still maintains that that the proposed factorization is a marginal contribution. AR2 feels that the proposed paper is highly related to numerous current works (e.g. mostly a mixture of existing contributions) and that evaluations have not been improved. AR3 also points that this paper lacks important comparisons for fairly evaluating the effectiveness of the proposed formulation and it lacks detailed description and discussion for the methods.

AC has also pointed several works to the authors which are highly related (but by no means this is not an exhaustive list and authors need to explore google scholar to retrieve more relevant papers than the listed ones):

[1] MoNet: Moments Embedding Network by Gou et al. (e.g. Stanford Cars via Tensor Sketching: 90.8 vs. 90.4 in this submission, Airplane: 88.1 vs. 87.3% in this submission, 85.7 vs. 84.3% in this submission)
[2] Second-order Democratic Aggregation by Lin et al. (e.g. Stanford Cars: 90.8 vs. 90.4 in this submission)
[3] Statistically-motivated Second-order Pooling by Yu et al (CUB: 85%)
[4] DeepKSPD: Learning Kernel-matrix-based SPD Representation for Fine-grained Image Recognition by Engin et al.
[5] Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks by Q. Wang et al. (512D representations)
[6] Low-rank Bilinear Pooling for Fine-Grained Classification' by S. Kong et al. (CVPR I believe). They get some reduction of size of 10x less than tensor sketch, higher results than here by some >2% (CUB), and all this obtained in somewhat more sophisticated way.

The authors brushed under the carpet some comparisons. Some methods above are simply better performing even if cited, e.g. MoNet [1] uses sketching and seems a better performer on several datasets, see [2] that uses sketching (Section 4.4), see [5] which also generates compact representation (8K). [4] may be not compact but the whole point is to compare compact methods with non-compact second-order ones too (e.g. small performance loss for compact methods is OK but big loss warrants a question whether they are still useful). Approach [6] seems to also obtain better results on some sets (common testbed comparisons are essentially encouraged). 

At this point, AC will also point authors to sparse coding methods on matrices (bilinear) and tensors (higher-order) from years 2013-2018 (TPAMI, CVPR, ECCV, ICCV, etc.). These all methods can produce compact representations (512 to 10K or so) of bilinear or higher-order descriptors for classification. This manuscript fails to mention this family of methods.

For a paper to be improved for the future, the authors should consider the following:
- make a thorough comparison with existing second-order/bilinear methods in the common testbed (most of the codes are out there on-line)
- the authors should vary the size of representation (from 512 to 8K or more) and plot this against accuracy
- the authors should provide theoretical discussion and guarantees on the quality of their low-rank approximations (e.g. sketching has clear bounds on its approximation quality, rates, computational cost). The authors should provide some bounds on the loss of information in the proposed method.
- authors should discuss the theoretical complexity of proposed method (and other methods in the literature)

Additionally, the authors should improve their references and the story line. Citing  (Lin et al. (2015)) in Eq. 1 and 2 as if they are the father of bilinear pooling is misleading. Citing (Gao et al. (2016)) in the context of polynomial kernel approximation in Eq. 3 to obtain bilinear pooling should be preceded with earlier works that expand polynomial kernel to obtain bilieanr pooling. AC can think of at least two papers from 2012/2013 which do derive bilinear pooling and could be cited here instead. AC encourages the authors to revise their references and story behind bilinear pooling to give unsuspected readers a full/honest story of bilinear representations and compact methods (whether they are branded as compact or just use sketching etc., whether they use dictionaries or low-rank representations).

In conclusion, it feels this manuscript is not ready for publication with ICLR and requires a major revision. However, there is some merit in the proposed direction and authors are encouraged to explore further."
iclr_2019_B1e4wo09K7,"The paper presents a new approach to learn separate class-invariant and class-equivariant latent representations, by training on labeled (and optional additional unlabelled) multi class data. Empirical results on MNIST and SVHN show that the method works well.
Reviewers initially highlighted the following weaknesses of the paper: insufficient references and contrasting with related work (given that this problem space has been much explored before), 
limited novelty of the approach, limited experiments (MNIST only). One reviewer also mentioned a sometimes vague, overly hyperbolic, and meandering writeup.

Authors did a commendable effort to improve the paper based on the reviews, adding new references, removing and rewriting parts of the paper to make it more focused, and providing experimental results on an additional dataset (SVHN). The paper did improve as a result. But while attenuated, the initial criticisms remain valid: the literature review and discussion remains short and too superficial. The peculiarities of the approach which grant it (modest) originality are insufficiently (theoretically and empirically) justified and not clearly enough put in context of the whole body of prior work. Consequently the proposed approach feels very ad-hoc. Finally the additional experiments are a step in the right direction, but experiments on only MNIST and SVHN are hardly enough in 2018 to convince the reader that a method has a universal potential and is more generally useful. Given the limited novelty, and in the absence of theoretical justification, experiments should be much more extensive, both in diversity of data/problems, and in the range of alternative approaches compared to, to build a convincing case.
"
iclr_2019_B1e7hs05Km,"There was a significant amount of discussion on this paper, both from the reviewers and from unsolicited feedback.  This is a good sign as it demonstrates interest in the work.  Improving exploration in Deep Q-learning through Thompson sampling using uncertainty from the model seems sensible and the empirical results on Atari seem quite impressive.  However, the reviewers and others argued that there were technical flaws in the work, particularly in the proofs.  Also, reviewers noted that clarity of the paper was a significant issue, even more so than a previous submission.  

One reviewer noted that the authors had significantly improved the paper throughout the discussion phase.  However, ultimately all reviewers agreed that the paper was not quite ready for acceptance.  It seems that the paper could still use some significant editing and careful exposition and justification of the technical content.

Note, one of the reviews was disregarded due to incorrectness and a fourth reviewer was brought in."
iclr_2019_B1e8CsRctX,"This paper suggests the use of generative ensembles for detecting out-of-distribution samples. 

The reviewers found the paper easy to read, especially after the changes made during the rebuttal. However, further elaboration in the technical descriptions (and assumptions made) could make the work seem more mature, as R2 and R1 point out. 

The general feeling by reading the reviews and discussions is that this is promising work that, nevertheless, needs some more novel elements. A possible avenue for increasing the contribution of the paper is to follow R1’s advice to extract more convincing insights from the results. 
"
iclr_2019_B1e9W3AqFX,"This paper presents a novel idea of transferring gradients between tasks to improve multi-task learning in neural network models. The write-up includes experiments with multi-task experiments with text classification and sequence labeling, as well as multi-domain experiments. After the reviews, there are still some open questions in the reviewer comments, hence the reviewer decisions were not updated.
For example, the impact of sequential update in pairwise task communication on performance can be analyzed. Two reviewers question task relatedness and the impact of how and when it is computed could be good to include in the work. Baselines could be improved to reflect reviewer suggestions."
iclr_2019_B1e9csRcFm,"This paper provides a generalization analysis for graph embedding methods concluding with the observation that the norm of the embedding vectors provides an effective regularization, more so than dimensionality alone. The main theoretical result is backed up by several experiments.  While the result appears to be correct, norm control, dimensionality reduction and early stopping during optimization are all very well studied in machine learning as effective regularizers, either operating alone or in conjunction. The regularization parameters, iteration count, embedding dimensionality is typically tuned for an application. The AC agrees with Reviewer 2 that the paper does not provide sufficiently interesting insights beyond this observation and is unlikely to influence practical applications of these methods.   Both reviewer 2 and 3 have also raised points on the need for stronger empirical analysis."
iclr_2019_B1eCCoR5tm,"The paper proposes a data augmentation technique to ensemble classifiers.
Reviewers pointed to a few concerns, including a lack of novelty, a lack
of proper comparison with state-of-the-art models or other data augmentation
approaches.
Overall, all reviewers recommended to reject the paper, and I concur with them."
iclr_2019_B1eEKi0qYQ,"The authors present a new method for leveraging multiple parallel agents to speed RL in continuous action spaces. By monitoring the best performers, that information can be shared in a soft way to speed policy search. The problem space is interesting and faster learning is important. However, multiple reviewers [R2, R1] had significant concerns with how the work is framed with respect to the wider literature (even after the revisions), and some concerns over the significance of the performance improvements which seem primarily to come from early boosts. There is also additional related work on concurrent RL (Guo and Brunskill 2015; Dimakopoulou, Van Roy 2018 ; Dimakopoulou, Osband, Van Roy 2018) which provides some more formal considerations of the setting the authors consider, which would be good to reference. 
"
iclr_2019_B1eKk2CcKm,"This paper proposes to learn continuous of k-mer embeddings for RNA-seq analysis. Major concerns of the paper include: 1. novelty seems limited; 2. questions about the scalability of the approach; 3. evaluation experiments were not suitable for supporting the aim. Overall, this paper cannot be accepted yet. "
iclr_2019_B1eO9oA5Km,All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR and the authors did not respond during the rebuttal phase.
iclr_2019_B1ePui0ctQ,"Reviewers mostly recommended to reject. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
"
iclr_2019_B1eSg3C9Ym,"This paper presents a mean field analysis of the effect of batch norm on optimization. Assuming the weights and biases are independent Gaussians (an assumption that's led to other interesting analysis), they propagate various statistics through the network, which lets them derive the maximum eigenvalue of the Fisher information matrix. This determines the maximum learning rate at which learning is stable. The finding is that batch norm allows larger learning rates.

In terms of novelty, the paper builds on the analysis of Karakida et al. (2018). The derivations are mostly mechanical, though there's probably still sufficient novelty.

Unfortunately, it's not clear what we learn at the end of the day. The maximum learning rate isn't very meaningful to analyze, since the learning rate is only meaningful relative to the scale of the weights and gradients, and the distance that needs to be moved to reach the optimum. The authors claim that a ""higher learning rate leads to faster convergence"", but this seems false, and at the very least would need more justification. It's well-known that batch norm rescales the norm of the gradients inversely to the norm of the weights; hence, if the weight norm is larger than 1, BN will reduce the gradient norm and hence increase the maximum learning rate. But this isn't a very interesting effect from an optimization perspective. I can't tell from the analysis whether there's a more meaningful sense in which BN speeds up convergence. The condition number might be more relevant from a convergence perspective.

Overall, this paper is a promising start, but needs more work before it's ready for publication at ICLR.

"
iclr_2019_B1eXbn05t7,"The paper is on the borderline. From my reading, the paper presents a reasonable idea with quite good results on novel image generation and one-shot learning. On the other hand, the comparison against the prior work (both generation task and one-shot classification task) is not convincing. I also feel that there are many work with similar ideas (I listed some below, but these are not exhaustive/comprehensive list), but they are not cited or compared, I am not sure about if the proposed concept is novel in high-level. Although some implementation details of this method may provide advantages over other related work, such comparison is not clear to me.

Disentangling factors of variation in deep representations using adversarial training
https://arxiv.org/abs/1611.03383
NIPS 2016

Rethinking Style and Content Disentanglement in Variational Autoencoders
https://openreview.net/forum?id=B1rQtwJDG
ICLR 2018 workshop

Disentangling Factors of Variation by Mixing Them
http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Disentangling_Factors_of_CVPR_2018_paper.pdf
CVPR 2018

Separating Style and Content for Generalized Style Transfer
https://arxiv.org/pdf/1711.06454.pdf

Finally, I feel that the writing needs improvement. Although the method is intuitive and has simple idea, the paper seems to lack full details (e.g., principled derivation of the model as a variant of the VAE formulation) and precise definitions of terms (e.g., second term of LF loss). 
"
iclr_2019_B1eZRiC9YX,"This paper conducts a study of the adversarial robustness of Bayesian Neural Network models. The reviewers all agree that the paper presents an interesting direction, with sound theoretical backing. However, there are important concerns regarding the significance and clarity of the work. In particular, the paper would greatly benefit from more demonstrated empirical significance, and more polished definitions and theoretical results. "
iclr_2019_B1edvs05Y7,"This paper proposes a sparse binary compression method for distributed training of neural networks with minimal communication cost. Unfortunately,  the proposed approach is not novel, nor supported by strong experiments. The authors did not provide a rebuttal for reviewers' concerns. 
"
iclr_2019_B1epooR5FX,"The paper proposes a framework at the intersection of programming and machine learning, where some variables in a program are replaced by PVars - variables whose values are learned using machine learning from data. The paper presents an API that is designed to support this scenario, as well as three case studies: binary search, quick sort, and caching - all implemented with PVars.

The reviewers and the AC agree that the paper presents and potentially valuable new idea, and shows concrete applications in the presented case studies. They provide example code in the paper, and present a detailed analysis of the obtained results.

The reviewers and AC also not several potential weaknesses - the AC will focus on a subset for the present discussion. The paper is unusual in that it presents a programming API rather than e.g., a thorough empirical comparison, a novel approach, or new theoretical insights. Paper at the intersection of systems and machine learning can make valuable contributions to the ICLR community, but need to provide a clear contributions which are supported in the paper by empirical or theoretical results. The research contributions of the present paper are vague, even after the revision phase. The main contribution claimed is the introduction of the API, and that such an API / system is feasible. This is an extremely weak claim. A stronger claim would be if e.g., the present approach would advance the state of the art beyond an existing such framework, e.g., probabilistic programming, either conceptually or empirically. I want to particularly highlight probabilistic programming here, as it is mentioned by the authors - this is a well developed research area, with existing approaches and widely used tools. The authors dismiss this approach in their related work section, saying that probabilistic programming is ""specialized on working with distributions"". Many would see the latter as a benefit, so the authors should clearly motivate how their approach improves over these existing methods, and how it would enable novel uses or otherwise provide benefits. At the current stage, the paper is not ready for publication."
iclr_2019_B1ethsR9Ym,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

- The paper tackles an interesting and relevant problem for ICLR: guided image modification of images (in this case of facial attributes).
- The proposed method is in general well-explained (although some details are lacking)
 
2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- The training set of faces and associated attributes were annotated using a pre-trained model which introduced a bias into the annotations used for training the method.
- The experimental results weren't convincing. The qualitative results showed no clear advantage of the proposed method and the quantitative comparison to StarGAN only considered two attribute manipulations and only found a statistically significant different in performance for one of those.
The second weakness was the key determining factor in the AC's final recommendation. 

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

There were no major points of contention and no author feedback.
 
4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be rejected.
"
iclr_2019_B1euhoAcKX,"The paper addresses the complexity issue of Determinantal Point Processes via generative deep models.

The reviewers and AC note the critical limitation of applicability of this paper to variable ground set sizes, whether authors' rebuttal is not convincing enough.

AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish."
iclr_2019_B1excoAqKQ,"
This is an interesting direction and all reviewers thought the idea has merit, but pointed out some significant limitations. The authors did an admirable job at addressing some of these but some remain, including R1’s point 2 which is a significant issue. The authors are encouraged to submit a revised version of their work which addresses all the discussed limitations and will likely be a competitive submission to another top ML conference.
"
iclr_2019_B1fA3oActQ,"This paper proposes a new method for graph representation in sequence-to-sequence models and validates its results on several tasks. The overall results are relatively strong.

Overall, the reviewers thought this was a reasonable contribution if somewhat incremental. In addition, while the experimental comparison has greatly improved from the original version, there are still a couple of less satisfying points: notably the size of the training data is somewhat small. In addition, as far as I can tell all comparisons with other graph-based baselines actually aren't implemented in the same toolkit with the same hyperparameters, so it's a bit difficult to tell whether the gains are coming from the proposed method itself or from other auxiliary differences.

I think this paper is very reasonable, and definitely on the borderline for acceptance, but given the limited number of slots available at ICLR this year I am leaning in favor of the other very good papers in my area."
iclr_2019_B1fPYj0qt7,"This paper proposes using a tensor train low rank decomposition for compressing neural network parameters.  However the paper falls short on multiple fronts 1)lack of comparison with existing methods 2) no baseline experiments. Further there are concerns about correctness of the math in deriving the algorithms, convergence and computational complexity of the proposed method.  I strongly suggest taking the reviews into account before submitting the paper it again. "
iclr_2019_B1fbosCcYm,"The paper received mixed and divergent reviews. As a paper of unusual topic in ICLR, the presentation of this work would need improvement. For example, it is difficult to understand what's the overall objective function, why a specific design choice was made, etc. It's nice to see that the authors somehow did quite a bit of engineering to make their model work for classification and drawing tasks, but (as an ML person) it’s difficult to get a clear rationale on why the method works other than that it’s biologically motivated. In addition, the proposed model (at a functional level) looks quite similar to Mnih et al.'s ""Recurrent Models of Visual Attention"" work (for classification) and Gregor et al's DRAW model (for generation) in that all these models use sequential/recurrent attention/glimpse mechanisms, but no direct comparisons are made. For classification, the method achieves strong performance on MNIST but this may be due to a better architecture choice compared to Mnih's model but not due to the difference of the memory mechanism. For image generation/reconstruction, the proposed method seems to achieve quite good results but they are not as good as those from DRAW method. Overall, the paper is on the borderline, and while this work has some merits and might be of interest to some subset of audience in ICLR, there are many issues to be addressed in terms of presentation and comparisons. Please see reviews for other detailed comments."
iclr_2019_B1fysiAqK7,"The paper proposes a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling.

The reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) preliminary experimental results.

AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish."
iclr_2019_B1g-X3RqKm,"This paper attempts at ranking of tasks handled by deep learning methods based on learning curves.  A main premise of the paper is ""fitting learning curves to a power law, and then sorting tasks by empirical estimates of exponents"".   The idea of the paper is quite interesting.

However, the paper makes some bold claims which are a bit distant from the empirical study it conducts.  It is hard to line up the order in Table 2 with the Chomsky hierarchy.  Also, for various tasks, various different deep models are used (ResNets for image classification, LSTMs for LM, and so on).  I was not convinced that the beta parameter is model-agnostic.

Similar concerns are expressed by the reviewers, and they agree that the paper should address the criticism that they express in their feedback."
iclr_2019_B1g29oAqtm,"The issue of when model based methods can be used successfully in RL is an interesting one. However, the reviewers had a number of concerns about the significance and framing of this work with respect to the related literature. In addition, the abstract and title suggest a very generic contribution will be made, whereas the actual contribution is to a much more specific subclass. Some relevant papers (and their related efforts) include the following.

The Dependence of Effective Planning Horizon on Model Accuracy. 
(AAMAS-15, best paper award) Nan Jiang, Alex Kulesza, Satinder Singh, Richard Lewis. 

Self-Correcting Models for Model-Based Reinforcement Learning. Erik Talvitie. In 'Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI).' 2017. 
"
iclr_2019_B1gHjoRqYQ,"The paper proposes a new method for adversarial attacks, MarginAttack, which finds adversarial examples with small distortion and runs faster than the CW baseline, but slower than other methods. The authors provide theoretical guarantees and a broad set of experiments. 

In the discussion, a consistent concern has been that, experimentally, the method does not perform noticeably better than previous approaches. The authors mention that the lines are too thick to reveal the difference. It has been pointed out that this might be related to the way the experiments are conducted, but the proposed method still does better than other methods. AnonReviewer1 mentions that the assumptions needed for the theoretical part might be too strong, meaning that the main contribution of the paper is in the experimental side. 

The comparisons with other methods and the assumptions made in the theorems seem to have caused quite some confusion and there was a fair amount of discussion. Following the discussion session, AnonReviewer1 updated his rating from 5 to 6 with high confidence. 

The referees all rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below the acceptance threshold.  

Although the paper seems to propose valuable ideas, and it appears that the discussion has clarified many questions from the initial submission, the paper has not provided a clear, convincing, selling point at this time. "
iclr_2019_B1gIf305Ym,"Pros:
- an explicitly multi-objective approach to neural architecture search
- multiple datasets
- ablation experiments

Cons:
- lack of baselines like hyperparameter search
- ill-justified increase in capacity after search
- ineffective use of the multiple objectives in assessment
- not clearly beating random search baseline

The reviewers adjusted their scores upward after the rebuttal, but serious concerns remain, and the consensus is still to (borderline) reject the paper."
iclr_2019_B1gJOoRcYQ,"1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

The paper 
- tackles an interesting problem
- makes a concerted effort to provide qualititative results that give insight into the models behaviour.
- sufficiently cites related work.

2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

- The model architecture lacks novelty.
- There was also agreement that the contributions - (i) minor modifications of existing sequential attention-based models, and (ii) application to the RL domain - are minor.
- A lot of space in the paper (section 4.2) is devoted to exploring the use of this model for image classification and video action recognition. However the proposed model performed poorly compared to SOTA methods for this task and no motivation was given for why the proposed model would be useful for such tasks.

All three points impacted the final decision.

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

There was high agreement between the reviewers on the main drawbacks of the paper, before and after the rebuttal.
The AC considered the rebuttals by the authors (in which they argued that there was sufficient contribution) but, in the end, agreed with the reviewers' assessments.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be rejected.
"
iclr_2019_B1ggosR9Ym,"The reviewers highlighted that the application in the paper is interesting, but note a lack of new methodology, and also highlight serious flaws in the testing methodology. Specifically, the reviewers are discouraged by the straightforward reuse of Siamese networks without clear modifications. Further, the testing setup might be unfairly easy, since chemical families are represented in both training and test sets, while in true application of the method would be exposed to previously unseen chemical families.

The authors did not participate in the discussion, and address concerns. The reviewer consensus is a rejection."
iclr_2019_B1l1b205KX,With an average review score of 4.67 and a short review for the one positive review it is just not possible to accept the paper.
iclr_2019_B1l6e3RcF7,The reviewers agree that the paper needs significantly more work to improve presentation and is not fully empirically and conceptually convincing.
iclr_2019_B1l8iiA9tQ,"Dear authors,

All reviewers pointed out that the proximity with Dropout warranted special treatment and that the justification provided in the paper was not enough to understand why exactly the changes were important. In its current state, this work is not suitable for publication to ICLR.

Should you decide to resubmit this work to another venue, please take the reviewers' comments into account."
iclr_2019_B1l9qsA5KQ,"The manuscript describes a novel technique predicting metal fatigue based on EEG measurements. The work is motivated by an application to driving safety. Reviewers and the AC agreed that the main motivation for the proposed work, and perhaps the results, are likely to be of interest to the applied BCI community. 

The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation and breadth of empirical evaluation. In particular, only a few baselines were considered. As a result, for the non-expert, it is also unclear if the proposed methods are compared against the state of the art. There was also a particular concern that this work may not be a good fit for an ICLR audience.
"
iclr_2019_B1lG42C9Km,"The reviewers raised a number of concerns including the appropriateness of the chosen application and the terms in which social dilemmas have been discussed, the lack of explanations and discussions, missing references, and the extent of the evaluation studies. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the work is interesting and may be useful to the community (though to a small extent., in my opinion). However, the paper would benefit from additional explanations, experiments and discussions pointed in quite some detail by the reviewers. AS is, the paper is below the acceptance threshold for presentation at ICLR."
iclr_2019_B1lKtjA9FQ,The reviewers reached a consensus that the paper is not fit for publication for the moment because a) the paper lacks thorough experiments and b) the criteria provided by the paper are relatively evague (see more details in reviewer 3's comments.）
iclr_2019_B1lXGnRctX,"The paper describes the use of tactile sensors for exploration.  An important topic which has been addressed in various previous publications, but is unsolved to date.

The research and the paper are unfortunately in a raw state.  Rejected unanimously by the reviewers, without rebuttal chances used by the authors."
iclr_2019_B1lfHhR9tm,"This paper presents a new multi-task training and evaluation set up called the Natural Language Decathlon, and evaluates models on it. While this AC is sympathetic to any work which introduces new datasets and evaluation tasks, the reviewers agreed amongst themselves that the paper is not quite ready for publication. The main concern is that multi-task learning should show benefits of transferring representations or other model components between tasks, demonstrating better generalisation and less task-specific overfitting, but that the results in the paper do not properly show this effect. A more thorough study of which tasks ""interact constructively"" and what model changes can properly exploit this needs to be done. With this further work, the AC has no doubt that this dataset and task suite, and associated models, will be very valuable to the NLP community.

I should note that there were some issues during the review period which lead to AC-confidential communication between AC and authors, and AC and reviewers, to be leaked to the reviewers. It was due to an OpenReview bug, and no party is at fault. Through private discussion with the interested parties, we were able to resolve this matter, and through careful examination of the discussion, I am satisfied that the reviews and final recommendations of the reviewers were properly argued for and presented in good faith."
iclr_2019_B1lnjo05Km,"The work presents a method of imposing harmonic structural regularizations to layers of a neural network. While the idea is interesting, the reviewers point out multiple issues.

Pros:
+ Interesting method
+ Hidden layer coherence tends to improve

Cons:
- Deficient comparisons to baselines or context with other works.
- Insufficient assessment of impact to model performance.
- Lack of strategy to select regularizers
- Lack of evaluation on more realistic datasets"
iclr_2019_B1lwSsC5KX,"This paper studies memorization properties of convnets by testing their ability to determine if an image/set of images was used during training or not. The experiments are reported on large-scale datasets using high-capacity networks. 

While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues:
(1) more formal justifications are required to assess the scope and significance of this work contributions -- see very detailed comments by R2 about measuring networks capacity to memorize and the role of network weights and depth as studied in MacKay,2002. In their response the authors acknowledged they didn’t take into account network weights and depth but strived at an empirical evaluation scenario. 
(2) writing and presentation clarity of the paper could be substantially improved – see very detailed comments by R3 and also R2; 
(3) empirical evaluations and effect of the negative set used for training are not well explained and analysed (R2, R3).

AC can confirm that all three reviewers have read the author responses and have contributed to the final discussion.
AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.
"
iclr_2019_B1lx42A9Ym,"This paper introduced a Neural Rendering Model, whose inference calculation corresponded to those in a CNN. It derived losses for both supervised and unsupervised learning settings. Furthermore, the paper introduced Max-Min network derived from the proposed loss, and showed strong performance on semi-supervised learning tasks.

All reviewers agreed this paper introduces a highly interesting research direction and could be very useful for probabilistic inference. However, all reviewers found this paper hard to follow. It was written in an overly condensed way and tried to explain several concepts within the page limit such as NRM, rendering path, max-min network. In the end, it was not able to explain key concepts sufficiently.

I suggest the authors take a major revision on the paper writing and give a better explanation about main components of the proposed method. The reviewer also suggested splitting the paper into two conference submissions in order to explain the main ideas sufficiently under a conference page limit."
iclr_2019_B1lxH20qtX,"Strengths: A co-evolution of body connectivity and its topology mimicing control policy is presented.

Weaknesses: Reviewers found the paper to be lacking in detail. The importance of message passing in achieving the given results is clear on one example but not some others. Some reviewers had questions regarding the baseline comparisons.
The authors provided lengthy details in responses on the discussion board, but reviewers likely had limited time to fully reread the many changes that were listed.
AC:  The physics in the motions shown in the video require signficant further explanation. It looks like the ball joints can directly attach themselves to the ground, and make that link stand up. Thus it seems that the robots are not underactuated and can effectively grab arbitrary points in the environment. Also it is strange to see the robot parts dynamically fly together as if attracted by a magnet.  The physics needs significant further explanation.

Points of Contention: The R2 review is positive on the paper (7), with a moderate confidence (3).
R1 contributed additional questions during the discussion, but R2 and R3 were silent.

The AC further examined  the submission (paper and video). 
The reviewers and the AC are in consensus regarding
the many details that are behind the system that are still not understood.  The AC is also skeptical
of the non-physical nature of the motion, or the unspecified behavior of fully-actuated contacts
with the ground.
"
iclr_2019_B1x-LjAcKX,This paper proposes a new training approach for deep neural interfaces. The idea is to bootstrap from critics of other layers instead of using the final loss as target. The method is evaluated of CIFAR-10 and CIFAR-100 and found to improve performance slightly upon Sobolev training while being simpler. The reviewers found the idea interesting but were concerned about the strength of the experimental results. The datasets are similar and the significance of the results is not clear. The revision submitted by the authors was only able to address some of these issues such as the evaluation protocol.
iclr_2019_B1x0enCcK7,"The paper presents a novel problem formulation, that of generating 3D object shapes based on their functionality. They use a dataset of 3d shapes annotated with functionalities to learn a voxel generative network that conditions on the desired functionality to generate a voxel occupancy grid. However, the fact that the results are not very convincing -resulting 3D shapes are very coarse- raises questions regarding the usefulness of the proposed problem formulation. 
Thus, the problem formulation novelty alone is not enough for acceptance. Combined with a motivating application to demonstrate the usefulness of the problem formulation and results, would make this paper a much stronger submission. Furthermore, the authors have greatly improved the writing of the manuscript during the discussion phase."
iclr_2019_B1x33sC9KQ,The paper describes a clipping method to improve the performance of quantization. The reviewers have a consensus on rejection due to the contribution is not significant. 
iclr_2019_B1x5KiCcFX,"This paper provides a theoretical analysis of GANs, showing its advantages when the measure satisfies the disconnected support property. Its main theoretical results are interesting, but the reviews and discussion shows some misleading places.  It was also found some of the claims and proof are not mathematically rigorous. "
iclr_2019_B1x9siCcYQ,The paper can also improved thorough a more thorough evaluation. 
iclr_2019_B1xFVhActm,"This paper presents a new unsupervised training objective for sentence-to-vector encoding, and shows that it produces representations that often work slightly better than those produced by some prominent earlier work.

The reviewers have some concerns about presentation, but the main issue—which all three reviewers pointed to—was the lack of strong recent baselines. Sentence-to-vector representation learning is a fairly active field with an accepted approach to evaluation, and this paper seems to omit conspicuous promising baselines. This includes labeled-data pretraining methods which are known to work well for English (including results from the cited Conneau paper)—while these may be difficult to generalize beyond English, this paper does not attempt such a generalization. This also includes more recent unlabeled-data methods like ULMFiT or Radford et al.'s Transformer which could be easily trained on the same sources of data used here. The authors argue in the comments that these language models tend to use more parameters, but these additional parameters are only used during pretraining, so I don't find this objection compelling enough to warrant leaving out baselines of this kind. Baselines of both kinds have been known for at least a year and come with distributed models and code for close comparison."
iclr_2019_B1xFhiC9Y7,"The paper explores unsupervised domain adaptation when the output is structured. Here they focus experimentally on semantic segmentation in driving scenes and use the spatial structure of the scene to produce two losses for adaptation: one global and one patch based. The method tackles an important problem and proposes a first attempt at a new solution. While the the experiments are missing ablations and some comparisons to prior work as noted by the reviewers, the authors have provided comments in their rebuttal explaining the relation to the prior work and promising to include more in the revised manuscript. 

The paper is borderline, but falls short on the necessary updates requested by reviewers.  The use of the structured output which is available in semantic segmentation of driving scenes is a useful direction. The paper is missing enough key results and analysis in it's current form to be accepted. "
iclr_2019_B1xFxh0cKX,"This paper proposes a “guided” evolution strategy method where the past surrogate gradients are used to construct a covariance matrix from which future perturbations are sampled. The bias-variance tradeoff is analyzed and the method is applied to real-world examples.

The method is not entirely new, and discussion of related work as well as comparison with them is missing. The main contribution is in the analysis and application to real-world examples, and the paper should be rewritten focusing on these contributions, while discussing existing work on this topic thoroughly.

Due to these issue, I recommend to reject this paper.
"
iclr_2019_B1xOYoA5tQ,"This paper proposes a method for improving robustness to black-box adversarial attacks by replacing the cross-entropy layer with an output vector encoding scheme. The paper is well-written, and the approach appears to be novel. However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks. Reviewer 1 echoes many of these concerns."
iclr_2019_B1xU4nAqK7,"Strengths

The paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling)
approach to learning the state transition function. The paper is clearly written.

Weaknesses

All reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation,
and aspects of the paper are difficult to follow or are sparse on details.
No revisions have been posted.

Summary

All reviewers are in agreement that the paper requires significant work and that it is not ready for ICLR publication.
"
iclr_2019_B1xeyhCctQ,"The work presents a method to back propagate and visualize bias distribution in network as a form of explainability of network decisions. Reviewers unanimous reject, no rebuttal from authors. "
iclr_2019_B1xnPsA5KX,"This paper presents a probabilistic programming language where models are constructed out of building blocks which specify both the distribution and an inference procedure. As a demonstration, they show how a GP-LVM can be implemented.

The paper spends a lot of space arguing for the benefits of modularity. Modularity is of course hard to argue with, and the benefits are already understood in the PPL community. But, as the reviewers point out, various other PPLs have already adopted various strategies to enable modular definition of models, and (in cases like Venture) special-purpose higher-level inference algorithms. This paper contains little discussion of other PPLs and how the specific design decisions relate to theirs, so it's hard to judge whether this paper really covers new ground. Such discussion wasn't added to the revised paper, even though multiple reviewers asked for it. I can't recommend acceptance.
"
iclr_2019_B1zMDjAqKQ,"The submission introduces a model that does learning of multisensory representations (by predicting one from the other), with an autoencoder structure. Generally, the reviewers liked the overall idea of the work, but found the clarity lacking, the evaluation insufficient (and not particularly state of the art), the requirement for paired training data quite limiting and the choices (VAE sometimes, autoencoder other times) somewhat ad hoc."
iclr_2019_BJ4AFsRcFQ,"The reviewers and this AC agree that the paper is not of acceptable form due to several issues: (1) limited novelty, (2) limited/unclear experimental validation, and (3) presentation issues."
iclr_2019_BJ4BVhRcYX,"The current version of the paper receives a unanimous rejection from reviewers, as the final proposal. "
iclr_2019_BJEOOsCqKm,"This paper focuses on the problem of detecting visual anomalies within textures. For that purpose, the authors consider several parametric texture models and train anomaly detection models on the corresponding outputs. 

Reviewers were generally positive about the topic under study, but were unanimous in signaling a severe weaknesses in the experimental setup. In particular, in R2 words, ""my main concern is that the performance evaluation is not suitable to achieve meaningful results"", and ""showing quantitative results from only two textures does not feel like a very comprehensive analysis"". Moreover, the authors did not respond to reviewers feedback. Therefore, the AC recommends rejection at this time."
iclr_2019_BJGVX3CqYm,"The paper proposes a quantization framework that learns a different bit width per layer.  It is based on a differentiable objective where the Gumbel softmax approach is used with an annealing procedure.  The objective trades off accuracy and model size.

The reviewers generally thought the idea has merit.  Quoting from discussion comments (R4): ""The paper cited by AnonReviewer 3 is indeed close to the current submission, but in my opinion the strongest contribution of this paper is the formulation from architecture search perspective.""
The approach is general, and seems to be reasonably efficient (ResNet 18 took ""less than 5 hours"")

The main negatives are the comparison to other methods.  In the rebuttal, the authors suggested in multiple places that they would update the submission with additional experiments in response to reviewer comments.  As of the decision deadline, these experiments do not appear to have been added to the document.
In the discussion: R4: ""This paper seems novel enough to me, but I agree that the prior work should at least be cited and compared to. This is a general weakness in the paper, the comparison to relevant prior works is not sufficient."" R3: ""Not only novel, but more general han the prior work mentioned, but the discussion / experiments do not seem to capture this.""

With a range of scores around the borderline threshold for acceptance at ICLR, this is a difficult case.  On the balance, it appears that shortcomings in the experimental results are not resolved in time for ICLR 2019.  The missing results include ablation studies (promised to R4) and a comparison to DARTS (promised to R3): ""We plan to perform the suggested experiments of comparing with exhaustive search and DARTS. The results will be hopefully updated before the revision deadline and the camera-ready if the paper is accepted."" These results are not present and could not be evaluated during the review/discussion phase."
iclr_2019_BJG__i0qF7,"Strengths: Execution of paper well received. Results on new dataset. Convincing demonstration that the proposed approach learns good semantic representations.

Weaknesses: Reviewers felt the positioning with prior work was not as strong as it could be. Reviewers wanted to have seen an ablation study.

Contention: Some general agreement among both the one positive reviewer and negative reviewer that the representation of prior work is skewed.

Consensus: With two 5s and one 6 the numerical average of 5.33 is representative of the aggregated consensus opinion which is that the work is just below threshold in its current form."
iclr_2019_BJGfCjA5FX," The paper proposes an augmented adversarial reconstruction loss for training a stochastic encoder-decoder architecture. It corresponds to a discriminator loss distinguishing between a pair of a sample from the data distribution and its augmentation and pair containing the sample and its reconstruction. The introduction of the augmentation function is an interesting idea, intensively tested in a set of experiments, but, as two of the reviewers pointed out, the paper could be improved by deeper investigation of the augmentation function and the way of choosing it, which would increase significance of the contribution. "
iclr_2019_BJGjOi09t7,"The paper introduces a variant of the variational autoencoder (VAE) for probabilistic non-negative matrix factorization. The main idea is to use a Weibull distribution in the latent space. There is agreement among the reviewers that the paper is technically sound and well written, but that it lacks in motivation and demonstration of utility of the proposed method.
All the reviewers think the approach is not particularly novel and somewhat incremental. The main issue is that the empirical evaluation of the algorithm is also quite limited. Specifically, it should have been compared with Bayesian NMF. Many papers have addressed Bayesian NMF with variational inference (Cemgil; Fevotte & Dikmen; Hoffman, Blei & Cook) like in VAE. Experimentally, Bayesian NMF and the proposed PAE-NMF could easily be quantitatively compared on matrix completion tasks. Overall, there was consensus among the reviewers that the paper is not ready for publication.
"
iclr_2019_BJMvBjC5YQ,"This paper proposed a method to reduce the memory of training neural nets, in exchange for additional training time. The paper is simple and looks reasonable. It's a natural followup with previous work.  The improvement over previous work is not significant, with some overhead incurred in training time. This is a borderline paper but given the <30% acceptance rate, I need to downgrade the paper to reject. "
iclr_2019_BJWfW2C9Y7,"Dear authors,

All reviewers pointed to severe issues with the analysis, making the paper unsuitable for publication to ICLR. Please take their comments into account should you decide to resubmit this work."
iclr_2019_BJe-Sn0ctm,"The paper presents a new neural program synthesis architecture, SAPS, which seems to produce accuracy improvements in some synthesis tasks. The reviewer consensus, even after discussion with the authors, was that the paper is not acceptable at the conference. Two concerns emerge during discussion, even considering the authors efforts to improve the paper. First, the system seems to have many ""moving parts"", but there is a lack of rigorous ablation studies to demonstrate which components of the system (or combination thereof) make significant contributions to the results. I agree with this assessment: it is not sufficient to demonstrate increased scores, even if the experimental protocol and clear and sound (more on this later), but there must be some evidence as to why this increase happens, both in the discussion and in the empirical segment of the paper, by conducting a thorough ablation study. Second, all reviewers had issues with proper and fair comparison with prior work, with the consensus being that the model is not adequately compared to convincing benchmarks in the paper.

The results of the paper sound like there is something promising going on, but the need for a clear presentation of what is the driving factor behind any improvement is not only a superficial stylistic requirement, but a key tenet of proper scholarship. This is one front on which the paper fails to make a successful case for the work and methods it describes, and unfortunately is not ready for publication at this time (despite having a cool title)."
iclr_2019_BJe1hsCcYQ,"Dear authors,

The reviewers all appreciated the treatment of the topic and the quality of the writing. It is rare for all reviewers to agree on this, congratulations.

However, all reviewers also felt that the paper could have gone further in its analysis. In particular, they noticed that quite a few points were either mentioned in recent papers or lacked an experimental validation.

Given the reviews, I strongly encourage the authors to expand on their findings and submit the improved work to a future conference."
iclr_2019_BJeRg205Fm,"This paper proposes to quantify the uncertainty of neural network models with Beta, Dirichlet and Dirichlet-Multinomial likelihood. This paper is clearly written with a sound main idea. However, it is a common practice to model different types of data with different likelihood, although the proposed distributions are not usually used for network output. All the reviewers therefore considered this paper to be of limited novelty. Reviewer 2 also had a concern about the mixed experimental results of the proposed method.

Reviewer 3 raised the concern that this paper did not model the uncertainty of prediction from the uncertainty of the model parameters. It is a common consideration in a Bayesian approach and I encourage the authors to discussed different sources of uncertainty in future revisions."
iclr_2019_BJeWOi09FQ,The paper addresses the problem semantic segmentation using a sequential patch-based model. I agree with the reviewers that the contributions of the paper are not enough for a machine learning venue: (1) there has been prior work on using sequence models for segmentation and (2) the complexity of the proposed approach is not fully justified. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account and improve the paper.
iclr_2019_BJeY6sR9KX,"This work provides two contributions: 1) Brain-Score, that quantifies how a given network's responses compare to responses from natural systems; 2) CORnet-S, an architecture trained to optimize Brain-Score, that performs well on Imagenet.
As noted by all reviewers, this work is interesting and shows a promising approach to quantifying how brain-like an architecture is, with the limitations inherent to the fact that there is a lot about natural visual processing that we don't fully understand. However, the work here starts from the premise that being more similar to current metrics of brain processes is by itself a good thing -- without a better understanding of what features of brain processing are responsible for good performance and which are mere by-products, this premise is not one that would appeal to most of ICLR audience. In fact, the best performing architectures on imagenet are not the best scoring for Brain-Score. Overall, this work is quite intriguing and well presented, but as pointed out by some reviewers, requires a ""leap of faith"" in matching signatures of brain processes that most of the ICLR audience is unlikely to be willing to take."
iclr_2019_BJeapjA5FX,"All three reviewers feel that the paper needs to provide more convincing results to support their robustness claim, in addition to a number of other issues that need to be clarified/improved. The authors did not provide any response. "
iclr_2019_BJeem3C9F7,"This paper proposes an approach for learning to generate 3D views, using a surfel-based representation, trained entirely from 2D images.  After the discussion phase, reviewers rate the paper close to the acceptance threshold.

AnonReviewer3, who initially stated ""My second concern is the results are all on synthetic data, and most shapes are very simple"", remains concerned after the rebuttal, stating ""all results are on synthetic, simple scenes. In particular, these synthetic scenes don't have lighting, material, and texture variations, making them considerably easier than any types of real images.""

The AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper.  Particularly relevant is the criticism that ""While the paper is called ‘pix2scene’, it’s really about ‘pix2object’ or ‘pix2shape’.""
"
iclr_2019_BJesDsA9t7,"Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. A significant concern is that the definition of privacy used here is not adequately justified. This opens up issues of: 1) possible attacks, 2) privacy-guarantees that are not worst-case, among others. "
iclr_2019_BJfRpoA9YX,"The paper proposes a supervised adversarial method for disentangling the latent space of a VAE into two groups: latents z which are independent of the given attribute y, and \hat{y} which contains information about y. Since the encoder also predicts \hat{y} it can be used for classification and the paper shows competitive results on this task, apart from the attribute manipulation task. Reviewers had raised points about model complexity and connections to prior works which the authors have addressed and the paper is on the borderline based on the scores. 

Though none of the reviewers explicitly pointed out the similarity of the paper with Fader networks (Lample et al., 2017), the adversarial setup for getting attribute invariant 'z' is exactly same as in Fader networks, as also pointed out in an anonymous comment. The only difference is that encoder in the current paper also predicts the attribute itself (\hat{y}), which is not the case in Fader n/w, and hence the encoder can be used as a classifier as well (authors have also mentioned and discussed this difference in their response). However, the core idea of the paper as outlined in the title of the paper, ie, using adversarial loss for information factorization, is very similar to this earlier work, which diminishes the originality of the work. 

With the borderline review scores, the paper can go in either of the half-spaces (accept/reject) but I am hesitant to recommend an ""accept"" due to limited originality of the approach. However, if there is space in the program, the paper can be accepted. "
iclr_2019_BJf_YjCqYX," The paper addresses an important problem of detecting biases in classifiers (e.g. in face detection), using simulation tools with Bayesian parameter search. While the direction of research and the presented approach seem to be practically useful, there were several concerns raised by the reviewers regarding strengthening the results (e.g., beyond single avatar, etc), and suggestions on possibly a more applied conference as a better venue.  While thourough rebuttals by the authors addressed some of these concerns, which increased some ratings, overall, the paper was still in the borderline range. We hope the suggestions and comments of the reviewers can help to improve the paper.

 
 "
iclr_2019_BJfguoAcFm,"This work propose a method for learning a Kolmogorov model,  which is a binary random variable model that is very similar (or identical) to a matrix factorization model. The work proposes an alternative optimization approach that is again similar to matrix factorization approaches.  Unfortunately, no discussion or experiments are made to compare the  proposed problem and method with standard matrix factorization; without such comparison, it is unclear if the proposed is substantially new, or a reformation of a standard problem. The authors are encouraged to improve the draft to clarify the connection matrix factorization and standard factor models. "
iclr_2019_BJfvAoC9YQ,"The paper proposes a framework for continual/lifelong learning that has potential to overcome the problems of catastrophic forgetting and data privacy. 
R1, R2 and AC agree that the proposed method is not suitable for lifelong learning in its current state as it linearly increases memory and computational cost over time (for storing features of all points in the past and increasing model capacity with new tasks) without account for budget constraints.

The authors responded in their rebuttal that the data is not stored in the original form, but using feature representation (which is important for privacy issues). The main concern, however, was about the fact that one has to store information about all previous data points which is not feasible in lifelong learning. In the revision the authors have tried to address some of the R1’s and R2’s suggestion about taking into account the budget constraints. However more in-depth analysis is required to assess feasibility and advantage of the proposed approach. 
The authors motivate some of the key elements in their model as to protect privacy. However no actual study was conducted to show that this has been achieved. 
The comments from R3 were too brief and did not have a substantial impact on the decision.

In conclusion, AC suggests that the authors prepare a major revision addressing suitability of the proposed approach for continual learning under budget constraints and for privacy preservation and resubmit for another round of reviews.  
"
iclr_2019_BJfvknCqFQ,"This paper demonstrated interesting observations that simple transformations such as a rotation and a translation is enough to fool CNNs. Major concern of the paper is the novelty. Similar ideas have been proposed before by many previous researchers. Other networks trying to address this issue have been proposed. Such as those rotation-invariant neural networks. The grid search attack used in the experiments may be not convincing. Overall, this paper is not ready for publication."
iclr_2019_BJgEjiRqYX,"The paper proposes a generative model that generates one object at a time, and uses a relational network to encode cross-object relationships. Similar  object-centric generation and object-object relational network  is proposed in ""sequential attend, infer, repeat"" of Kosiorek et al. for video generation, which first appeared on arxiv on June 5th 2018 and was officially accepted in NIPS 2018 before the submission deadline for ICLR 2019. Moreover, several recent generative models have been proposed that consider object-centric biases,  which the current paper references  but does not compare against, e.g., 'attend, infer, repeat' of Eslami et al., or ""DRAW: A Recurrent Neural Network For Image Generation"" of Gregor et al. . The CLEVR dataset considered, though it contains real images, the intrinsic image complexity is low because it features a small number of objects against table background. As a result, the novelty of the proposed work may not be sufficient in light of recent literature, despite the fact that the paper presents a reasonable and interesting approach for image generation. 
"
iclr_2019_BJgGhiR5KX,"Pros:

- A new framework for learning sentence representations
- Solid experiments and analyses
- En-Zh / XNLI dataset was added, addressing the comment that no distant languages were considered; also ablation tests

Cons:

-  The considered components are not novel, and their combination is straightforward
-  The set of downstream tasks is not very diverse (See R2)
-  Only high resource languages are considered (interesting to see it applied to real low resource languages)

All reviewers agree that there is no modeling contribution.  Overall, it is a solid paper but I do not believe that the contribution is sufficient. "
iclr_2019_BJgQB20qFQ,"This paper provides a new approach for progressive planning on discrete state and action spaces. The authors use LSTM architectures to iteratively select and improve local segments of an existing plan. They formulate the rewriting task as a reinforcement learning problem where the action space is the application of a set of possible rewriting rules. These models are then evaluated on a simulated job scheduling dataset and Halide expression simplification. This is an interesting paper dealing with an important problem. The proposed solution based on combining several existing pieces is novel. On the negative side, the reviewers thought the writing could be improved, and the main ideas are not explained clearly. Furthermore, the experimental evaluation is weak."
iclr_2019_BJgTZ3C5FX,"This method proposes a primal approach to minimizing Wasserstein distance for generative models. It estimates WD by computing the exact WD between empirical distributions.

As the reviewers point out, the primal approach has been studied by other papers (which this submission doesn't cite, even in the revision), and suffers from a well-known problem of high variance. The authors have not responded to key criticisms of the reviewers. I don't think this work is ready for publication in ICLR.
"
iclr_2019_BJgYl205tQ,"The paper propose a new metric for the evaluation of generative models, which they call CrossLID and which assesses the local intrinsic dimensionality (LID) of input data with respect to neighborhoods within generated samples, i.e. which is based on nearest neighbor distances between samples from the real data distribution and the generator. The paper is clearly written and provides an extensive experimental analysis, that shows that LID is an interesting metric to use in addition to exciting metrics as FID, at least for the case of not to complex image distributions The paper would  be streghten by showing that the metric can also be applied in those more complex settings. 
"
iclr_2019_BJgbzhC5Ym,"This paper proposes a principled solution to the problem of joint source-channel coding. The reviewers find the perspectives put forward in the paper refreshing and that the paper is well written. The background and motivation is explained really well.

However, reviewers found the paper limited in terms of modeling choices and evaluation methodology. One major flaw is that the experiments are limited to unrealistic datasets, and does not evaluate the method on a realistic benchmarks. It is also questioned whether the error-correcting aspect is practically relevant.


 "
iclr_2019_BJgnmhA5KQ,"+ a simple method
+ producing diverse translation is an important problem 

- technical contribution is limited / work is incremental
- R1 finds writing not precise and claims not supported,  also discussion of related work is considered weak by R3
- claims of modeling uncertainty are not well supported


There is no consensus among reviewers.  R4 provides detailed arguments why (at the very least) certain aspects of presentations are misleading (e.g., claiming that a uniform prior promotes diversity). R1 is also negative, his main concerns are limited contribution and he also questions the task (from their perspective  producing diverse translation is not a valid task; I would disagree with this).  R2 likes the paper and believes it is interesting, simple to use and the paper should be accepted. R3 is more lukewarm. 

"
iclr_2019_BJgolhR9Km,The paper presents a novel unit making the networks intrinsically more robust to gradient-based adversarial attacks. The authors have addressed some concerns of the reviewers (e.g. regarding pseudo-gradient attacks) but experimental section could benefit from a larger scale evaluation (e.g. Imagenet).
iclr_2019_BJgsN3R9Km,"The submission proposes a method that combines sparsification and low rank projections to compress a neural network.  This is in line with nearly all state-of-the-art methods.  The specific combination proposed in this instance are SVD for low-rank and localized group projections (LGP) for sparsity.

The main concern about the paper is the lack of stronger comparison to sota compression techniques.  The authors justify their choice in the rebuttal, but ultimately only compare to relatively straightforward baselines.  The additional comparison with e.g. Table 6 of the appendix does not give sufficient information to replicate or to know how the reduction in parameters was achieved.

The scores for this paper were  borderline, and the reviewers were largely in consensus with their scores and the points raised in the reviews.  Given the highly selective nature of ICLR, the overall evaluations and remaining questions about the paper and comparison to baselines indicates that it does not pass the threshold for acceptance."
iclr_2019_BJgvg30ctX,"This paper proposes an approach to regularizing classifiers based on invertible networks using concepts from the information bottleneck theory. Because mutual information is invariant under invertible maps, the regularizer only considers the latent representation produced by the last hidden layer in the network and the network parameters that transform that representation into a classification decision. This leads to a combined ℓ1 regularization on the final weights, W, and ℓ2 regularization on W^{T} F(x), where F(x) is the latent representation produced by the last hidden layer. Experiments on CIFAR-100 image classification show that the proposed regularization can improve test performance. The reviewers liked the theoretical analysis, especially proposition 2.1 and its proof, but even after discussion and revision wanted a more careful empirical comparison to established forms of regularization to establish that the proposed approach has practical merit. The authors are encouraged to continue this line of research, building on the fruitful discussions they had with the reviewers."
iclr_2019_BJgy-n0cK7,"Strengths:
Paper uses an efficient inference procedure cutting inference time on intermediate frames by 53%, & yields better accuracy and IOU compared to the one recent closely related work.

The ablation study seems sufficient and well-designed. The paper presents two feature propagation strategies and three feature fusion methods. The experiments compare these different settings, and show that interpolation-BMV is indeed a better feature propagation.

Weaknesses: Reviewers believed the work to be of limited novelty. The algorithm is close to the optical-flow based models Shelhamer et al. (2016) and Zhu et al. (2017). Reviewer asserts that the main difference is that the optical-flow is replaced with BMV, which is a byproduct of modern cameras.  R3 felt that there was Insufficient experimental comparison with other baselines and that technical details were not clear enough.

Contention: Authors assert that Shelhamer et al. (2016) does not use optical flow, and instead simply copies features from frame to frame (and schedules this copying). Zhu et al. (2017) then proposes an improvement to this scheme, forward feature warping with optical flow. In general, both these techniques fail to achieve speedups beyond small multiples of the baseline (< 3x), without impacting accuracy.

Consensus: It was disappointing that some of the reviewers did not engage after the author review (perhaps initial impressions were just too low). However, after the author rebuttal R1 did respond and held to the position that the work should not be accepted, justified by the assertion that other modern architectures that are lighter weight and are able to produce fast predictions. 
"
iclr_2019_BJl4f2A5tQ,"The paper addresses questions on the relationship between model-free and model-based reinforcement learning, in particular focusing on planning using learned generative models. The proposed approach, GATS, uses learned generative models for rollouts in MCTS, and provide theoretical insights that show a favorable bias-variance tradeoff. Despite this theoretical advantage, and high-quality models, the proposed approach fails to perform well empirically. This surprising negative results motivates the paper and providing insights on it is the main contribution.

Based on the initial submitted version, the reviewers positively emphasized the need to understand and publish important negative results. All reviewers and the AC appreciate the import role that such a contribution can bring to the research community. Reviewers also note the careful discussion of modeling choices for the generative models. 

The reviewers also noted several potential weaknesses. Central were the need to better motivate and investigate the hypothesis proposed to explain the negative results. Several avenues towards a better understanding were proposed, and many of these were picked up by the authors in the revision and rebuttal. A novel toy domain ""goldfish and gold bucket"" was introduced for empirical analysis, and experiments there show that GATS can outperform DQN when a longer planning horizon is used. 

The introduced toy domain provides additional insights into the relationship between planning horizon and GATS / MCTS performance. However, it does not address key questions around why the negative result is maintained. The authors hypothesize that the Q-value is less accurate in the GATS setting - this is something that can be empirically evaluated, but specific evidence for this hypothesis is not clearly shown. Other forms of analysis that could shed further light on why the specific negative result occurs could be to inspect model errors. For example, if generated frames are sorted by the magnitude of prediction errors - what are the largest mistakes? Could these cause learning performance to deteriorate?

The reviewers also raised several issues around the theoretical analysis, clarity (especially of captions) and structure - these were largely addressed by the revision. The concern that most strongly affected the final evaluation is the limited insight (and evidence) of the factors that influence performance of the proposed approach. Due to this, the consensus is to not accept the paper for publication at ICLR at this stage."
iclr_2019_BJl65sA9tm,"This paper proposes a variant of GAIL that can learn from both expert and non-expert demonstrations. The paper is generally well-written, and the general topic is of interest to the ICLR community. Further, the empirical comparisons provide some interesting insights. However, the reviewers are concerned that the conceptual contribution is quite small, and that the relatively small conceptual contribution also does not lead to large empirical gains. As such, the paper does not meet the bar for publication at ICLR."
iclr_2019_BJlMcjC5K7,"There is a clear reviewer consensus to reject this paper so I am also recommending rejecting it. The paper is about an interesting and underused technique. However, ultimately the issue here is that the paper does not do a good enough job of explaining the contribution. I hope the reviews have given the authors some ideas on how to frame and sell this work better in the future.

For instance, from my own reading of the abstract, I do not understand what this paper is trying to do and why it is valuable. Phrases such as ""we exploit the sparsity"" do not tell me why the paper is important to read or what it accomplishes, only how it accomplishes the seemingly elided contribution. I am forced to make assumptions that might not be correct about the goals and motivation. It is certainly true that the implicit one-hot representation of words most common in neural language models is not the only possibility and that random sparse vectors for words will also work reasonably well. I have even tried techniques like this myself, personally, in language modeling experiments and I believe others have as well, although I do not have a nice reference close to hand (some of the various Mikolov models use random hashing of n-grams and I believe related ideas are common in the maxent LM literature and elsewhere). So when the abstract says things like ""We show that guaranteeing approximately equidistant vector representations for unique discrete inputs is enough to provide the neural network model with enough information to learn"" my immediate reaction is to ask why this would be surprising or why it would matter. Based on the reviews, I believe these sorts of issues affect other parts of the manuscript as well. There needs to be a sharper argument that either presents a problem and its solution or presents a scientific question and its answer. In the first case, the problem should be well motivated and in the second case the question should not yet have been adequately answered by previous work and should be non-obvious. I should not have to read beyond the abstract to understand the accomplishments of this work.

Moving to the conclusion and future work section, I can see the appeal of the future work in the second paragraph, but this work has not been done. The first paragraph is about how it is possible to use random projections to represent words, which is not something I think most researchers would question. Missing is a clear demonstration of the potential advantages of doing so.
"
iclr_2019_BJlSHsAcK7,"The authors propose an approach for continual learning of a sequence of tasks which augments the network with task-specific neurons which encode 'adversarial subspaces' and prevent interference and forgetting when new tasks are being learnt. The approach is novel and seems to work relatively well on a simple sequence of MNIST or CIFAR10 classes, and has certain advantages, such as not requiring any stored data. However, the reviewers agreed that the presentation of the method is quite confusing and that the paper does not provide adequate intuition, visualisation, or explanation of the claim that they are preventing forgetting through the intersection of adversarial subspaces. Moreover, there was a concern that the baselines were not strong enough to validate the approach."
iclr_2019_BJlVhsA5KX,This paper proposes a new batching strategy for training deep nets. The idea is to have the properties of sampling with replacement while reducing the chance of not touching an example in a given epoch. Experimental results show that this can improve performance on one of the tasks considered. However the reviewers consistently agree that the experimental validation of this work is much too limited. Furthermore the motivation for the approach should be more clearly established.
iclr_2019_BJlXUsR5KQ,"This paper proposes to automatically learn the form of the non-linearities of neural networks in deep neural networks, which the reviewers noted to be an interesting albeit significantly studied direction.   Overall, this paper falls just below the bar, with no reviewer really willing to champion for acceptance.  Reviewer 3 found the paper to be marginally above the acceptance threshold and found the insights provided in the paper (in Section 2) to be a neat and strong contribution.  Reviewers 1-2, however, found the paper marginally below the bar and seemed confused by the presentation of the paper.  They seemed to believe in the motivation and idea, but they found the paper hard to follow and not particularly clearly written.  It would seem that the paper could significantly benefit from careful editing and restructuring to disambiguate contributions from motivation and existing literature.  Also, the authors should provide clear justification for their design choices and modeling assumptions. "
iclr_2019_BJl_VnR9Km,"There was major disagreement between reviewers on this paper. Two reviewers recommend acceptance, and one firm rejection. The initial version of the manuscript was of poor quality in terms of exposition, as noted by all reviewers. However, the authors responded carefully and thoroughly to reviewer comments, and major clarity and technical issues were resolved by all authors. 

I ask PCs to note that the paper, as originally submitted, was not fit for acceptance, and reviewers noted major changes during the review process. I do believe this behavior should be discouraged, since it effectively requires reviewers to examine the paper twice. Regardless, the final overall score of the paper does not meet the bar for acceptance into ICLR."
iclr_2019_BJlc6iA5YX,The reviewers have agreed this work is not ready for publication at ICLR.
iclr_2019_BJleciCcKQ,"In this work, the authors conduct experiments using variants of RNNs and Gated CNNs on a speech recognition task, motivated by the goal of reducing the computational requirements when deploying these models on mobile devices.
While this is an important concern for practical deployment of ASR systems, the main concerns expressed by the reviewers is that the work lacks novelty. Further, the authors choice to investigate CTC based systems which predict characters. These models are not state-of-the-art for ASR, and as such it is hard to judge the impact of this work on a state-of-the-art embedded ASR system. Finally, it would be beneficial to replicate results on a much larger corpus such as Librispeech or Switchboard. Based on the unanimous decision from the reviewers, the AC agrees that the work, in the present form, should be rejected.
"
iclr_2019_BJll6o09tm,"This paper proposes a simple modification of the Adam optimizer, introducing a hyper-parameter 'p' (with value in the range [0,1/2]) parameterizing the parameter update:
theta_new = theta_old + m/v^p
where p=1/2 falls back to the standard Adam/Amsgrad optimizer, and p=0 falls back to a variant of SGD with momentum. 

The authors motivate the method by pointing out that:
 - Through the value of 'p', one can interpolate between SGD with momentum and Adam/Amsgrad. By choosing a value of 'p' smaller than 0.5, one can therefore use perform optimization that is 'partially adaptive'.
 - The method shows good empirical performance.
 
The paper contains an inaccuracy, which we hope will be solved before the final version. The authors argue that the 1/sqrt(v) term in Adam results in a lower learning rate, and the authors argue that the effective learning rate ""easily explodes"" (section 3) because of this term, and that a ""more aggressive"" learning rate is more appropriate. This last point is false; the value of 1/sqrt(v) can be smaller or larger than 1 depending on the value of 'v', and that a decrease in value of 'p' can result in either an increase or decrease in effective learning rate, depending on the value of v. The value of 'v' is a function of the scale of loss function, which can really be arbitrary. (In case of very high-dimensional predictions, for example, the scale of the loss function is often proportional with the dimensionality of variable to be modeled, which can be arbitrarily large, e.g. in image or video modeling the loss function tends to be of a much larger scale than with classification.)

The authors promise to include a comparison to AdamW [Loshchilov, 2017] that includes tuning of the weight decay parameter. The lack of this experiments makes it more difficult to make a conclusion regarding the performance relative to AdamW. However, the methods offer potentially orthogonal (and combinable) advantages.

[Loshchilov, 2017] https://arxiv.org/pdf/1711.05101.pdf
"
iclr_2019_BJlpCsC5Km,"The paper proposes to define the GAN discriminator as an explicit function of a invertible generator density and a structured Gibbs distribution to tackle the problems of spurious modes and mode collapse. The resulting model is similar to R2P2, i.e. it can be seen as adding an adversarial component to R2P2, and shows competitive (but no better) performance. Reviewers agree, that these limits the novelty of the contribution, and that the paper would be improved by a more extensive empirical evaluation. "
iclr_2019_BJlyznAcFm,"The paper presents a novel architecture, reminescent of mixtures-of-experts,
composed of a set of advocates networks providing an attention map to a
separate ""judge"" network. Reviewers have several concerns, including lack
of theoretical justification, potential scaling limitations, and weak
experimental results. Authors answered to several of the concerns, which did
not convinced reviewers. The reviewer with the highest score was also the least
confident, so overall I will recommend to reject the paper."
iclr_2019_BJx1SsAcYQ,"This paper proposes methods to improve the performance of the low-precision neural networks. The reviewers raised concern about lack of novelty. Due to insufficient technical contribution, recommend for rejection. "
iclr_2019_BJx9f305t7,"The paper introduces a W2GAN method for training GAN by minimizing 2-Wasserstein distance using 
by computing an optimal transport (OT) map between distributions. However, the difference of previous works  is not significant or clearly clarified as pointed out some of the reviewers. The advantage of W2GAN over standard WGAN is also superficially explained, and did not supported by strong empirical evidence. "
iclr_2019_BJxLH2AcYX,"The paper proposes the unique setting of adapting to multiple target domains. The idea being that their approach may leverage commonality across domains to improve adaptation while maintaining domain specific parameters where needed. This idea and general approach is interesting and worth exploring. The authors' rebuttal and paper edits significantly improved the draft and clarified some details missing from the original presentation. 

There is an ablation study showing that each part of the model contributes to the overall performance. However, the approach provides only modest improvements over comparative methods which were not designed to learn from multiple target domains. In addition, comparison against the latest approaches is missing so it is likely that the performance reported here is below state-of-the-art. 

Overall, given the modest experimental gains combined with incremental improvement over single source information theoretic methods, this paper is not yet ready for publication."
iclr_2019_BJxOHs0cKm,"This paper proposes a generalization metric depending on the Lipschitz of the Hessian.

Pros: Paper has some nice experiments correlating their Hessian based generalization metric with the generalization gap, 

Cons: The paper does not compare its results with existing generalization bounds, as there is substantial work in the area now.  It is not clear whether existing generalization bounds do not capture this phenomenon with different batch sizes/learning rates, and the necessity of having and explicit dependence on the Lipschitz of the Hessian.

The bound by itself is also weak because of its dependence on number of parameters 'm'. 

The paper is poorly written and all reviewers complain about its readability.

I suggest authors to address concerns of the reviewers before submitting again. "
iclr_2019_BJxPk2A9Km,"
Pros:
- This is an interesting and relevant topic
- It is well motivated and mostly clear

Cons:
- The motivation, large amounts of data such as occur in lifelong learning, is not well examined in the evaluation which focuses on quite small problems.  For an example of work which addresses the lifelong memory management issue (though does not learn a memory management policy) see [1].
- In general the evaluation is not adequate to the claims.
- Reviewer 2 is concerned with the use of a bi-directional RNN for the comparison of memory entries since it may overfit to order.
- Reviewer 1 is somewhat concerned with novelty over other memory management schemes.

[1] Scalable Recollections for Continual Lifelong Learning. https://arxiv.org/pdf/1711.06761.pdf"
iclr_2019_BJxRVnC5Fm,"This paper proposes an approach to pruning units in a deep neural network while training is in progress. The idea is to (1) use a specific ""scoring function"" (the absolute-valued Taylor expansion of the loss) to identify the best units to prune, (2) computing the mean activations of the units to be pruned on a small sample of training data, (3) adding the mean activations multiplied by the outgoing weights into the biases of the next layer's units, and (4) removing the pruned units from the network. Extensive experiments show that this approach to pruning does less immediate damage than the more common zero-replacement approach, that this advantage remains (but is much smaller) after fine-tuning, and that the importance of units tends not to change much during training. The reviewers liked the quality of the writing and the extensive experimentation, but even after discussion and revision had concerns about the limited novelty of the approach, the fact that the proposed approach is incompatible with batch normalization (which severely limits the range of architectures to which the method may be applied), and were concerned that the proposed method has limited impact after fine-tuning."
iclr_2019_BJxYEsAqY7,"The paper describes knowledge distillation methods. As noted by all reviewers, the methods are very similar to the prior art, so there is not enough novelty for the paper to be accepted. The reviewers' opinion didn't change after the rebuttal."
iclr_2019_BJxbYoC9FQ,"{418}; {Classifier-agnostic saliency map extraction}; {Avg: 4.33}; {}

1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.

The paper is well-written and the method is simple, effective, and well-justified.

2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.

1. The introduction, in particular the last row of pg 1, implies that this work is the first to show that a class-agnostic saliency estimation method can produce higher-quality saliency maps than class-dependent ones. However, Fan et al. have already shown this. For this reason, AR1 recommended that the authors reword the introduction to reflect prior work on this aspect but the authors declined to do so. The AC would have liked to see a discussion of how the different points of view of the two works (robustness to corruption vs class-agnosticism) both address the same issue (poor segmentation of the salient image regions).
2. The work of Fan et al has a very similar approach and a deeper comparison is needed. While the authors dedicated two paragraphs of discussion to this work, they should have gone further. For example, the work of Fan et al. uses a very simple saliency map extraction network and it's unclear how much this impacts their performance when compared to the proposed method, which uses ResNet50. The AC agrees with the authors that re-implementing the method of Fan et al. is asking a lot but a discussion of the potential impact would have sufficed.
3. The authors didn't mention at all the vast body of work on salient object detection (for a somewhat recent review see Borji et al. ""Salient object detection: A benchmark."" IEEE TIP). The differences to this line of work should have been discussed.

Points 1 and 2 were particularly salient for the final decision.

3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.

Two major points of contention were:
- The discussion of differences between the proposed method and the method of Fan et al.
- The fairness of the comparison to Fan et al.
AR1 felt that the paper was deficient on both counts (AR2 had similar concerns) and the authors disagreed, arguing that the discussion was complete and the quantitative comparison fair.

The AC was sympathetic to these concerns and found the authors' responses to be dismissive of those concerns. In particular, the AC agrees that the paper, as currently organized, minimizes the degree to which the work is derived from Fan et al.

4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.

The reviewers reached a consensus that the paper should be rejected.
"
iclr_2019_BJxmXhRcK7,"AR1 is concerned about the poor organisation of this paper.  AR2 is concerned about the similarity between TRL and TR. The authors show some empirical results to support their intuition, however, no theoretical guarantees are provided regarding TRL superiority.  Moreover,  experiments for the Taskonomy dataset as well as on RNN have not been demonstrated, thus AR2 did not increase his/her score.  AR3 is the most critical and finds the clarity and explanations not ready for publication.

AC agrees with the reviewers in that the proposed idea has some merits, e.g. the reduction in the number of parameters seem a good point of this idea. However, reviewer urges the authors to seek non-trivial theoretical analysis for this method. Otherwise, it indeed is just an intelligent application paper and, as such, it cannot be accepted to ICLR."
iclr_2019_BJzmzn0ctX,"This paper focuses on scaling up neural theorem provers, a link prediction system that combines backward chaining with neural embedding of facts, but does not scale to most real-world knowledge bases. The authors introduce a nearest-neighbor search-based method to reduce the time/space complexity, along with an attention mechanism that improves the training. With these extensions, they scale NTP to modern benchmarks for the task, including ones that combine text and knowledge bases, thus providing explanations for such models.

The reviewers and the AC note the following as the primary concerns of the paper: (1) the novelty of the contributions is somewhat limited, as nearest neighbor search and attention are both well-known strategies, as is embedding text+facts jointly, (2) there are several issues in the evaluation, in particular around analysis of benefits of the proposed work on new datasets. There were a number of other potential weaknesses, such the performance on some benchmarks (Fb15k) and clarity and writing quality of a few sections.

The authors provided significant revisions to the paper that addressed many of the clarity and evaluation concerns, along with providing sufficient comments to better contextualize some of the concerns. However, the concerns with novelty and analysis of the results still hold. Reviewer 3 mentions that it is still unclear in the discussion why the accuracy of the proposed approach matches/outperforms that of NTP, i.e. why is there not a tradeoff. Reviewer 4 also finds the analysis lacking, and feels that the differences between the proposed work and the single-link approaches, in terms of where each excels, are described in insufficient detail. Reviewer 4 focused more on the simplicity of the text encoding, which restricts the novelty as more sophisticated text embeddings approaches are commonplace.

Overall, the reviewers raised different concerns, and although all of them appreciated the need for this work and the revisions provided by the authors, ultimately feel that the paper did not quite meet the bar."
iclr_2019_BJzuKiC9KX,"The paper presents a well conducted empirical study of the Reweighted Wake Sleep (RWS) algorithm (Bornschein and Bengio, 2015). It shows that it performs consistently better than alternatives such as Importance Weighted Autoencoder (IWAE) for the hard problem of learning deep generative models with discrete latent variables acting as a stochastic control flow. 
The work is well-written and extracts valuable insights supported by empirical observations: in particular the fact that increasing the number of particles improves learning in RWS but hurts in IWAE, and the fact that RWS can also be successfully applied to continuous variables.
The reviewers and AC note the following weaknesses of the work as it currently stands:  a) it is almost exclusively empirical and while reasonable explanations are argued, it does not provide a formal theoretical analysis justifying the observed behaviour b) experiments are limited to MNIST and synthetic data, confirmation of the findings on larger-scale real-world data and model would provide a more complete and convincing evidence. 
The paper should be made stronger on at least one (and ideally both) of these accounts.

"
iclr_2019_BkE8NjCqYm,"This paper examines a concept (also coined by the paper) of ""search discrepancies"" where the search algorithm behaves differently with large beam sizes. It then proposes heuristics to help prevent the model from performing worse when the size of the beam is increased.

I think there are some interesting insights in this paper with respect to how search works in modern neural models, but most reviewers (and me) were concerned by the heuristic approach taken to fix these errors. I still think that within a search paper, a clear separation between modeling errors and search errors is useful, and adding heuristics on top has a potential to making things more complicated down the road when, for example, we change our model or we change our training algorithm.

It would be nice if the nice insights in the paper could be turned into a more theoretically clean framework that could be re-submitted to a future conference."
iclr_2019_BkGiPoC5FX,"This paper proposes a training algorithm for ConvNet architectures in which the final few layers are fully connected.  The main idea is to use direct feedback alignment with carefully chosen binarized (±1) weights to train the fully connected layers and backpropagation to train the convolutional layers. The binarization reduces the memory footprint and computational cost of direct feedback alignment, while the careful selection of feedback weights improves convergence. Experiments on CIFAR-10, CIFAR-100, and an object tracking task are provided to show that the proposed algorithm outperforms backpropagation, especially when the amount of training data is small. The reviewers felt that the paper does a terrific job of introducing the various training algorithms --- backpropagation, feedback alignment, and direct feedback alignment --- and that the paper clearly explained what the novel contributions were. However, the reviewers felt the paper had limited novelty because it combines ideas that were already known, that it has limited applicability because it will not work with fully convolutional architectures, that the baselines in the experiments were somewhat weak, and that the paper provided no insights on why the proposed algorithm might be better than backpropagation in some cases. Regrettably, only one reviewer (R2) participated in the discussion, though this was the reviewer who provided the most constructive review. The AC read the revised paper, and agrees with R2's concerns about the limited applicability of the proposed algorithm and lack of insight or analysis explaining why the proposed training algorithm would improve over backpropagation."
iclr_2019_BkMWx309FX,"This paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned.  Some experiments are used to support the algorithm, which involves learning the reward perturbation process (the confusion matrix) using existing techniques from the supervised learning (and crowdsourcing) literature.

Reviewers found the problem setting new and worth investigating, but had concerns over the scope/significance of this work, mostly about how the confusion matrix is learned.  If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards.  Specifically, the work seems to be limited in two substantial ways, both related to how the confusion matrix is learned.
  * The reward function needs to be deterministic.
  * Majority voting requires the number of states to be finite.
The significance of this work is therefore mostly limited to finite-state problems with deterministic reward, which is quite restricted.

As the authors pointed out, the paper uses discretization to turn a continuous state space into a finite one, which is how the experiment was done.  But discretization is likely not robust or efficient in many high-dimensional problems.

It should be noted that the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work.  However, the exact problem setting is not completely clear in the paper, and the limitations of the technical contributions is also somewhat unclear.  The authors are strongly advised to revise the paper accordingly to make their contributions clearer.

Minor questions:
  - In lemma 2, what if C is not invertible.
  - The sampling oracle assumed in def 1 is not very practical, as opposed to what the paper claims.
  - There are more recent work at NIPS and STOC on attacking RL (including bandits) algorithms by manipulating the reward signals.  The authors may want to cite and discuss."
iclr_2019_BkMXkhA5Fm,"The paper introduces a new dataset that contains multiple landings from the X plane simulator, and each includes readings from multiple sensors for aircraft landing. The  paper also trains a set of self-supervised methods presented in previous works in order to learn sensory representations, and evaluates the learnt representations in terms of disentanglement and re-purposing to a discriminative task. 
 Though the evaluations presented are interesting, they are not convincingly useful, as noted by the reviewers. Overall, it is not clear why this dataset is particularly well suited for representation learning. Furthermore, it is difficult to evaluate representation learning methods without relating them to an end-task, e.g., that of landing the aircraft. 
The paper writing would also benefit from restructuring and improving on English expressions. In particular, the conclusion section contains half-finished sentences.
"
iclr_2019_BkMn9jAcYQ,"This paper proposes a method to resolve ""language drift,"" where a pre-trained X->language model trained in an X->language->Y pipeline drifts away from being natural language. In particular, it proposes to add an auxiliary training objective that performs grounding with multimodal input to fix this problem. Results are good on a task where translation is done between two languages.

The main concern that was raised with this paper by most of the reviewers is the validity of the proposed task itself. Even after extensive discussion with the authors, it is not clear that there is a very convincing scenario where we both have a pre-trained X->language, care about the intermediate results, and have some sort of grounded input to fix this drift. While I do understand the MT task is supposed to be a testbed for the true objective, it feel it is necessary to additionally have one convincing use case where this is a real problem and not just the artificially contrived. This use case could either be of practical use (e.g. potentially useful in an application), or of interest from the point of view of cognitive plausibility (e.g. similar to how children actually learn, and inspired by cognitive science literature).

A concern that offshoots from this is that because the underlying idea is compelling (some sort of grounding to inform language learning), a paper at a high-profile conference such as ICLR may help re-popularize this line of research, which has been a niche for a while. Normally I would say this is definitely a good thing; I think considering grounding in language learning is definitely an important research direction, and have been a fan of this line of work since reading Roy's seminal work on it from 15 years ago. However, if the task used in this paper, which is of questionable value and reality, becomes the benchmark for this line of work I think this might lead other follow-up work in the wrong direction.  I feel that this is a critical issue, and the paper will be much stronger after a more realistic task setting is added.

Thus, I am not recommending acceptance at this time, but would definitely like the authors to think hard and carefully about a good and realistic benchmark for the task, and follow up with a revised version of the paper in the future."
iclr_2019_BkMq0oRqFQ,"This paper interprets batch norm in terms of normalizing the backpropagated gradients. All of the reviewers believe this interpretation is novel and potentially interesting, but that the paper doesn't make the case that this helps explain batch norm, or provide useful insights into how to improve it. The authors have responded to the original set of reviews by toning down some of the claims in the original paper, but haven't addressed the reviewers' more substantive concerns. There may potentially be interesting ideas here, but I don't think it's ready for publication at ICLR.

"
iclr_2019_BkNUFjR5KQ,"This paper proposes a genetic algorithm to search neural network architectures with locally dense and globally sparse connections. A population-based genetic algorithm is used to find the sparse, connections between dense module units. The local dense but global sparse architecture is an interesting idea, yet is not well studied in the current version, e.g. overfitting and connections with other similar architecture search methods. Based on reviewers’ ratings (5,5,6), the current version of paper is proposed as borderline lean reject.

"
iclr_2019_BkVVOi0cFX,"This paper presents a model for question answering, where the idea is to have a collaborative model that aligns queries and sentences on a small supervised dataset and also uses semi-supervised information from a weakly supervised corpus to answer open domain questions resulting in short answer spans.

The main criticism of the paper is regarding its novelty, and reviewers cite the similarities with prior work such as Chen et al. and Min et al.  There is relative consensus between the reviewers that further work using the semi-supervised outlook with stronger results could strengthen the paper further."
iclr_2019_Bke0rjR5F7,"The paper presents a method to stochastically optimize second-order penalties and show how this could be applied to training fairness-aware classifiers, where the linear penalties associated with common fairness criteria are expressed as the second order penalties. 

While the reviewers acknowledged the potential usefulness of the proposed approach, all of them agreed that the paper requires: (1) major improvement in clarifying important points related to the approach (see R3’s detailed comments; R2’s concern on using the double sampling method to train non-convex models; see R1’s and R3’s concerns regarding the double summation/integral terms and how this effects runtime), and (2) major improvement in justifying its application to fairness; as noted by R2, “there is no sufficient evidence why non-convex models are actually useful in the experiments”. Given that fairness problems are currently studied on the small scale datasets (which is not this paper’s fault), a comparison to simpler methods for fairness or other applications could substantially strengthen the contribution and evaluation of this work.
We hope the reviews are useful for improving and revising the paper. 
"
iclr_2019_Bke96sC5tm,"This paper proposes a method to learn representations to infer simple local models that can be used for policy improvement. All the reviewers agree that the paper has interesting ideas, but they found the main contribution to be a bit weak and the experiments to be insufficient.

Post rebuttal, the reviewers discussed extensively with each other and agreed that, given more work is done on a clear presentation and improving the experiments, this paper can be accepted. In its current form however, the paper is not ready to be accepted. I have recommended to reject this paper, but I will encourage the authors to resubmit after improving the work.
"
iclr_2019_BkeDEoCctQ,"Pros:
- novel idea of intra-life curiosity that encourages diverse behavior within each episode rather than across episodes.

Cons:
- privileged/ad-hoc information (RAM state, distinguishing rooms)
- lack of sufficient ablations/analysis
- insufficient revision/rebuttal

The reviewers reached consensus that the paper should be rejected in its current form."
iclr_2019_BkeK-nRcFX,"This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test-time performance of neural networks at initialization. The idea is interesting and novel, and has clear practical implications. Reviewers unanimously agreed that the direction is a worthwhile one to pursue. However, several reviewers also raised concerns about how well-justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad-hoc. Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity. 

These concerns make it clear that the paper needs more work before it can be published. And, in particular, addressing the reviewers' concerns and providing proper comparison to related works will go a long way in that direction."
iclr_2019_BkeUasA5YQ,"The authors propose a method for distilling a student network from a teacher network and while additionally constraining the intermediate representations from the student to match those of the teacher, where the student has the same width, but less depth than the teacher. The main novelty of the work is to use the intermediate representation from the teacher as an input to the student network, and the experimental comparison of the approach against previous work. 

 The reviewers noted that the method is simple to implement, and the paper is clearly written and easy to follow. The reviewers raised some concerns, most notably that the authors were using validation accuracy to measure performance, and were thus potentially overfitting to the test data, and regarding the novelty of the work. Some of the criticisms were subsequently amended in the revised version where results were reported on a test set (the conclusions are as before).  Overall, the scores for this paper were close to the threshold for acceptance, and while it was a tough decision, the AC ultimately felt that the overall novelty of the work was slightly below the acceptance bar."
iclr_2019_BkedwoC5t7,"The paper proves that the Donsker-Varadhan lower bound on KL divergence cannot be used to estimate KL divergences of more than tens of bits, and that more generally any distribution-free high-confidence lower bound on mutual information cannot be larger than O(ln N) where N is the size of the data sample. As an alternative for applications such as maximum mutual information predictive coding, a form of representation learning, the paper proposes using the cross-entropy upper bound on entropy and estimating mutual information as a difference of two cross-entropies. These cross-entropy bounds converge to the true entropy as 1/\sqrt(N), but at the cost of providing neither an upper nor a lower bound on mutual information. There was a divergence of opinion between the reviewers on this paper. The most negative reviewer (R3) thought there should be experiments confirming that the DV bound fails when mutual information is high, was concerned that the theory applied only in the case of discrete distributions, and was concerned that the proposed optimization problem in Section 6 would be challenging due to its adversarial (max-inf) structure. The authors responded that they felt the theory could stand on its own without empirical tests (a point with which R1 agreed); that although their exposition was for discrete variables, the analysis applies to the continuous case as well; and that they agreed with the point about the difficulty of the optimization, but that GANs face similar difficulties. Because R3 did not participate in the discussion and the AC believes that the authors adequately addressed most of R3's issues in their response and revision, this review has been discounted. The next most negative reviewer (R2) wanted a discussion relating the ideas in this paper to kNN and kernel-based estimators of mutual information, wanted an empirical evaluation (like R3), and was concerned about whether the difference of cross-entropies provides an upper or lower bound on mutual information. In their response and revision the authors added some discussion of kNN methods (but not enough to make R2 happy) and clarified that the difference of cross-entropies provides neither an upper nor a lower bound. The most positive reviewer (R1) thinks the theoretical contribution of the paper is significant enough to justify publication in ICLR. The AC likes the theoretical work and feels that it raises important concerns about MINE, but concurs with R2 and R3 that some empirical validation of the theory is needed for the paper to appear in ICLR. The authors are strongly encouraged to perform an empirical validation of the theory and to submit this work to another machine learning venue."
iclr_2019_BkesGnCcFX,"This manuscript presents a reinterpretation of hindsight experience replay which aims to avoid recomputing the reward function, and investigates Floyd-Warshall RL in the function approximation setting.

The paper was judged as relatively clear. The authors report a slight improvement in computational cost, which some reviewers called into question. However, all of the reviewers pointed out that the experimental evidence for the method's superiority is weak. Two reviewers additionally raised that this wasn't significantly different than the standard formulation of Hindsight Experience Replay, which doesn't require the computation of rewards for relabeled goals.

Ultimately, reviewers were in agreement that the novelty of the method and quality of the obtained results rendered the work insufficient for publication. The Area Chair concurs, and urges the authors to consider the reviewers' pointers to the existing literature in order to clarify their contribution for subsequent submission."
iclr_2019_BkesJ3R9YX,"Strengths: The paper presentation was assessed as being of high quality. Experiments were diverse in terms of datasets and tasks.

Weaknesses: Multiple reviewers commented that the paper does not present substantial novelty compared to previous work.

Contention: One reviewer holding out on giving a stronger rating to the paper due to the issue of novelty. 

Consensus: Final scores were two 6s one 3. 

This work has merit, but the degree of concern over the level of novelty leads to an aggregate rating that is too low to justify acceptance. Authors are encourage to re-submit to another venue.
"
iclr_2019_Bkeuz20cYm,"The reviewers agreed that there are some promising ideas in this work, and useful empirical analysis to motivate the approach. The main concern is in the soundness of the approach (for example, comments about cumulative learning and negative samples). The authors provided some justification about using previous networks as initialization, but this is an insufficient discussion to understand the soundness of the strategy. The paper should better discuss this more, even if it is not possible to provide theory. The paper could also be improved with the addition of a baseline (though not necessarily something like DeepStack, which is not publicly available and potentially onerous to reimplement).  "
iclr_2019_BkewX2C9tX,"This paper proposes model poisoning (poisoned parameter updates in a federated setting) in contrast to data poisoning (poisoned training data). It proposes an attack method and compares to baselines that are also proposed in the paper (there are no external baselines). While model poisoning is indeed an interesting direction to consider, I agree with reviewer concerns that the relation to data poisoning is not clearly addressed. In particular, any data poisoning attack could be used as a model poisoning attack (just provide whatever updates would be induced by the poisoned data), so there is no good excuse to not compare to the existing strong data poisoning attacks. One reviewer raised concerns about lack of theoretical guarantees but I do not agree with these concerns (the authors correctly point out in the rebuttal that this is not necessary for an attack-focused paper). I do feel there is room to improve the overall clarity/motivation (for instance, equation (1) is presented without any explanation and it is still not clear to me why this is the right formulation)."
iclr_2019_Bkf1tjR9KQ,"The paper describes an architecture search method which optimises multiple objectives using a genetic algorithm. All reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground-breaking to justify acceptance based on results alone.
"
iclr_2019_BkfPnoActQ,"This paper proposes a combination of three techniques to improve the learning performance of Atari games. Good performance was shown in the paper with all three techniques together applied to DQN. However, it is hard to justify the integration of these techniques. It is also not clear why the specific decisions were made when combining them. More comprehensive experiments, such as a more systematic ablation study, are required to convince the benefits of individual components. Furthermore, it seems very hard to tell whether the improvement of existing approaches, such as Ape-X DQN, was from using the proposed techniques or a deeper architecture (Tables 1&2&4&5). Overall, this paper is not ready for publication.
"
iclr_2019_BkfhZnC9t7,"This paper studies the really hard problem of zero-shot learning in acoustic modeling for languages with limited resources, using data from English. Using a novel universal phonetic model, the authors show improvements compared to using an English model for 20 other languages in phone recognition quality.

Strengths
- Reviewers agree that the problem is an important one, and the presented ideas are novel.
- Universal phonetic model to represent phones in any language is interesting.

Weaknesses
- The results are really weak, to the point that it is unclear how effective or general the techniques are. The work is an interesting first step, but is not developed enough to be accepted at this point.
- The universal phonetic model being trained only in English might affect generalizability to languages that do not share phonetic characteristics. The authors agree partly, and argue that the method already addresses some issues since the model can already represent unseen phones. But, coupled with the high phone error rates, it is still unclear how appropriate the technique will be in addressing this issue.
- Novelty: Although the idea of mapping phones to attributes, and using those for ASR is not novel (e.g., using articulatory features), application for zero-shot learning is. The work assumes availability of a small text corpus to learn phone-sequence distribution, so is similar to other zero-resource approaches that assume some data (audio, as opposed to text) is available in the new language.

This paper presents interesting first steps, but lacks sufficient experimental validation at this point. Therefore, AE recommendation is to reject the paper. I encourage the authors to improve and resubmit in the future."
iclr_2019_BkfxKj09Km,"Reviewer ratings varied radically (from a 3 to an 8). However, the reviewer rating the paper as 8 provided extremely little justification for their rating. The reviewers providing lower ratings gave more detailed reviews, and also engaged in  discussion with the authors. Ultimately neither decided to champion the paper, and therefore, I cannot recommend acceptance."
iclr_2019_Bkg5aoAqKm,"This paper proposes an Optimal Binary Functional Search (OBFS) algorithm for searching with general score functions, which generalizes the standard similarity measures based on Euclidean distances. This yields an extension of the classical approximate nearest neighbor search (ANNS). As observed by the reviewers, this work targets an important research direction. Unfortunately, the reviewers raised several concerns regarding the clarity and significance of the work. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. "
iclr_2019_Bkg93jC5YX,"This paper is very close to the decision boundary and the reviewers were split about whether it should be accepted or not. The authors updated the paper with additional experiments as request by the reviewers.
The area chair acknowledges that there is some novelty that leads to (moderate) empirical gains but does not see these as sufficient to push the paper over the very competitive acceptance threshold. "
iclr_2019_BkgFqiAqFX,"The reviewers reached a consense on that the paper is not quite ready for publication at ICRL. The main potential drawback include a) the exposition of the paper can be improved; b) it's not entirely clear that some of the assumptions (such as the threshold for the first layer, the polynomial approximation of higher layers) are meaningful , and it seems that the proof technique exploits heavily some of these assumptions and some of the key intermediate steps won't hold in practice. (see reviewer 3's comment for more details.) The authors clarify the writing and intuitions in the response, but overall the AC decided that the paper is not quite ready for publications at the moment.  "
iclr_2019_BkgGmh09FQ,"This paper targets improving the computation efficiency of super resolution task. Reviewers have a consensus that this paper lacks technical contribution, therefore not recommend acceptance. "
iclr_2019_BkgVx3A9Km,"Dear authors,

The reviewers all appreciated your goal of improving dimensionality reduction techniques. This is a field which does not enjoy the popularity it once did but remains nonetheless important.

They also appreciated the novel loss and the use of triplets.to get the global structure.

However, the paper lacks some guidance. In particular, it oscillates between showing qualitative results (robustness to outliers, ""nice"" visualizations) and quantitative ones (running time, classification performance). I agree with the reviewers that the quantitative ones should have used the same preprocessing for t-SNE and TriMap (either PCA or no PCA), regardless of the current implementation in software tools.

Given that the quantitative results are not that impressive, may I suggest focusing on the qualitative ones for a resubmission? The robustness of the emeddings to the addition or removal of a few points is definitely interesting and worth further investigation, optionally with a corresponding metric."
iclr_2019_BkgYIiAcFQ,"there is a disagreement among the reviewers, and i am siding with the two reviewers (r1 and r3) and agree with r3 that it is rather unconventional to pick learning-to-learn to experiment with modelling variable-length sequences (it's not like there's no other task that has this characteristics, e.g., language modelling, translation, ...) "
iclr_2019_BkgiM20cYX,"The paper proposes a novel approach to interfacing robots with humans, or rather vv: by mapping instructions to goals, and goals to robot actions.   A possibly nice idea, and possibly good for more efficient learning.

But the technical realisation is less strong than the initial idea.  The original idea merits a good evaluation, and the authors are strongly encouraged to follow up on this idea and realise it, towards a stronger publication.

It be noted that the authors refrained from using the rebuttal phase."
iclr_2019_BkgosiRcKm,"This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs. While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas. Furthermore, R3 is not convinced that the approach is well motivated, beyond “filling the gap” in the literature. 

All reviewers also pointed out that the paper is very hard to read. The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way. 

Overall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty.
"
iclr_2019_Bkl2SjCcKQ,"The paper points out a statistical properties of GAN samples which allows their identification as synthetic.

The paper was praised by one reviewer as well-written, easy to follow, and addressing an interesting topic. Another added that the authors did an excellent job of ""probing into different statistical perspectives"", and the fact that they did not confine their investigation to images.

Two reviewers leveraged the criticism that various properties discovered are not surprising given the loss functions and associated metrics as well as the inductive biases of continuous-valued generator networks. Tests employed were criticized as ad hoc, and reviewers felt that their generality was limited given their reduced sensitivity on certain modalities. (While Figure 10b is raised by the authors several times in the discussion, and the test statistics of samples are noted to be closer to the test data than to the random baseline, the test falsely rejects the null [p-value ~= 0.0] for non-synthetic test data.)

I would encourage the authors to continue this line of inquiry as it is overall agreed to be an interesting topic of relevance and increasing importance, however based on the criticisms of reviewers and the content of the ensuing discussion I do not recommend acceptance at this time."
iclr_2019_Bkl87h09FX,"This paper presents an extensive empirical study to sentence-level pre-training. The paper compares pre-trained language models to other potential alternative pre-training options, and concludes that while pre-trained language models are generally stronger than other alternatives, the robustness and generality of the currently available method is less than ideal, at least with respect to ELMO-based pretraining. 

Pros:
The paper presents an extensive empirical study that offers new insights on pre-trained language models with respect to a variety of sentence-level tasks.   

Cons:
The primarily contributions of this paper is empirical and technical novelty is relatively weak. Also, the insights are based just on ELMO, which may have a relatively weak empirical impact. The reviews were generally positive but marginally positive, which reflect that insights are interesting but not overwhelmingly interesting. None of these is a deal-breaker per say, but the paper does not provide sufficiently strong novelty, whether based on insights or otherwise, relative to other papers being considered for acceptance.

Verdict:
Leaning toward reject due to relatively weak novelty and empirical impact.

Additional note on the final decision: 
The insights provided by the paper are valuable, thus the paper was originally recommended for an accept. However, during the calibration process across all areas, it became evident that we cannot accept all valuable papers, each presenting different types of hard work and novel contributions. Consequently, some papers with mostly positive (but marginally positive) reviews could not be included in the final cut, despite their unique values, hard work, and novel contributions. "
iclr_2019_BklACjAqFm,"Pros:
- interesting algorithmic idea for using successor features to propagate uncertainty for use in epxloration
- clarity

Cons:
- moderate novelty
- initially only simplistic experiments (later complemented with Atari results)
- initially missing baseline comparisons
- no regret-based analysis
- questionable soundness because uncertainty is not guaranteed to go down

All the reviewers found the initial submission to be insufficient for acceptance, and the one reviewer who read the rebuttal/revision did not change their mind, despite the addition of some large-scale results (Atari)."
iclr_2019_BklAEsR5t7,"The paper addresses the problem of large scale fine-grained classification by estimating pairwise potentials in a CRF model. The reviewers believe that the paper has some weaknesses including (1) the motivation for approximate learning is not clear (2) the approximate objective is not well studied and (3) the experiments are not convincing. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account to improve the paper.
"
iclr_2019_BklKFo09YX,"This paper introduces a variant of the CycleGAN designed to optimize molecular graphs to achieve a desired quality.  The work is reasonably clear and sensible, however it's of limited technical novelty, since it's mainly just combining two existing techniques.  Overall its specificity and incrementalness make it not meet the bar."
iclr_2019_BklMYjC9FQ,"The paper proposes an approach to remedying mode collapse problem in GANs. This approach relies on using multiple discriminators and assigning a different portion of each minibatch to each discriminator. 

+ preventing mode collapse in GAN training is an important problem

- the exact motivation for the proposed techniques is not fully fleshed out

- the evaluation and baselines used are lacking"
iclr_2019_BklUAoAcY7,The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
iclr_2019_BklpOo09tQ,"While the proposed method is novel, the evaluation is not convincing. In particular, the datasets and models used are small. Susceptibility to adversarial examples is tightly related to dimensionality. The study could benefit from more massive datasets (e.g., Imagenet)."
iclr_2019_Bklzkh0qFm,"The authors propose an architecture for learning and predicting graphs with relations between nodes. The approach is a combination of recent research efforts into Graph Attention Networks and Relational Graph Convolutional Networks. The authors are commended for their clear and direct writing and presentation and their honest claims and their empirical setup. However, the paper simply doesn't have much to offer to the community, since the algorithmic contributions are marginal and the results unimpressive. While the authors justify the submission in terms of the difficult implementation and the extensive experiments, this is not enough to support its publication at a top conference. Rather, this could be a technical report."
iclr_2019_Bkx8OiRcYX,"All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance."
iclr_2019_BkxAUjRqY7,"The paper proposes an information theoretic quantity to measure the performance of transferred representations with an operational appeal, easier computation, and empirical validation. 

The relation of the proposed measure to test accuracy is not considered. The operational meaning holds exactly only in the special case of linear fine tuning layers. The paper seems to import heavily from previous works. 

Reviewers found it difficult to understand whether the proposed method makes sense, that the computation of relevant quantities might be difficult in general, and that the comparison with mutual information was not clear. The revision addresses these points, adding experiments and explanations. Yet, none of the reviewers gives the paper a rating beyond marginally above acceptance threshold. 

All reviewers found the paper interesting and relevant, but none of them found the paper particularly strong. This is a borderline case of a sound and promising paper, which nonetheless seems to be missing a clear selling point. 

I would suggest that developing the program laid out in the conclusions could make the contributions more convincing, in particular the development of more scalable algorithms and the application of the proposed measure to the design of hierarchies for transfer learning. "
iclr_2019_BkxSHsC5FQ,"The authors propose using a SVM, trained as a last layer of a neural network, to identify exemplars (support vectors) to save and use to prevent forgetting as the model is trained on further tasks. The method is effective on several supervised benchmarks and is compared to several other methods, including VCL, iCARL, and GEM. The reviewers had various objections to the initial paper that centered around comparisons to other methods and reporting of detailed performance numbers, which the authors resolved convincingly in their revised paper. However, the AC and 2 of the reviewers were unconvinced of the contribution of the approach. Although no one has used this particular strategy, of using support vectors to prevent forgetting, the approach is a simplistic composition of the NN and the SVM which is heuristic, at least in how the authors present it. Most importantly, the approach is limited to supervised classification problems, yet catastrophic forgetting is not commonly considered to be a problem for the supervised classifier setting; rather it is a problem for inherently sequential learning environments such as RL (MNIST and CIFAR are just commonly used in the literature for ease of evaluation)."
iclr_2019_Bkx_Dj09tQ,"The authors conduct experiments to study orientation selectivity in neural networks. 

The reviewers generally agreed that the paper was clearly written and easy to follow. Further, the experimental analysis demonstrates that contrary to what was claimed in some previous work, the learned orientation selectivity can be useful for generalization. 

However, the reviewers also raised a number of concerns: 1) that the conclusions are drawn on the basis of a couple of neural network architectures; the authors attempted to add results using a Resnet50 model, but this analysis was ultimately removed when the authors discovered a bug; 2) in the context of the contributions in neuroscience it was not clear that the limited results on the two artificial networks are sufficient to help draw such conclusions, and that 3) since the network is trained to recognize objects, it would seem natural that the model would learn neurons that are sensitive to orientation and that it is not clear how the author’s observations might lead to better trained models. 
While the reviewers were not completely unanimous in their scores, the AC agrees with a majority of the reviewers that the work while interesting could be strengthened by additional experiments on other architectures. 
"
iclr_2019_Bkxdqj0cFQ,"The reviewers and AC note the potential weaknesses of the paper in various aspects, and decided that the authors need more works to publish. "
iclr_2019_BkxgbhCqtQ,The reviewers agree this paper is not good enough for ICLR.
iclr_2019_BkxkH30cFm,"This paper tackles a very valuable problem of learning object detection and object dynamics from video sequences, and builds upon the method of Zhu et al. 2018. The reviewers point out that there is a lot of engineering steps in the object proposal stage, which takes into account background subtraction to propose objects. In its current form, the writing of the paper is not clear enough on the object instantiation part, which is also the novel part over Zhu et al., potentially due to the complexity of using motion to guide object proposals. A limitation of the proposed formulation is that it works for moving cameras but only in 2d environments. Experiments on 3D environments would make this paper a much stronger submission. "
iclr_2019_By40DoAqtX,"All three reviewers expressed concerns about the writing of the paper. The AC thus recommends ""revise and resubmit""."
iclr_2019_By41BjA9YQ,"Dear authors,

The topic of variance reduction in optimization is timely and the reviewers appreciated your attempt at circumventing the issues faced with the current popular methods.

They however had a concern about the significance of the results, which I echo:
- First, there have been previous attempts at variance reduction which share some similarity with yours, for instance ""No more pesky learning rate"", ""Topmoumoute online natural gradient algorithm"" or even Adam (which does variance reduction without mentioning it).
- The fact that previous similar methods exist is a non-issue should yours perform better. However, the absence of stepsize tuning in the experimental evaluation is a big issue as the performance of an iterative algorithm is highly sensitive to it.

Finally, the link between flatness of the minimum and generalization is dubious, as mentioned for instance by Dinh et al. (2017).

As a consequence, I cannot accept this work for publication to ICLR but I encourage you to address the points of the reviewers should you wish to resubmit it to a future conference.
"
iclr_2019_ByEtPiAcY7,"The presented paper introduces a method to represent neural networks as logical rules of varying complexity, and demonstrate a tradeoff between complexity and error. Reviews yield unanimous reject, with insufficient responses by authors.

Pros:
+ Paper well written

Cons:
- R1 states inadequacy of baselines, which authors do not address.
- R3&4 raise issues about the novelty of the idea.
- R2&4 raise issues on limited scope of evaluation, and asked for additional experiments on at least 2 datasets which authors did not provide.

Area chair notes the similarity of this work to other works on network compression, i.e. compression of bits to represent weights and activations. By converting neurons to logical clauses, this is essentially a similar method. Authors should familiarize themselves with this field and use it as a baseline comparison. i.e.: https://arxiv.org/pdf/1609.07061.pdf "
iclr_2019_ByGOuo0cYm,"This paper addresses the problem of few shot learning and then domain transfer. The proposed approach consists of combining a known few shot learning model, prototypical nets, together with image to image translation via CycleGAN for domain adaptation.  Thus the algorithmic novelty is minor and amounts to combining two techniques to address a different problem statement. In addition, as mentioned by Reviewer 2, though meta learning could be a solution to learn with few examples, the solution being used in this work is not meta learning and so should not be in the title to avoid confusion. 

As this is a new problem statement the authors apply multiple existing works from few shot learning (and now adaptation) to their setting. The proposed approach does outperform prior work, however this is not surprising as the prior work was not designed for this task. Despite improvements during the rebuttal to address clarity the specific experimental setting is still unclear -- especially the setup of meta test data vs unsupervised da data. 

This paper is borderline. However, since the main contribution consists of proposing a new problem statement and suggesting a combination of prior techniques as a first solution, the paper needs a more thorough ablation of other possible combination of techniques as well as a clearly defined experimental setup before it is ready for publication."
iclr_2019_ByGUFsAqYm,"This paper studies the question of memorization within overparametrised neural networks. Specifically, the authors conjecture that memorization is linked to the downsampling operators present in many convolutional autoencoders. 

All reviewers agreed that this is an interesting question that deserves further analysis. However, they also agreed that in its current form, the paper lacks mathematical and experimental rigor. In particular, the paper does not follow the basic mathematical standards of proving any stated proposition/theorem, instead mixing empirical with mathematical proofs. The AC fully agrees with the points raised by reviewers, and therefore recommends rejection at this point, encouraging the authors to address these important points before resubmitting their work. "
iclr_2019_ByGVui0ctm,"The authors have proposed 3 continual learning variants which are all based on MNIST and which vary in terms of whether task ids are given and what the classification task is, and they have proposed a method which incorporates a symmetric VAE for generative replay with a class discriminator. The proposed method does work well on the continual learning scenarios and the incorporation of the generative model with the classifier is more efficient than keeping them separate. The discussion of the different CL scenarios and of related work is nice to read. However, the authors imply that these scenarios cover the space of important CL variants, yet they do not consider many other settings, such as when tasks continually change rather than having sharp boundaries. The authors have also only focused on the catastrophic forgetting aspect of continual learning, without considering scenarios where, e.g., strong forward transfer (or backwards transfer) is very important. Regarding the proposed architecture that combines a VAE with a softmax classifier for efficiency, the reviewers all felt that this was not novel enough to recommend publication."
iclr_2019_ByG_3s09KX,"The paper presents Dopamine, an open-source implementation of plenty of DRL methods. It presents a case study of DQN and experiments on Atari. The paper is clear and easy to follow.

While I believe Dopamine is a very welcomed contribution to the DRL software landscape, it seems there is not enough scientific content in this paper to warrant publication at ICLR. Regarding specifically the ELF and RLlib papers, I think that the ELF paper had a novelty component, and presented RL baselines to a new environment (miniRTS), while the RLlib paper had a stronger ""systems research"" contribution. This says nothing about the future impact of Dopamine, ELF, and RLlib – the respective software."
iclr_2019_ByGq7hRqKX,"The authors have proposed a language+vision 'dual' attention architecture, trained in a multitask setting across SGN and EQA in vizDoom, to allow for knowledge grounding. The paper is interesting to read. The complex architecture is very clearly described and motivated, and the knowledge grounding problem is ambitious and relevant. However, the actual proposed solution does not make a novel contribution and the reviewers were unconvinced that the approach would be at all scalable to natural language or more complex tasks. In addition, the question was raised as to whether the 'knowledge grounding' claims by the authors are actually much more shallow associations of color and shape that are beneficial in cluttered environments.
This is a borderline case, but the AC agrees that the paper falls a bit short of its goals."
iclr_2019_ByN7Yo05YX,"This paper proposes adaptive neural trees (ANT), a combination of deep networks and decision tress. Reviewers 1 leans toward reject the paper, pointing out several flaws. Reviewer 3 also raises concerns, despite later increasing rating to marginally above threshold.  Of particular note is the weak experimental validation.

The paper reports results only on MNIST and CIFAR-10. MNIST performance is too easily saturated to be meaningful. The CIFAR-10 results show ANT models to have far greater error than the state-of-the-art deep neural network models.

As Reviewer 1 states, ""performance of the proposed method is also not the best on either of the tested datasets. Please clearly elaborate on why and how to address this issue. It would be more interesting and meaningful to work with a more recent large datasets, such as ImageNet or MS COCO.""

The rebuttal fails to offer the type of additional results that would remedy this situation. Without a convincing experimental story, it is not possible to recommend acceptance of this paper."
iclr_2019_Bye5OiR5F7,"Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with ""revise and resubmit""."
iclr_2019_Bye9LiR9YX,"This paper introduces a novel idea, and demonstrates its utility in several simulated domains. The key parts of the algorithm are (a) to prefer keeping and using samples in the ER buffer where the corresponding rho_t, using the current policy pi_t, are not too big or small and (b) preventing the policy from changing too quickly, so that samples in the ER buffer are more on-policy.  

They key weakness is not better investigating the idea of making the ER buffer more on-policy, and the effect of doing so. The experiments compare to other algorithms, but do not sufficiently investigate the use of both Point 1 and Point 3. Further, the appendix contains an investigation into parameter sensitivity and gives some confidence intervals. However, the presentation of this is difficult to follow, and so it is difficult to gauge the sensitivity of Ref-ER. With a more thorough experimental section, better demonstrating the results (not necessarily running more things), the paper would be much stronger. 

For more context, the authors rightly mention ""It is commonly believed that off-policy methods (e.g. Q-learning) can handle the dissimilarity between off-policy and on-policy outcomes. We provide ample evidence that training from highly similar-policy experiences is essential to the success of off-policy continuous-action deep RL."" Q-learning can significantly suffer from changing the state-sampling distribution. However, adjusting sampling in the ER buffer using rho_t does not change the state-sampling distribution, and so that mismatch remains a problem. Changing the policy more slowly (Point 3) could help with this more. In general, however, these play two different roles that need to be better understood. The introduction more strongly focuses on classifying samples as more on or off-policy, to solve this problem, rather than the strategy used in Point 3. So, from the current pitch, its not clear which component is solving the issues claimed with off-policy updates. 

Overall, this paper has some interesting results and is well-written. With more clarity on the roles of the two components of Ref-ER and what they mean for making the ER buffer more on-policy, in terms of both action selection and state distribution, this paper would be a very useful contribution to stable control. "
iclr_2019_ByeDojRcYQ,"Pros:
- interesting novel formulation of policy learning in homogeneous swarms
- multi-stage learning process that trades off diversity and consistency (fig 1)

Cons:
- implausible mechanisms like averaging weights of multiple networks
- minor novelty
- missing ablations of which aspect is crucial 
- dubious baseline results
- no rebuttal

One reviewer out of three would have accepted the paper, the other two have major concerns. Unfortunately the authors did not revise the paper or engage with the reviewers to clear up these points, so as it stand the paper should be rejected."
iclr_2019_ByeLBj0qFQ,"This paper was reviewed by three experts. After the author response, R2 and R3 recommend rejecting this paper citing concerns of experimental evaluation and poor quality of the manuscript. All three reviewers continue to have questions for the authors, which the authors have not responded to. The AC finds no basis for accepting this paper in this state. "
iclr_2019_ByeLmn0qtX,"This paper proposes using conditional VAEs for multi-domain transfer and presents results on CelebA and SCUT. As mentioned by reviewers, the presentation and clarity of the work could be improved. It is quite difficult to determine the new/proposed aspects of the work from a first read through. Though we recognize and appreciate that the authors updated their manuscript to improve its clarity, another edit pass with particular focus on clarifying prior work on conditional VAEs and their proposed new application to domain transfer would be beneficial. 

In addition, as DIS is the main metric for comparison to prior work and for evaluation of the final approach, the conclusions about the effectiveness of this method would be easier to see if a more detailed description of the metric and analysis of the results were provided. 

Given the limited technical novelty and discussion amongst reviewers of the desire for more experimental evidence, this work is not quite ready for publication."
iclr_2019_ByeNFoRcK7,"The submission hypothesizes that in typical GAN training the discriminator is too strong, too fast, and thus suggests a modification by which they gradually increases the task difficulty of the discriminator. This is done by introducing (effectively) a new random variable -- which has an effect on the label -- and which prevents the discriminator from solving its task too quickly. 

There was a healthy amount of back-and-forth between the authors and the reviewers which allowed for a number of important clarifications to be made (esp. with regards to proofs, comparison with baselines, etc). My judgment of this paper is that it provides a neat way to overcome a particular difficulty of training GANs, but that there is a lot of confusion about the similarities (of lack thereof) with various potentially simpler alternatives such as input dropout, adding noise to the input etc. I was sometimes confused by the author response as well (they at once suggest that the proposed method reduces overfitting of the discriminator but also state that ""We believe our method does not even try to “regularize” the discriminator""). Because of all this, the significance of this work is unclear and thus I do not recommend acceptance."
iclr_2019_ByePUo05K7,"This paper claims to demonstrate that CNNs, unlike human vision, do not have a bias towards reliance on shape for object recognition. Both AnonReviewer1 and AnonReviewer2 point to fundamental flaws in the paper's argument, which the rebuttal fails to resolve. (AnonReviewer1's criticisms are unfortunately conflated with AnonReviewer1's reluctance to view neuroscience or biological vision as an appropriate topic for ICLR; nonetheless AnonReviewer1's technical criticism stands).

These observations are:

AnonReviewer2:

""Authors have carefully designed a set of experiments which shows CNNs will [overfit] to non-shape features that they added to training images. However, this outcome is not surprising.""

AnonReviewer1:

""The experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias""

""The best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn’t be used to predict the object label. The paper doesn’t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented.""

The AC agrees with both of these observations. CNN behavior is partially a product of the training regime. To examine the scientific question of whether CNNs have similar biases as human vision, the training regimes should be similar. Conversely, if human vision evolved in an environment in which shortcut recognition cues were available via indicator pixels, perhaps it would not have a shape bias.

This paper appears fundamentally flawed in its approach. The results are not informative about differences between human vision and CNNs, nor are they surprising to machine learning practitioners."
iclr_2019_ByeTHsAqtX,"The paper is overally interesting and addresses an important problem, however reviewers ask for more rigorous empirical study and less restrictive settings."
iclr_2019_ByeWdiR5Ym,"The paper presents a modification of the convolution layer, where the convolution weights are generated by another convolution operation. While this is an interesting idea, all reviewers felt that the evaluation and results are not particularly convincing, and the paper is not ready for acceptance."
iclr_2019_ByecAoAqK7,"This paper is essentially an application of dual learning to multilingual NMT. The results are reasonable.

However, reviewers noted that the methodological novelty is minimal, and there are not a large number of new insights to be gained from the main experiments.

Thus, I am not recommending the paper for acceptance at this time."
iclr_2019_ByezgnA5tm,"The paper studies the problem of reinforcement learning under certain constraints on action sequences. The reviewers raised important concerns regarding (1) the general motivation, (2) the particular formulation of constraints in terms of action sequences and (3) the relevance and significance of experimental results. The authors did not submit a rebuttal. Given the concerns raised by the reviewers, I encourage the authors to improve the paper to possibly resubmit to another venue."
iclr_2019_ByfXe2C5tm,"This paper combines Prolog-like reasoning with distributional semantics, applied to natural language question answering. Given the importance of combining neural and symbolic techniques, this paper provides an important contribution. Further, the proposed method complements standard QA models as it can be easily combined with them.

The reviewers and AC note the following potential weaknesses:
(1) The evaluation consisted primarily on small subsets of existing benchmarks, 
(2) the reviewers were concerned that the handcrafted rules were introducing domain information into the model, and (3) were unconvinced that the benefits of the proposed approach were actually complementary to existing neural models. 

The authors addressed a number of these concerns in the response and their revision. They discussed how OpenIE affects the performance, and other questions the reviewers had. Further, they clarified that the rule templates are really high-level/generic and not ""prior knowledge"" as the reviewers had initially assumed. The revision also provided more error analysis, and heavily edited the paper for clarity. Although these changes increased the reviewer scores, a critical concern still remains: the evaluation is not performed on the complete question-answering benchmark, but on small subsets of the data, and the benefits are not significant. This makes the evaluation quite weak, and the authors are encouraged to identify appropriate evaluation benchmarks. 

There is disagreement in the reviewer scores; even though all of them identified the weak evaluation as a concern, some are more forgiving than others, partly due to the other improvements made to the paper. The AC, however, agrees with reviewer 2 that the empirical results need to be sound for this paper to have an impact, and thus is recommending a rejection. Please note that paper was incredibly close to an acceptance, but identifying appropriate benchmarks will make the paper much stronger."
iclr_2019_ByfbnsA9Km,"The paper challenges claims about cross-entropy loss attaining max margin when applied to linear classifier and linearly separable data. This is important in moving forward with the development of better loss functions. 

The main criticism of the paper is that the results are incremental and can be easily obtained from previous work. 

The authors expressed certain concerns about the reviewing process. In the interest of dissipating any doubts, we collected two additional referee reports. 

Although one referee is positive about the paper, four other referees agree that the paper is not strong enough. 



"
iclr_2019_BygANjA5FX,"The method under consideration uses parallel convolutional filter groups per layer, where activations are averaged between the groups, forming ""inner ensembles"".

Reviewers raised a number of concerns, including the increased computational cost for apparently little performance gain, the choice of base architecture (later addressed with additional experiments using WideResNet and ResNeXt), issues of clarity of presentation (some of which were addressed). One reviewer was unconvinced without direct comparison to full ensembles. Another reviewer raised the issue of a missing direct comparison to the most similar method in the literature, maxout (Goodfellow et al, 2013). Authors rebutted this by claiming that maxout is difficult to implement and offering vague arguments for its inferiority to their method.

The AC agrees that a maxout baseline is important here, as it is extremely close to the proposed method and also trivially implemented, and that in light of maxout (and other related methods) the degree of novelty is limited.  The AC also concurs that a full ensemble baseline would strengthen the paper's claims. In the absence of either of these the AC concurs with the reviewers that this work is not suitable for publication at this time."
iclr_2019_BygGNnCqKQ,"The authors propose a scheme to learn a mapping between the discrete space of network architectures into a continuous embedding, and from the continuous embedding back into the space of network architectures. During the training phase, the models regress the number of parameters, and expected accuracy given the continuous embedding. Once trained, the model can be used for compression by first embedding the network structure and then performing gradient descent to maximize accuracy by minimizing the number of parameters. The optimized representation can then be mapped back into the discrete architecture space.
Overall, the main idea of this work is very interesting, and the experiments show that the method has some promise. However, as was noted by the reviewers, the paper could be significantly strengthened by performing additional experiments and analyses. As such, the AC agrees with the reviewers that the paper in its present form is not suitable for acceptance, but the authors are encouraged to revise and resubmit this work to a future venue.
"
iclr_2019_BygIV2CcKm,"This paper proposes and end-to-end trainable architecture for data augmentation, by defining a parametric model for data augmentation (using spatial transformers and GANs) and optimizing validation classification error through the notion of influence functions. Experiments are reported on MNIST and CIfar-10. 

This is a borderline submission. Reviewers found the theoretical framework and problem setup to be solid and promising, but were also concerned about the experimental setup and the lack of clarity in the manuscript. In particular, one would like to evaluate this model against similar baselines (e.g. Ratner et al) on a large-scale classification problem. The AC, after taking these comments into account and making his/her own assessment, recommends rejection at this time, encouraging the authors to address the above comments and resubmit this promising work in the next conference cycle. "
iclr_2019_BygMAiRqK7,"The paper's strength is in that it shows the log likelihood objective is lower bounded by a GAN objective plus an entropy term. The theory is novel (but it seems to relate closely to the work https://arxiv.org/abs/1711.02771.) The main drawback the reviewer raised includes a) it's not clear how tight the lower bound is; b) the theory only applies to a particular subcase of GANs --- it seems that the only reasonable instance that allows efficient generator is the case where Y = G(x)+\xi where \xi is Gaussian noise. The authors addressed the issue a) with some new experiments with linear generators and quadratic loss, but it lacks experiments with deep models which seems to be necessary since this is a critical issue. Based on this, the AC decided to recommend reject and would encourage the authors to add more experiments on the tightness of the lower bound with bigger models and submit to other top venues. 
"
iclr_2019_BygNqoR9tm,"The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm.

Yet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation.

While R1-R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments.

The AC decided to follow the recommendation of R4 as they were the most expert reviewer. The AC thus recommends to ""revise and resubmit"" the paper."
iclr_2019_BygREjC9YQ,"The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters. These algorithms are derived as inference procedures in various dynamical systems. The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop.

This was a controversial paper, and each of the reviewers had a significant back-and-forth with the authors. The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers. Unfortunately, I don't think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don't think it's quite ready for publication at ICLR.

There was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers. The authors believe the added value of this submission is that it recovers features such as momentum and root-mean-square normalization. This would be a very interesting contribution beyond those works. But R2 and R3 feel like these particular features were derived using fairly ad-hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers.

There was a lot of back-and-forth about the correctness of various theoretical claims. But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren't rigorous enough for the paper to stand purely based on the theoretical contributions. 

Unfortunately, the empirical part of the paper is rather lacking. The only experiment reported is on MNIST, and the only result is improved test error. The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken. Simply measuring test error doesn't really get at the benefits of Bayesian approaches, as it doesn't distinguish it from the many other regularizers that have been proposed. Since the proposed method is nearly identical to things like Adam or NAG, I don't see any reason it can't be evaluated on more challenging problems (as reviewers have asked for). 

Overall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at ICLR.
"
iclr_2019_BygRNn0qYX,"AR1 is concerned with the presentation of the paper and the complexity as well as missing discussion on recent  embedding methods. AR2 is concerned about comparison to recent methods and the small size of datasets.  AR3 is also concerned about limited comparisons and evaluations. Lastly, AR4 again points out the poor complexity due to the spectral decomposition. While authors argue that the sparsity can be exploited to speed up computations, AR4 still asks for results of the exact model with/without any approximation, effect of clipping spectrum, time complexity versus GCN, and more empirical results covering all these aspects. On balance, all reviewers seem to voice similar concerns which need to be resolved. However, this requires more than just a minor revision of the manuscript. Thus, at this time, the proposed paper cannot be accepted.

"
iclr_2019_ByghKiC5YX,"I appreciate the willingness of the authors to engage in vigorous discussion about their paper. Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing. The paper considers automated methods for finding errors in text classification models. I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models.

A paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. A paper need not achieve all of these things, any one of them would suffice:

1. Show that the errors found can be used to meaningfully improve the models. 

This requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. Ideally it would also consider alternative, simpler ways to improve the models (e.g. making them larger).

2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non-obvious to a researcher in the field.

This is not applicable here because errors are extremely easy to find on the test set and from labeling more data. If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non-obvious and it seems to work fine, making the Gumbel method unnecessary.

3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight.

I do not believe this submission attempts to show this type of contribution. One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models).

4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models

Given that the authors use human labelers to validate examples this is potentially another path. Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326
However, I believe the paper would need to be rethought and rewritten to make this sort of contribution.


Ultimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper. Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. If the authors believe that the Gumbel algorithm provides application-relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. However, I do not believe the experimental results support this idea.
"
iclr_2019_BygmRoA9YQ,"As the reviewers point out, the paper seems to be below the ICLR publication bar due to low novelty and limited significance. "
iclr_2019_Bygre3R9Fm,"Since the reviewers unanimously recommended rejecting this paper, I am also recommending against publication. The paper considers an interesting problem and expresses some interesting modeling ideas. However, I concur with the reviewers that a more extensive and convincing set of experiments would be important to add. Especially important would be more experiments with simple extensions of previous approaches and much simpler models designed to solve one of the tasks directly, even if it is in an ad hoc way. If we assume that we only care about results, we should first make sure these particular benchmarks are difficult (this should not be too hard to establish more convincingly if it is true) and that obvious things to try do not work well."
iclr_2019_BygrtoC9Km,"The reviewers all appreciate the idea, and the competitive performance, however the consensus is that this is a simple extension of the work of Han et al. and therefore the current submission contains little novelty. There are also numerous issues regarding clarity that the reviewers have pointed out. It is unfortunate that the authors have not engaged in discussion with the reviewers to resolve these, however they are encouraged to consider the reviewer feedback in order to improve the paper."
iclr_2019_BylBfnRqFm,"This paper proposes a meta-learning algorithm that performs gradient-based adaptation (similar to MAML) on a lower dimensional embedding. The paper is generally well-written, and the reviewers generally agree that it has nice conceptual properties. The method also draws similarities to LEO. The main weakness of the paper is with regard to the strength of the experimental results. In a future version of the paper, we encourage the authors to improve the paper by introducing more complex domains or adding experiments that explicitly take advantage of the accessibility of the task embedding.
Without such experiments that are more convincing, I do not think the paper meets the bar for acceptance at ICLR."
iclr_2019_BylBns0qtX,"This paper shows experiments in favor of learning and using heteroscedastic noise models for differentiable Bayes filter. Reviewers agree that this is interesting and also very useful for the community. However, they have also found plenty of issues with the presentation, execution and evaluations shown in the paper. Post rebuttal, one of the reviewer increased their score, but the other has reduced the score. Overall, the reviewers are in agreement that more work is required before this work can be accepted.

Some of existing work on variational inference has not been included which, I agree, is problematic. Simple methods have been compared but then why these methods were chosen and not the other ones, is not completely clear. The paper definitely can improve on this aspect, clearly discussing relationships to many existing methods and then picking important methods to clearly bring some useful insights about learning heteroscedastic noise. Such insights are currently missing in the paper.

Reviewers have given many useful feedback in their review, and I believe this can be helpful for the authors to improve their work. In its current form, the paper is not ready to be accepted and I recommend rejection. I encourage the authors to resubmit this work.
"
iclr_2019_BylRVjC9K7,The reviewers have agreed this paper is not ready for publication at ICLR. 
iclr_2019_Byl_ciRcY7,The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
iclr_2019_BylctiCctX,"This submission proposes an interesting new approach on how to evaluate what features are the most useful during training. The paper is interesting and the proposed approach has the potential to be deployed in many applications, however the work as currently presented is demonstrated in a very narrow domain (stability prediction), as noted by all reviewers. Authors are encouraged to provide stronger experimental validation over more domains to show that their approach can truly improve over existing multitask frameworks."
iclr_2019_Byldr3RqKX,"This paper explores an interpretation of generative models in terms of interventions on their latent variables.  The overall set of ideas seems novel and potentially useful, but the presentation is unclear, the goal of the method seems poorly defined, and the qualitative results (including the videos) are unconvincing.

I recommend you put work into factoring the ideas in this paper into smaller ones.  For instance, definition 1 is a mess.  I would also recommend the use of algorithm boxes."
iclr_2019_BylkG20qYm,"This paper present a framework for creating meaning-preserving adversarial examples. It then proposes two attacks within this framework: one based on k-NN in the word embedding space, and another one based on character swapping. 

Overall, the goal of constructing such meaning-preserving attacks is very interesting. However, it is unclear how successful the proposed approach really is in the context of this goal. 

Additionally, it is not clear how much novelty there is compared to already existing methods that have a very similar aim."
iclr_2019_Byx1VnR9K7,"The paper considers the problem of imitating multi-modal expert demonstrations using a variational auto-encoder to embed demonstrated trajectories into a structured latent space. The problem is important, and the paper is well written. The model is shown to work well on toy examples. However, as pointed out by the reviewers, given that multi-modal has been studied before, the approach should have been compared both in theory and in practice to existing methods and baselines (e.g., InfoGAIL). Furthermore, the technical contribution is somewhat limited as it using an existing model on a new application domain."
iclr_2019_Byx7LjRcYm,"Average score of 3.33, highest score of 4.
The AC recommends rejection.
"
iclr_2019_Byx93sC9tm,"The reviewers in general found the paper approachable, well written and clear.  They noted that the empirical observation of mode collapse in active learning was an interesting insight.  However, all the reviewers had concerns with novelty, particularly in light of Lakshminarayanan et al. who also train ensembles to get a measure of uncertainty.  An interesting addition to the paper might be some theoretical insight about what the model corresponds to when one ensembles multiple models from MC Dropout.  One reviewer noted that it's not clear that the ensemble is capturing the desired posterior.

As a note, I don't believe there is agreement in the community that MC dropout is state-of-the-art in terms of capturing uncertainty for deep neural networks, as argued in the author response (and the abstract).  To the contrary, I believe a variety of papers have improved over the results from that work (e.g. see experiments in Multiplicative Normalizing Flows from over a year ago)."
iclr_2019_ByxAOoR5K7,"The paper studies RL from a rate-distortion (RD) theory perspective.  A new actor-critic algorithm is developed and evaluated on a series of 2D grid worlds.

The paper has some novel idea, and the connection of RL to RD is quite new.  This seems like an interesting direction that is worth further investigation.  On the other hand, all reviewers agreed there is a severe flaw in this work, casting a doubt where RD can be directly applied to an RL setting because the distribution is not fixed (unlike in standard RD).  This issue could have been addressed empirically, by running controlled experiments, something the the paper might include in a future version."
iclr_2019_ByxAcjCqt7,"Reviewers mostly recommended to reject after engaging with the authors, however since not all author answers have been acknowledged by reviewers, I am not sure if there are any remaining issues with the submission. I thus lean to recommend to reject and resubmit. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
"
iclr_2019_ByxF-nAqYX,"This  paper presents an LLE-based unsupervised feature selection approach. While one of the reviewers has acknowledged that the paper is well-written with clear mathematical explanations of the key ideas, it also lacks a sufficiently strong theoretical foundation as the authors have acknowledged in their responses; as well as novelty in its tight connection to LLE. When theoretical backbone is weak, the role of empirical results is paramount, but the paper is not convincing in that regard."
iclr_2019_ByxHb3R5tX,"In considering the reviews and the author response, I would summarize the evaluation of the paper as following: The main idea in the paper -- to combine goal-conditioning with successor features -- is an interesting direction for research, but is somewhat incremental in light of the prior work in the area. Most of the reviewers generally agreed on this point. While a relatively incremental technical contribution could still result in a successful paper with a thorough empirical analysis and compelling results, the evaluation in the paper is unfortunately not very extensive: the provided tasks are very simple, and the difference from prior methods is not very large. All of the tasks are equivalent to either grid worlds or reaching, which are very simple. Without a deeper technical contribution or a more extensive empirical evaluation, I do not think the paper is ready for publication in ICLR."
iclr_2019_ByxLl309Ym,"This paper proposes to approximate arbitrary conditional distribution of a pertained VAE using variational inferences. The paper is technically sound and clearly written. A few variants of the inference network are also compared and evaluated in experiments.

The main problems of the paper are as follows:
1. The motivation of training an inference network for a fixed decoder is not well explained.
2. The application of VI is standard, and offers limited novelty or significance of the proposed method.
3. The introduction of the new term cross-coding is not necessary and does not bring new insights than a standard VI method.

The authors argued in the feedback that the central contribution is using augmented VI to do conditioning inference, similar to Rezende at al, but didn't address reviewers' main concerns. I encourage the authors to incorporate the reviewers' comments in a future revision, and explain why this proposed method bring significant contribution to either address a real problem or improve VI methodology."
iclr_2019_ByxZdj09tX,"The paper presents ""deep deducing"", which means learning the state-action value function of 2 player games from a payoff table, and using the value function by maximizing over the (actionable) inputs at test time.

The paper lacks clarity overall. The method does not contain any new model nor algorithm. The experiments are too weak (easy environments, few/no comparisons) to support the claims.

The paper is not ready for publication at this time."
iclr_2019_ByxkCj09Fm,"The paper proposes to take into accunt the label structure for classification
tasks, instead of a flat N-way softmax. This also lead to a zero-shot setting
to consider novel classes. Reviewers point to a lack of reference to prior
work and comparisons. Authors have tried to justify their choices, but the
overall sentiment is that it lacks novelty with respect to previous approaches.
All reviewers recommend to reject, and so do I."
iclr_2019_ByxmXnA9FQ,"The paper proposes a new framework for out-of-distribution detection, based on variational inference and a prior Dirichlet distribution.

The reviewers and AC note the following potential weaknesses: (1) arguable and not well justified choices of parameters and (2) the performance degradation under many classes (e.g., CIFAR-100).

For (2), the authors mentioned that this is because ""there are more than 20% of misclassified test images"". But, AC rather views it as a limitation of the proposed approach. The out-of-detection detection problem is a one or two classification task, independent of how many classes exist in the neural classifier.

In overall, the proposed idea is interesting and makes sense but AC decided that the authors need more significant works to publish the work."
iclr_2019_Byxr73R5FQ,"Pros:
- simple, sensible subgoal discovery method
- strong inuitions, visualizations
- detailed rebuttal, 15 appendix sections

Cons:
- moderate novelty
- lack of ablations
- assessments don't back up all claims
- ill-justified/mismatching design decisions
- inefficiency due to relying on a random policy in the first phase

There is consensus among the reviewers that the paper is not quite good enough, and should be (borderline) rejected."
iclr_2019_Byxz4n09tQ,"The authors propose a scheme to compress models using student-teacher distillation, where training data are augmented using examples generated from a conditional GAN.
The reviewers were generally in agreement that 1) that the experimental results generally support the claims made by the authors, and 2) that the paper is clearly written and easy to follow.
However, the reviewers also raised a number of concerns: 1) that the experiments were conducted on small-scale tasks, 2) the use of the compression score might be impractical since it would require retraining a compressed model, and is affected by the effectiveness of the compression algorithm which is an additional confounding factor. The authors in their rebuttal address 2) by noting that the student training was not too expensive, but I believe that this cost is task specific. Overall, I think 1) is a significant concern, and the AC agrees with the reviewers that an evaluation of the techniques on large-scale datasets would strengthen the paper.
"
iclr_2019_H1ERcs09KQ,"While this was a borderline paper, concerns about the novelty and significance of the presented work exist on the part of all reviewers, and no reviewer was willing to argue for acceptance. Many good points to the work exist, and a stronger case on these issues would greatly strengthen the paper overall. I look forward to a future submission."
iclr_2019_H1GLm2R9Km,"The reviewers mostly raised two concerns regarding the paper: a) why this algorithm is more interpretability than BP (which is just gradient descent); b) the exposition of the paper is somewhat confusing at various places; c) the lack of large-scale experiment results to show this is practically relevant. In the AC's opinion, a principled kernel-based approach can be counted as interpretable, and there the AC would support the paper if a) is the only concern. However, c) seems to be a serious concern since the paper doesn't seem to have experiments beyond fashion MNIST (e.g., CIFAR is pretty easy to train these days) and doesn't have experiments with convolutional models. Based on c), the AC decided that the paper is not quite ready for acceptance. "
iclr_2019_H1GaLiAcY7,"AR1 finds the paper overly lengthy and ill-focused on contributions of this work. Moreover, AR1 would like to see more results for G-ZSL. AR2 finds the  paper is lacking in clarity, e.g. Eq. 9, and complete definition of the end-to-end decision pipeline is missing. AR2 points that the manuscript relies on GZSL and comparisons to it but other more recent methods could be also cited:
- Generalized Zero-Shot Learning via Synthesized Examples by Verma et al.
- Zero-Shot Kernel Learning by Zhang et al.
- Model Selection for Generalized Zero-shot Learning by Zhang et al.
- Generalized Zero-Shot Learning with Deep Calibration Network by Liu et al.
- Multi-modal Cycle-consistent Generalized Zero-Shot Learning by Felix et al.
- Open Set Learning with Counterfactual Images
- Feature Generating Networks for Zero-Shot Learning
Though, the authors are welcome to find even more relevant papers in google scholar.

Overall, AC finds the paper interesting and finds the idea has some merits. Nonetheless, two reviewers maintained their scores below borderline due to numerous worries highlighted above. The authors are encouraged to work on presentation of this method and comparisons to more recent papers where possible. AC encourages the authors to re-submit their improved manuscript as, at this time, it feels this paper is not ready and cannot be accepted to ICLR.
"
iclr_2019_H1Gfx3Rqtm,"This paper presents a reinforcement learning approach to hierarchical text classification.

Pros: A potentially interesting idea to drive the search process over a hierachical set of labels using reinforcement learning.

Cons: The major concensus among all reviewers was that there were various concerns about experimental results, e.g., apple-to-apple comparisons against prior art (R1), proper tuning of hyper-parameters (R1, R2), the label space is too small (539) to have practical significance compared to tens of thousands of labels that have been used in other related work (R3), and other missing baselines (R3). In addition, even after the rebuttal, some of the technical clarity issues have not been fully resolved, e.g., what the proposed method is actually doing (optimizing F1 metric vs the ability to fix inconsistent labeling problem).

Verdict: 
Reject. While authors came back with many detailed responses, they were not enough to address the major concerns reviewers had about the empirical significance of this work."
iclr_2019_H1M7soActX,"The reviewers point our concerns regarding paper's novelty, theoretical soundness, and empirical strength. The authors provided to clarifications to the reviewers."
iclr_2019_H1MBuiAqtX,"The authors present an interesting approach but there were multiple significant concerns with the clarity of the presentation, and some concern with the significance of the experimental results."
iclr_2019_H1MzKs05F7,"This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally. 

The work is well-written, and all of the reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm-bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.) 

However, many of the reviewers also agreed that the theoretical assumptions - and, in particular, the random initialization of the weights - greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. "
iclr_2019_H1V4QhAqYQ,"The authors propose to use large batch training of neural networks, where each batch contains multiple augmentations of each sample. The experiments demonstrate that this leads to better performance compared to training with small batches. However, as noted by Reviewers 2 and 3, the experiments do not convincingly show where the improvement comes from. Considering that the described technique is very simplistic, having an extensive ablation study and comparison to the strong baselines is essential. The rebuttal didn’t address the reviewers' concerns, and they argue for rejection."
iclr_2019_H1e0-30qKm,"The paper received mixed reviews. It proposes a variant of Siamese network objective function, which is interesting. However, it’s unclear if the performance of the unguided method is much better than other baselines (e.g., InfoGAN). The guided version of the method seems to require much domain-specific knowledge and design of the feature function, which makes the paper difficult to apply to broader cases. 
"
iclr_2019_H1e572A5tQ,"The reviewers raised a number of concerns including the lack of clarity of various parts of the paper, lack of explanation, incremental novelty, and insufficiently demonstrated significance of the proposed. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the paper presents some interesting extensions for multi-agent communication but in its current form the paper lacks explanations, comparisons and discussions. Hence, I cannot recommend this paper for presentation at ICLR."
iclr_2019_H1e6ij0cKQ,"this is an interesting approach to use reinforcement learning to replace CRF for sequence tagging, which would potentially be beneficial when the tag set is gigantic. unfortunately the conducted experiments do not really show this, which makes it difficult to see whether the proposed approach is indeed a viable alternative to CRF for sequence tagging with a large tag set. this sentiment was shared by all the reviewers, and R1 especially pointed out major and minor issues with the submission and was not convinced by the authors' response."
iclr_2019_H1e8wsCqYX,"The paper proposes a new graph-based regularizer to improve the robustness of deep nets. The idea is to encourage smoothness on a graph built on the features at different layers. Experiments on CIFAR-10 show that the method provides robustness over very different types of perturbations such as adversarial examples or quantization. The reviewers raised concerns around the significance of the results, the reliance on a single dataset and the unexplained link between adversarial examples and the regularization. Despite the revision, the reviewers maintain their concerns. For this reason this work is not ready for publication."
iclr_2019_H1eH4n09KX,"The paper presents an algorithm for audio super-resolution using adversarial models along with additional losses, e.g. using auto-encoders and reconstruction losses, to improve the generation process. 

Strengths
- Proposes audio super resolution based on GANs, extending some of the techniques proposed for vision / image to audio.
- The authors improved the paper during the review process by including results from a user study and ablation analysis.

Weaknesses
- Although the paper presents an interesting application of GANs for the audio task, overall novelty is limited since the setup closely follows what has been done for vision and related tasks, and the baseline system. This is also not the first application of GANs for audio tasks. 
- Performance improvement over previously proposed (U-Net) models is small. It would have been useful to also include UNet4 in user-study, as one of the reviewers’ pointed out, since it sounds better in a few cases.
- It is not entirely clear if the method would be an improvement of state-of-the-art audio generative models like Wavenet.

Reviewers agree that the general direction of this work is interesting, but the results are not compelling enough at the moment for the paper to be accepted to ICLR. Given these review comments, the recommendation is to reject the paper."
iclr_2019_H1eMBn09Km,"While the reviewers all agree that this paper proposes an interesting application of GANs, they would like to see clearer explanations of the technical details, more convincing evaluations, and better justifications of the assumptions and practical values of the proposed algorithms. "
iclr_2019_H1eRBoC9FX,"This paper introduces unsupervised meta-learning algorithms for RL. Major concerns of the paper include: 1. Lack of clarity. The presentation of the method can be improved. 2. The motivation and justification of applying unsupervised meta-learning needs to be strengthened. More discussions and better motivating examples may be useful. 3. Experimental details are not sufficient and comparisons may not be sufficient to support the aim. Overall, this paper cannot be accepted yet. "
iclr_2019_H1eadi0cFQ,"The paper proposes a method to escape saddle points by adding and removing units during training. The method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point. The experimental evaluation shows that the proposed method does escape when positioned at a saddle point - as found by the Newton method. The reviewers find the theoretical ideas interesting and novel, but they raised concerns about the method's applicability for typical initializations, the experimental setup, as well as the terminology used in the paper. The title and terminology were improved with the revision, but the other issues were not sufficiently addressed."
iclr_2019_H1ecDoR5Y7,"All three reviewers expressed concerns about the assumptions made for the local stability analysis. The AC thus recommends ""revise and resubmit""."
iclr_2019_H1eiZnAqKm,"The paper analyses GRUs using dynamic systems theory.  The paper is well-written and the theory seems to be solid.

But there is agreement amongst the reviewers that the application of the method might not scale well beyond rather simple 1- or 2-D GRUs (i.e., with one or two GRUs).  This limitation, which is an increasingly serious problem in machine-learning papers, should be solved before the paper should be published.  A very recent extension of the simulations to 16 GRUs improves this, but a rigorous analysis of higher-dimensional systems is pending and poses a considerable block for acceptance."
iclr_2019_H1eqviAqYX,"This paper seeks to shed light on why seq2seq models favor generic replies. The problem is an important one, unfortunately the responses proposed in the paper are not satisfactory. Most reviewers note problems and general lack of rigorousness in the assumptions used to produce the theoretical part of the paper (e.g., strong assumption of independence of generated words). The experiments themselves are not convincing enough to warrant acceptance by themselves."
iclr_2019_H1f7S3C9YQ,"This paper presents a model to identify entity mentions that are synonymous.  This could have utility in practical scenarios that handle entities.

The main criticism of the paper is regarding the baselines used.  Most of the baselines that are compared against are extremely simple.  There is a significant body of literature that models paraphrase and entailment and many of those baselines are missing (decomposable attention, DIIN, other cross-attention mechanisms).  Adding those experiments would make the experimental setup stronger.

There is a bit of a disagreement between reviewers, but I agree with the two reviewers who point out the weakness of the experimental setup, and fixing those issues could improve the paper significantly."
iclr_2019_H1fF0iR9KX,"Strengths:

This paper proposed to use graph-based deep learning methods to apply deep learning techniques to images coming from omnidirectional cameras.

Weaknesses:

The projected MNIST dataset looks very localized on the sphere and therefore does not seem to leverage that much of the global connectivity of the graph
All reviewers pointed out limitations in the experimental results.
There were significant concerns about the relation of the model to the existing literature.  It was pointed out that both the comparison to other methodology, and empirical comparisons were lacking.


The paper received three reject recommendations.  There was some discussion with reviewers, which emphasized open issues in the comparison to and references to existing literature as highlighted by contributed comment from Michael Bronstein.  Work is clearly not mature enough at this point for ICLR, insufficient comparisons / illustrations"
iclr_2019_H1faSn0qY7,"Unfortunately, this paper fell just below the bar for acceptance.  The reviewers all saw significant promise in this work, stating that it is intriguing, ""novel and provides an interesting solution to a challenging problem"" and that ""many interesting use cases are clear"".  AnonReviewer2 particularly argued for acceptance, arguing that the proposed approach provides a very flexible method for incorporating constraints in neural network training.  A concern of AnonReviewer2 was that there was no guarantee that this loss would be convex or converge to an optimum while statisfying the constraints.  The other two reviewers unfortunately felt that while the proposed approach was ""interesting"", ""promising"" and ""intriguing"", the quality of the paper, in terms of exposition, was too low to justify acceptance.  Arguably, it seems the writing doesn't do the idea justice in this case and the paper would ultimately be significantly more impactful if it was carefully rewritten.  "
iclr_2019_H1fevoAcKX,"This paper proposes new heuristics to prune and compress neural networks. The paper is well organized. However, reviewers are concerned that the novelty is relatively limited. The advantage of the proposed method is marginal on ImageNet. What is effective is not very clear. Therefore, recommend for rejection. "
iclr_2019_H1fsUiRcKQ,"The paper combines the ideas of VAT and Bad GAN, replacing the fake samples in Bad GAN objective with VAT generated samples. The motivation behind using the K+1 SSL framework with VAT examples remains unclear, particularly in the light of Prop. 2 which shows smoothness of classifier around the unlabeled examples is enough (which VAT already encourages). R2 and R3 have raised the point of limited insight and lack of motivation behind combining VAT and Bad GAN objectives in this way. R2 and R3 are also concerned about the empirical results which show only marginal improvements over VAT/BadGAN in most settings. 

AC feels that the idea of the paper is interesting but agrees with R2/R3 that the proposed objective is not motivated well enough (what is the precise advantage of using K+1 SSL formulation with VAT examples?). The paper really falls on the borderline and could be improved if this point is addressed convincingly. "
iclr_2019_H1g0piA9tQ,The reviewers agree the paper is not ready for publication. 
iclr_2019_H1gDgn0qY7,"The paper presents a novel view on adversarial examples, where models using
ReLU are inherently sensitive to adversarial examples because ReLU activations
yield a polytope of examples with exactly the same activation. Reviewers
found the finding interesting and novel but argue it is limited in impact.
I also found the idea interesting but the paper could probably be improved
as all reviewers have remarked. Overall, I found it borderline but probably not enough for acceptance."
iclr_2019_H1gFuiA9KX,"although the proposed method could be considered an interesting application to recently popular hypobolic space to word embeddings, it is unclear why this needs to be done so. experiments also do not support why or whether the application of hyperbolic space to word embedding is necessary."
iclr_2019_H1gNHs05FX,"There was discussion of this paper, and the accept reviewer was not willing to argue for acceptance of this paper, while the reject reviewers, specifically pointing to the clarity of the work, argued for rejection. There appear to be many good ideas related to wavelets, and hopefully the authors can work on polishing the paper and resubmitting."
iclr_2019_H1gRM2A5YX,"This paper presents a taxonomic study of neural network architectures, focussing on those which seek to map onto different part of the hierarchy of models of computation (DFAs, PDAs, etc). The paper splits between defining the taxonomy and comparing its elements on synthetic and ""NLP"" tasks (in fact, babi, which is also synthetic). I'm a fairly biased assessor of this sort of paper, as I generally like this topical area and think there is a need for more work of this nature in our field. I welcome, and believe the CFP calls for, papers like this (""learning representations of outputs or [structured] states"", ""theoretical issues in deep learning"")). However, despite my personal enthusiasm, the reviews tell a different story.

The scores for this paper are all over the place, and that's after some attempt at harmonisation! I am satisfied that the authors have had a fair shot at defending their paper and that the reviewers have engaged with the discussion process. I'm afraid the emerging consensus still seems to be in favour of rejection. Despite my own views, I'm not comfortable bumping it up into acceptance territory on the basis of this assessment. Reviewer 1 is the only enthusiastic proponent of the paper, but their statement of support for the paper has done little to sway the others. The arguments by reviewer 3 specifically are quite salient: it is important to seek informative and useful taxonomies of the sort presented in this work, but they must have practical utility. From reading the paper, I share some of this reviewer's concerns: while it is clear to me what use there is the production of studies of the sort presented in this paper, it is not immediately clear what the utility of *this* study is. Would I, practically speaking, be able to make an informed choice as to what model class to attempt for a problem that wouldn't be indistinguishable from common approaches (e.g. ""start simple, add complexity""). I am afraid I agree with this reviewer that I would not.

My conclusion is that there is not a strong consensus for accepting the paper. While I wouldn't mind seeing this work presented at the conference, but due to the competitive nature of the paper selection process, I'm afraid the line must be drawn somewhere. I do look forward to re-reading this paper after the authors have had a chance to improve and expand upon it."
iclr_2019_H1gZV30qKQ,"The paper studies whether the best strategy for transfer learning in RL is to transfer value estimates or policy probabilities. The paper also presents a model-based value-centric (MVC) framework for continuous RL. The reviewers raised concerns regarding (1) the coherence of the story, (2) the novelty and importance of the MVC framework and (3) the significance of the experiments. I encourage the authors to either focus on the algorithmic aspect or the transfer learning aspect and expand on the experimental results to make  them more convincing. I appreciate the changes made to improve the paper, but in its current form the paper is still below the acceptance threshold at ICLR.

PS: in my view one can think of value as (shifted and scaled) log of policy. Hence, it is a bit ambiguous to ask whether to transfer value or policy."
iclr_2019_H1gh_sC9tm,"This paper addresses an important topic and was generally well-written. However, reviewers pointed out serious issues with the evaluation (using weak or poorly chosen attacks), and some conceptual confusions (e.g. conflating adversarial examples with out-of-distribution examples, unsubstantiated claim that adversarial examples lie off the data manifold)."
iclr_2019_H1glKiCqtm,"All three reviewers agree that the research question—should pretrained embeddings be used in code understanding tasks—is a reasonable one. However, there were some early issues with the way in which the paper reported results (involving both metrics and baselines). After some discussion with the reviewers, it seems that the paper now presents a clear picture of the results, but that these results are not sufficiently strong to warrant acceptance. 

I'm wary to turn down a paper over what are basically negative results, but for results like this to be useful to the community, they'd have to come from a very thorough experiment, and they'd have to be accompanied by a frank and detailed discussion. Neither of the two more confident authors are convinced that this paper meets that bar."
iclr_2019_H1gupiC5KQ,"The paper suggests using an ensemble of Q functions for Q-learning. This idea is related to bootstrapped DQN and more recent work on distributional RL and quantile regression in RL. Given the similarity, a comparison against these approaches (or a subset of those) is necessary. The experiments are limited to very simple environment (e.g. swing-up and cart-pole). The paper in its current form does not pass the bar for acceptance at ICLR."
iclr_2019_H1l-SjA5t7,"While the paper has good quality and clarity and the proposed idea seems interesting, all three reviewers agree that the paper needs more challenging experiments to justify the proposed idea. The authors are not able to include additional experiments (such as these based on different transformations) into their revision to better convince the reviewers. In addition, the AC feels that the technical novelty of the paper is rather minor (some incremental change to VAE). In particular, related to some concerns of Reviewer 3, the AC feels the proposed idea is not too much different than introducing certain kind of side-information for supervision; the main novelty seems to be distorting the data itself somehow to provide these side information (which does not seems to be that novel).
"
iclr_2019_H1lADsCcFQ,"On the positive side, this is among the first papers to exploit non-Euclidean geometry, specifically curvature for adversarial learning. However, reviewers are largely in agreement that the technical correctness of this paper is unconvincing despite substantial technical exchanges with the authors.  "
iclr_2019_H1lC8o0cKX,"This paper is borderline for publication for the following reasons:
1) the title is misleading. The majority of the ICLR audience understands by ""spatial structure"" the structure of the external 3D world, as opposed to the position of the sensors in the internal coordinate system of the agent. Though the authors argue that knowing the positions of the sensors eventually leads to learning the 3D world structure, this appears like a leap in the argument. 
2) The equation s=\phi(m) described a mapping from robot postures to sensory states. This means the agent should remain within the same scene. The description of this equation in the manuscript as ""The mapping  can be seen as describing how “the world” transforms changes in motor states into  changes in sensory states ..."" makes this equation appear more general than what it is. s'=\psi(s,m) would be better described by such sentence.


"
iclr_2019_H1lFZnR5YX,"While the idea of revisiting regression-via-classification is interesting, the reviewers all agree that the paper lacks a proper motivating story for why this perspective is important. Furthermore, the baselines are weak, and there is additional relevant work that should be considered and discussed."
iclr_2019_H1lGHsA9KX,"It is a simple but good idea to consider the choice of mini-batch size as a multi-armed bandit problem. Experiments also show a slight improvement compared to the best fixed batch size.

The main concerns from the reviewers are that (1) treating the choice of hyper-parameters as a bandit problem is known and has been exploited in different context, and this paper is limited to the choice of the mini-batch size, (2) the improvement in the test error is not significant. The authors' feedback did not solve the concerns raised by R2.

This paper conveys a nice idea but as the current form it falls slightly below the standard of the ICLR publications. One direction for improvement, as suggested by the reviewer, would be extending the idea for a wider hyper-parameter selection problems."
iclr_2019_H1lIzhC9FX,"The authors propose to tackle the problem of catastrophic forgetting in continual learning by adopting the generative replay strategy with the generator network as an extendable memory module. 

While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues:
(1) poor presentation clarity of the manuscript and incremental technical contribution in light of prior work by Serra et al. (2018); (2) rigorous experiments and in-depth analysis of the baseline models in terms of accuracy, number of parameters, memory demand and model complexity would significantly strengthen the evaluation – see R1’s and R3’s suggestions how to improve; (3) simple strategies such as storing a number of examples and memory replay should not be neglected and evaluated to assess the scope of the contribution. 
Additionally R1 raised a concern that preventing the generator from forgetting should be supported by an ablation study on both, the discriminator and the generator, abilities to remember and to forget.

R1 and R3 provided very detailed and constructive reviews, as acknowledged by the authors. R2 expressed similar concerns about time/memory comparison of different methods, but his/her brief review did not have a substantial impact on the decision.

AC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.
"
iclr_2019_H1lJws05K7,"The paper attempts to extend the recent analysis of random deep networks to alternative activation functions.  Unfortunately, none of the reviewers recommended the paper be accepted.  The current presentation suffers from a lack of clarity and a sufficiently convincing supporting argument/evidence to satisfy the reviewers.  The contribution is perceived as too incremental in light of previous work."
iclr_2019_H1lPUiRcYQ,"This paper proposes a neural network based method for computing committor functions, which are used to understand transitions between stable states in complex systems.
The authors improve over the techniques of Khoo et al. with a method to approximately satisfy boundary conditions and an importance sampling method to deal with rare events.
This is a good application paper, introducing a new application to the ML audience, but the technical novelty is a bit limited. The reviewers see value in the paper, however scaling w.r.t. dimensionality appears to be an issue with this approach."
iclr_2019_H1lS8oA5YQ,"All in all, while the reviewers found that the problem at hand is interesting to study, the submission's contributions in terms of significance/novelty did not rise to the standards for acceptance. The reasoning is most succinctly discussed by R3 who argues that IFS and EFS are basically feature selection and applying them to feature attribution is not particularly novel from a methodological point of view. "
iclr_2019_H1lUOsA9Fm,"The paper describes a WaveNet-like model for MIDI-conditional music audio generation. As noted by all reviewers, the major limitation of the paper is that the method is evaluated on a synthetic dataset. The rebuttal and post-rebuttal discussion didn't change the reviewers' opinion."
iclr_2019_H1ldNoC9tX,"The paper proposes an algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.

The reviewers and AC commonly note the critical limitation of practical value of the paper and results are rather straightforward.

AC decided the paper might not be ready to publish as other contributions are not enough to compensate the issue."
iclr_2019_H1lnJ2Rqt7,"I would like to commend the authors on their work engaging with the reviewers and for working to improve training time. However, there is not enough support among the reviewers to accept this submission. The reviewers raised several important points about the paper, but I believe there are a few other issues not adequately highlighted in the reviews that prevent this work from being accepted:

1. [premises] It has not been adequately established that ""large batch training often times leads to degradation in accuracy"" inherently which is an important premise of this work. Reports from the literature can largely be explained by other things in the experimental protocol. Even the framing of this issue has become confused since, although it may be possible to achieve the same accuracy at any batch size with careful tuning, this might require using (at worst) the same number of steps as the smaller batch size in some cases and thus result in little to no speedup. For example see https://arxiv.org/abs/1705.08741 and recent work in https://arxiv.org/abs/1811.03600 for more information. Even Keskar et al. reported that data augmentation eliminated the solution quality difference between their larger batch size and their smaller batch size experiments which indicates that even if noisiness from small batches serving to regularize training other regularization techniques can serve just as well.

2. [baseline strength] The appropriate baseline is standard minibatch SGD w/momentum (or ADAM or whatever) algorithm with extremely careful tuning of *all* of the hyperparameters. None of the popular learning rate heuristics will always work and other optimization parameters need to be tuned as well. If learning rate decay is used, it should also be tuned especially if one is trying to measure a speedup. The submission does not provide a sufficiently convincing baseline.

3. [measurement protocol] The protocol for measuring a speedup is not convincing without more information on how the baselines were tuned to achieve the same accuracy in the fewest steps. Approximating the protocols in https://arxiv.org/abs/1811.03600 would be one alternative.

Additionally there are a variety of framing of issues around hyperparameter tuning, but, because they are easier to fix, they are not as salient for the decision. 
"
iclr_2019_H1lo3sC9KX,"Improving the staleness of asynchronous SGD is an important topic. This paper proposed an algorithm to restrict the staleness and provided theoretical analysis. However, the reviewers did not consider the proposed algorithm a significant contribution. The paper still did not solve the staleness problem, and it was lack of discussion or experimental comparison with the state of the art ASGD algorithms. Reviewer 3 also found the explanation of the algorithm hard to follow."
iclr_2019_H1ltQ3R9KQ,"The reviewers raised a number of concerns including insufficiently demonstrated benefits of the proposed methodology, lack of explanations, and the lack of thorough and convincing experimental evaluation. The authors’ rebuttal failed to alleviate these concerns fully. I agree with the main concerns raised and, although I also believe that the work can result eventually in a very interesting paper, I cannot suggest it at this stage for presentation at ICLR."
iclr_2019_H1lug3R5FX,"The paper gives a theoretical analysis highlighting the role of codimension on the pervasiveness of adversarial examples. The paper demonstrates that a single decision boundary cannot be robust in different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. 

The main concern with the paper is that most of the theoretical results might have a very restrictive scope and the writing is difficult to follow. 

The authors expressed concerns about a review not being very constructive. In a nutshell, the review in question points out that the theory might be too restrictive, that the experimental section is not very strong, that there are other works on related topics, and that the writing of the paper could be improved. While I understand the disappointing of the authors, the main points here appear to be consistent with the other reviews, which also mention that the theoretical results in this paper are not very general, that the writing is a bit complicated or heavy in mathematics, and not easy to follow, or that it is not clear if the bounds can be useful or easily applied in other work. 

One reviewer rates the paper marginally above the acceptance threshold, while two other reviewers rate the paper below the acceptance threshold. "
iclr_2019_H1x1noAqKX,"The paper addresses the problem of out-of-distribution detection for helping the segmentation process.

The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of ICLR. AC also thinks the authors should avoid using explicit OOD datasets (e.g., ILVRC) due to the nature of this problem. Otherwise, this is a toy binary classification problem.

AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish."
iclr_2019_H1x3SnAcYQ,"This paper extends the DiCE estimator with a better control variate baseline for variance reduction. 
The reviewers all think the paper is fairly clear and well written. However, as the reviews and discussion indicates,  there are several critical issues, including lack of explanation of the choice of baseline, the lack more realistic experiments and a few misleading assertions.  We encourage the authors to rewrite the paper to address these criticism. We believe this work will make a successful submission with proper modification in the future. 
"
iclr_2019_H1xAH2RqK7,"While there was some support for the ideas presented, the majority of the reviewers did not think the submission was ready for presentation at ICLR. Concerns raised included that the experiments needed more work, and the paper needs to do a better job of distinguishing the contributions beyond those of past work."
iclr_2019_H1xEtoRqtQ,"As all the reviewers have highlighted, there is some interesting analysis in this paper on understanding which models can be easier to complete. The experiments are quite thorough, and seem reproducible. However, the biggest limitation---and the ones that is making it harder for the reviewers to come to a consensus---is the fact that the motivation seems mismatched with the provided approach. There is quite a lot of focus on security, and being robust to an adversary. Model splitting is proposed as a reasonable solution. However, the Model Completion hardness measure proposed is insufficiently justified, both in that its not clear what security guarantees it provides nor is it clear why training time was chosen over other metrics (like number of samples, as mentioned by a reviewer). If this measure had been previously proposed, and the focus of this paper was to provide empirical insight, that might be fine, but that does not appear to be the case. This mismatch is evident also in the writing in the paper. After the introduction, the paper largely reads as understanding how retrainable different architectures are under which problem settings, when replacing an entire layer, with little to no mention of security or privacy. 

In summary, this paper has some interesting ideas, but an unclear focus. The proposed strategy should be better justified. Or, maybe even better for the larger ICLR audience, the provided analysis could be motivated for other settings, such as understanding convergence rates or trainability in neural networks."
iclr_2019_H1xEwsR9FX,"The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier.

In practice, the synthetic experiments seem to show a greater improvement than appears in real data.  There are concerns about the clarity, lack of theoretical proofs, and at times overstated claims that do not have sufficient support.

The ratings before the rebuttal and discussion were 7-4-6.  After, R1 adjusted their score from 6 to 4.  R2 initially gave a 7 but later said ""I think the authors missed an opportunity here. I rated it as an accept, because I saw what it could have been after a good revision. The core idea is good, but fully agree with R1 and R3 that the paper needs work (which the authors were not willing to do). I checked the latest revision (as of Monday morning). None of R3's writing/claims issues are fixed, neither were my additional experimental requests, not even R1's typos."" There is therefore a consensus among reviewers for reject.
"
iclr_2019_H1xQSjCqFQ,"The reviewers overall agree that excitation dropout is a novel idea that seems to produce good empirical performance. However, they remain optimistic, but unconvinced by the experiments in their current form. The authors have done an admiral job of addressing this through more experiments, including providing error bars, however it seems as though the reviewers still require more. I would recommend creating tables of architecture x dropout technique, where dropout technique includes information dropout, adaptive dropout, curriculum dropout, and standard dropout, across several standard datasets. Alternatively, the authors could try to be more ambitious and classify Imagenet. Essentially, it seems as though the current small-scale datasets have become somewhat saturated, and therefore the bar for gauging a new method on them is higher in terms of experimental rigor. This means the best strategy is to either try more difficult benchmarks, or be extremely thorough and complete in your experiments.

Regarding the wide resnet result, while I can appreciate that the original version published with higher errors, the later draft should still be taken into account as it has a) been out for a while now and b) can been reproduced in open source implementations (e.g., https://github.com/szagoruyko/wide-residual-networks)."
iclr_2019_H1xk8jAqKQ,"
-pros:
- good, sensible idea
- good evaluations on the domains considered
- good analysis

-cons:
- novelty, broader evaluation

I think this is a good and interesting paper and I appreciate the authors' engagment with the reviewers.  I agree with the authors that it is not fair to compare their work to a blog post which hasn't been published and I have taken this into account.  However, there is still concern among the reviewers about the strength of the technical contribution and the decision was made not to accept for ICLR this year. "
iclr_2019_H1xmqiAqFm,"The paper analyzes the performance of CNN models when data is mislabelled in different manners.

The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of ICLR.

AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish."
iclr_2019_H1xpe2C5Km,"This paper proposes a method for tracing activations in a capsule-based network in order to obtain semantic segmentation from classification predictions.

Reviewers 1 and 2 rate the paper as marginally above threshold, while Reviewer 3 rates it as marginally below. Reviewer 3 particularly points to experimental validation as a major weakness, stating: ""not sure if the method will generalize well beyond MNIST"", ""I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only.""

The AC shares these concerns and does not believe the current experimental validation is sufficient. MNIST is a toy dataset, and may have been appropriate for introducing capsules as a new concept, but it is simply not difficult enough to serve as a quantitative benchmark to distinguish capsule performance from U-Net. U-Net and Tr-CapsNet appear to have similar performance on both MNIST and the hippocampus dataset; the relatively small advantage to Tr-CapsNet is not convincing.

Furthermore, as Reviewer 1 suggests, it would seem appropriate to include experimental comparison to other capsule-based segmentation approaches (e.g. LaLonde and Bagci, Capsules for Object Segmentation, 2018). This related work is mentioned, but not used as an experimental baseline.
"
iclr_2019_H1z_Z2A5tX,"This paper analyses the dynamics of RNNs, cq GRU and LSTM.  

The paper is mostly experimental w.r.t. the difficulty of training RNNs; this is also caused by the fact that the theoretical foundations of the paper seem not to be solid enough.  Experimentation with CIFAR10 is not completely stable.

The review results make the paper balance at the middle.  The merit of the paper for the greater community is doubted, in its current form."
iclr_2019_H1zxjsCqKQ,"This manuscript proposes a gradient-based learning scheme for non-differentiable and non-decomposable metrics. The key idea is to optimize a soft predictor directly (instead of aiming for a deterministic predictor), which results in a differentiable loss for many of these metrics. Theoretical results are provided which describe the performance of this approach.

The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation and novelty as related to already published work. There was also a concern about the usefulness the main theoretical results due to asymptotic assumptions. The manuscript would be significantly strengthened if the reliance on infinite sample sizes is resolved, or sufficient empirical evidence is provided which suggests that the asymptotic issues are not practically significant."
iclr_2019_HJG0ojCcFm,"This paper was reviewed by three experts. Initially, the reviews were mixed with several concerns raised. After the author response, there continue to be concerns about need for significantly more experiments. If this were a journal, it is clear that recommendation would be ""major revision"". Since that option is not available and the paper clearly needs another round of reviews, we must unfortunately reject. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue. "
iclr_2019_HJG1Uo09Fm,"This paper proposes a meta-learning algorithm for reinforcement learning that incorporates expert demonstrations. The objective is to improve sample efficiency, which is an important problem. 

The referees find the approach well-motivated and pertinent, but the theoretical and practical contributions of the paper too slim. A concern was also raised in regard to reproducibility of the results, missing details about the implementation and comparisons with previous results. 

The authors did not respond to the reviews. 

The four referees are not convinced by this paper, with ratings from strong reject to ok, but not good enough. "
iclr_2019_HJG7m2AcF7,"The paper proposes to build word representation based on a histogram over context word vectors, allowing them to measure distances between words in terms of optimal transport between these histograms. An empirical analysis shows that the proposed approach is competitive with others on semantic textual similarity and hypernym detection tasks. While the idea is definitely interesting, the paper would be streghten by a more extensive empirical analysis. "
iclr_2019_HJGtFoC5Fm,"This paper has received reviews from multiple experts who raise a litany of issues. These have been addressed quite convincingly by the authors, but I believe that ultimately this work needs to go through another round of reviewing, and this cannot be achieved in the context of ICLR's reviewing setup. I look forward to reading the final version of the paper in the near future."
