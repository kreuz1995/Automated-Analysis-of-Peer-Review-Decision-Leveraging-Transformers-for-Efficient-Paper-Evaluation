[
  {
    "paper_id": "iclr_2018_ryQu7f-RZ",
    "paper_title": "On the Convergence of Adam and Beyond",
    "paper_acceptance": "accepted-oral-papers",
    "meta_review": "This paper analyzes a problem with the convergence of Adam, and presents a solution. It identifies an error in the convergence proof of Adam (which also applies to related methods such as RMSProp) and gives a simple example where it fails to converge. The paper then repairs the algorithm in a way that guarantees convergence without introducing much computational or memory overhead. There ought to be a lot of interest in this paper: Adam is a widely used algorithm, but sometimes underperforms SGD on certain problems, and this could be part of the explanation. The fix is both principled and practical. Overall, this is a strong paper, and I recommend acceptance.\n",
    "reviews": [
      {
        "review_id": "HkhdRaVlG",
        "comment": "The paper presents three contributions: 1) it shows that the proof of convergence Adam is wrong; 2) it presents adversarial and stochastic examples on which Adam converges to the worst possible solution (i.e. there is no hope to just fix Adam's proof); 3) it proposes a variant of Adam called AMSGrad that fixes the problems in the original proof and seems to have good empirical properties.  The contribution of this paper is very relevant to ICLR and, as far as I know, novel. The result is clearly very important for the deep learning community. I also checked most of the proofs and they look correct to me: The arguments are quite standard, even if the proofs are very long.  One note on the generality of the results: the papers states that some of the results could apply to RMSProp too. However, it has been proved that RMSProp with a certain settings of its parameters is nothing else than AdaGrad (see Section 4 in  Mukkamala and Hein, ICML'17). Hence, at least for a certain setting of its parameters, RMSProp will converge. Of course, the proof in the ICML paper could be wrong, I did not check that...  A general note on the learning rate: The fact that most of these algorithms are used with a fixed learning rate while the analysis assume a decaying learning rate should hint to the fact that we are not using the right analysis. Indeed, all these variants of AdaGrad did not really improve the AdaGrad's regret bound. In this view, none of these algorithms contributed in any meaningful way to our understanding of the optimization of deep networks *nor* they advanced in any way the state-of-the-art for optimizing convex Lipschitz functions. On the other hand, analysis of SGD-like algorithms with constant step sizes are known. See, for example, Zhang, ICML'04 where linear convergence is proved in a neighbourhood of the optimal solution for strongly convex problems. So, even if I understand this is not the main objective of this paper, it would be nice to see a discussion on this point and the limitations of regret analysis to analyse SGD algorithms.  Overall, I strongly suggest to accept this paper.   Suggestions/minor things: - To facilitate the reader, I would state from the beginning what are the common settings of beta_1 and beta_2 in Adam. This makes easier to see that, for example, the condition of Theorem 2 is verified. - \\\\hat{v}_{0} is undefined in Algorithm 2. - The graphs in figure 2 would gain in readability if the setting of each one of them would be added as their titles. - McMahan and Streeter (2010) is missing the title. (Also, kudos for citing both the independent works on AdaGrad) - page 11, last equation, 2C-4=2C-4. Same on page 13. - Lemma 4 contains x_1,x_2,z_1, and z_2: are x_1 and z_1 the same? also x_2 and z_2?",
        "rating": 9,
        "confidence": 5,
        "writer": "official_reviewer"
      },
      {
        "review_id": "H15qgiFgf",
        "comment": "This work identifies a mistake in the existing proof of convergence of Adam, which is among the most popular optimization methods in deep learning. Moreover, it gives a simple 1-dimensional counterexample with linear losses on which Adam does not converge. The same issue also affects RMSprop, which may be viewed as a special case of Adam without momentum. The problem with Adam is that the \"learning rate\" matrices V_t^{1/2}/alpha_t are not monotonically decreasing. A new method, called AMSGrad is therefore proposed, which modifies Adam by forcing these matrices to be decreasing. It is then shown that AMSGrad does satisfy essentially the same convergence bound as the one previously claimed for Adam. Experiments and simulations are provided that support the theoretical analysis.  Apart from some issues with the technical presentation (see below), the paper is well-written.  Given the popularity of Adam, I consider this paper to make a very interesting observation. I further believe all issues with the technical presentation can be readily addressed.    Issues with Technical Presentation:  - All theorems should explicitly state the conditions they require   instead of referring to \"all the conditions in (Kingma & Ba, 2015)\". - Theorem 2 is a repetition of Theorem 1 (except for additional   conditions). - The proof of Theorem 3 assumes there are no projections, so this   should be stated as part of its conditions. (The claim in footnote 2   that they can be handled seems highly plausible, but you should be up   front about the limitations of your results.) - The regret bound Theorem 4 establishes convergence of the optimization   method, so it plays the role of a sanity check. However, it is   strictly worse than the regret bound O(sqrt{T}) for online gradient   descent [Zinkevich,2003], so it cannot explain why the proposed   AMSgrad method might be adaptive. (The method may indeed be adaptive   in some sense; I am just saying the *bound* does not express that.   This is also not a criticism of the current paper; the same remark   also applies to the previously claimed regret bound for Adam.) - The discussion following Corollary 1 suggests that sum_i   hat{v}_{T,i}^{1/2} might be much smaller than d G_infty. This is true,   but we should always expect it to be at least a constant, because   hat{v}_{t,i} is monotonically increasing by definition of the   algorithm, so the bound does not get better than O(sqrt(T)).   It is also suggested that sum_i ||g_{1:T,i}|| = sqrt{sum_{t=1}^T   g_{t,i}^2} might be much smaller than dG_infty, but this is very   unlikely, because this term will typically grow like O(sqrt{T}),   unless the data are extremely sparse, so we should at least expect   some dependence on T. - In the proof of Theorem 1, the initial point is taken to be x_1 = 1,   which is perfectly fine, but it is not \"without loss of generality\",   as claimed. This should be stated in the statement of the Theorem. - The proof of Theorem 6 in appendix B only covers epsilon=1. If it is   \"easy to show\" that the same construction also works for other   epsilon, as claimed, then please provide the proof for general   epsilon.   Other remarks:  - Theoretically, nonconvergence of Adam seems a severe problem. Can you   speculate on why this issue has not prevented its widespread adoption?   Which factors might mitigate the issue in practice? - Please define g_t \\\\circ g_t and g_{1:T,i} - I would recommend sticking with standard linear algebra notation for   the sqrt and the inverse of a matrix and simply using A^{-1} and   A^{1/2} instead of 1/A and sqrt{A}. - In theorems 1,2,3, I would recommend stating the dimension (d=1) of   your counterexamples, which makes them very nice!  Minor issues:  - Check accent on Nicol\\\\`o Cesa-Bianchi in bibliography. - Near the end of the proof of Theorem 6: I believe you mean Adam   suffers a \"regret\" instead of a \"loss\" of at least 2C-4.   Also 2C-4=2C-4 is trivial in the second but last display. ",
        "rating": 8,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "Hyl2iJgGG",
        "comment": "This paper examines the very popular and useful ADAM optimization algorithm, and locates a mistake in its proof of convergence (for convex problems). Not only that, the authors also show a specific toy convex problem on which ADAM fails to converge. Once the problem was identified to be the decrease in v_t (and increase in learning rate), they modified the algorithm to solve that problem. They then show the modified algorithm does indeed converge and show some experimental results comparing it to ADAM.  The paper is well written, interesting  and very important given the popularity of ADAM.   Remarks: - The fact that your algorithm cannot increase the learning rate seems like a possible problem in practice. A large gradient at the first steps due to bad initialization can slow the rest of training. The experimental part is limited, as you state \"preliminary\", which is a unfortunate for a work with possibly an important practical implication. Considering how easy it is to run experiments with standard networks using open-source software, this can easily improve the paper. That being said, I understand that the focus of this work is theoretical and well deserves to be accepted based on the theoretical work.  - On page 14 the fourth inequality not is clear to me.  - On page 6 you talk about an alternative algorithm using smoothed gradients which you do not mention anywhere else and this isn't that clear (more then one way to smooth). A simple pseudo-code in the appendix would be welcome.  Minor remarks: - After the proof of theorem 1 you jump to the proof of theorem 6 (which isn't in the paper) and then continue with theorem 2. It is a bit confusing. - Page 16 at the bottom v_t= ... sum beta^{t-1-i}g_i should be g_i^2 - Page 19 second line, you switch between j&t and it is confusing. Better notation would help. - The cifarnet uses LRN layer that isn't used anymore.",
        "rating": 8,
        "confidence": 3,
        "writer": "official_reviewer"
      }
    ]
  },
  {
    "paper_id": "iclr_2018_Hk2aImxAb",
    "paper_title": "Multi-Scale Dense Networks for Resource Efficient Image Classification",
    "paper_acceptance": "accepted-oral-papers",
    "meta_review": "As stated by reviewer 3 \"This paper introduces a new model to perform image classification with limited computational resources at test time. The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer.\"\nAs stated by reviewer 2 \"My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017). \".  The authors assert novelty in the sense that they provide a solution to improve computational efficiency and focus on this aspect of the problem. Overall, the technical innovation is not huge, but I think this could be a very useful idea in practice.\n",
    "reviews": [
      {
        "review_id": "rJSuJm4lG",
        "comment": "This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time. The paper is very well written, experiments are clearly presented and convincing and, most importantly, the research question is exciting (and often overlooked).   My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017). The authors add a hierarchical, multi-scale structure and show that DenseNet can better cope with it than ResNet (e.g., Fig. 3). They investigate pros and cons in detail adding more valuable analysis in the appendix. However, this work is basically an extension of the DenseNet approach with a new problem statement and additional, in-depth analysis.     Some more minor comments:   -\tPlease enlarge Fig. 4.  -\tI did not fully grasp the details in the first \"Solution\" paragraph on P5. Please extend and describe in more detail.   In conclusion, this is a very well written paper that designs the network architecture (of DenseNet) such that it is optimized to include CPU budgets at test time. I recommend acceptance to ICLR18.        ",
        "rating": 8,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "SJ7lAAYgG",
        "comment": "This paper presents a method for image classification given test-time computational budgeting constraints.  Two problems are considered:  \"any-time\" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.  A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.  In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.  Evaluations are performed on ImageNet and CIFAR-100.  I would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?  Also on p.6 I'm not entirely clear on how the \"network reduction\" is performed --- it looks like finer scales are progressively dropped in successive blocks, but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is \"lazy evaluation\").  A picture would help here, showing where the depth-layers are divided between blocks.  I was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result:  It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.  So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).  This is fine, but could perhaps be pointed out if that is indeed the case.  Overall, this seems like a natural and effective approach, and achieves good results. ",
        "rating": 7,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "rk6gRwcxz",
        "comment": "This paper introduces a new model to perform image classification with limited computational resources at test time. The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer. The multiple classifiers allow for a finer selection of the amount of computation needed for a given input image. The multi-scale representation allows for better performance at early stages of the network. Finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following layers. A thorough evaluation on ImageNet and Cifar100 shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of computation.  Pros: - The presentation is clear and easy to follow. - The structure of the network is clearly justified in section 4. - The use of dense connectivity to avoid the loss of performance of using early-exit classifier is very interesting. - The evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenarios. - Results are very promising, with 5x speed-ups and same or better accuracy that previous models. - The extensive experimentation shows that the proposed network is better than previous approaches under different regimes.  Cons: - Results about the more efficient densenet* could be shown in the main paper  Additional Comments: - Why in training you used logistic loss instead of the more common cross-entropy loss? Has this any connection with the final performance of the network? - In fig. 5 left for completeness I would like to see also results for DenseNet^MT and ResNet^MT - In fig. 5 left I cannot find the 4% and 8% higher accuracy with 0.5x10^10 to 1.0x10^10 FLOPs, as mentioned in section 5.1 anytime prediction results - How the budget in terms of Mul-Adds is actually estimated?  I think that this paper present a very powerful approach to speed-up the computational cost of a CNN at test time and clearly explains some of the common trade-offs between speed and accuracy and how to improve them. The experimental evaluation is complete and accurate.   ",
        "rating": 10,
        "confidence": 4,
        "writer": "official_reviewer"
      }
    ]
  },
  {
    "paper_id": "iclr_2018_H196sainb",
    "paper_title": "Word translation without parallel data",
    "paper_acceptance": "accepted-poster-papers",
    "meta_review": "There is significant discussion on this paper and high variance between reviewers:  one reviewer gave the paper a low score.  However the committee feels that this paper should be accepted at the conference since it provides a better framework for reproducibility, performs more large scale experiments than prior work.  One small issue the lack of comparison in terms of empirical results between this work and Zhang et al's work, but the responses provided to both the reviewers and anonymous commenters seem to be satisfactory.",
    "reviews": [
      {
        "review_id": "SyE3AHgxG",
        "comment": "This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages. The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping). The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks.  The paper presents an interesting approach which achieves good performance. The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed.  My one concern is that the supervised approach that the paper compares to is limited: it is trained on a small fixed number of anchor points, while the unsupervised method uses significantly more words. I think the paper's comparisons are valid, but the abstract and introduction make very strong claims about outperforming \"state-of-the-art supervised approaches\". I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened. The same holds for statements like \"... our method is a first step ...\", which is very hard to justify. I also do not think it is necessary to over-sell, given the solid work in the paper.  Further comments, questions and suggestions: - It might be useful to add more details of your actual approach in the Abstract, not just what it achieves. - Given you use trained word embeddings, it is not a given that the monolingual word embedding spaces would be alignable in a linear way. The actual word embedding method, therefore, has a big influence on performance (as you show). Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains/data with similar co-occurrence statistics, in order for your method to be appropriate? - Would it be possible to add weights to the terms in eq. (6), or is this done implicitly? - How were the 5k source words for Procrustes supervised baseline selected? - Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces? - Do you think your approach would benefit from having a few parallel training points?  Some minor grammatical mistakes/typos (nitpicking): - \"gives a good performance\" -> \"gives good performance\" - \"Recent works\", \"several works\", \"most works\", etc. -> \"recent studies\", \"several studies\", etc. - \"i.e, the improvements\" -> \"i.e., the improvements\"  The paper is well-written, relevant and interesting. I therefore recommend that the paper be accepted.  ",
        "rating": 9,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "rJEg3TtxM",
        "comment": "The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique. The task is interesting and relevant, especially for in low-resource language pair settings.  The paper, however, misses comparison against important work from the literature that is very relevant to their task \u2014 decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA.   The former set of works, while focused on machine translation also learns a translation table in the process. Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions. Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction. The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods. While some of the bilingual lexicon extraction works are cited (Haghighi et al., 2008; Artetxe et al., 2017), they do not demonstrate how their approach performs against these baseline methods. Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach.  The proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works.   For the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach.  The paper mentions that the approach is \u201cunsupervised\u201d. However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages. How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?",
        "rating": 3,
        "confidence": 5,
        "writer": "official_reviewer"
      },
      {
        "review_id": "H1Qhqm9ez",
        "comment": "An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning.  The paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general: it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times.  There are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016).  The evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings. Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest.  In my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision.",
        "rating": 8,
        "confidence": 3,
        "writer": "official_reviewer"
      }
    ]
  },
  {
    "paper_id": "iclr_2018_r1ZdKJ-0W",
    "paper_title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking",
    "paper_acceptance": "accepted-poster-papers",
    "meta_review": "The paper proposes a method to embed graph nodes into a gaussian distribution rather than the standard latent vector embeddings. The reviewers concur that the method is interesting and the paper is well-written especially after the opportunity to update.",
    "reviews": [
      {
        "review_id": "BkNHltugM",
        "comment": "This paper is well-written and easy follow. I didn't find serious concern and therefore suggest an acceptance.  Pros Methodology 1. inductive ability: can generalize to unseen nodes without any further training 2. personalized ranking: the model uses natural ranking that embeddings of closer nodes (considers node pairs of any distance) should be closer in the embedding space, which is more general than prevailing first and second order proximity 3. sampling strategy: the proposed node-anchored sampling method gives unbiased estimates of loss function and successfully reduces the time complexity  Experiment 1. Evaluation tasks including link prediction and node classification are conducted across multiple datasets with additional parameter sensitivity and missing-link robustness experiments 2. Compared with various baselines with diverse model designs such as GCN and node2vec as well as compared with naive baseline (using original node attributes as model inputs) 3. Demonstrated the model captures uncertainties and the learned uncertainties can be used to infer latent dimensions Related Works The survey of related work is sufficiently wide and complete.  Cons Authors should include which kind of model is used to do the link prediction task given embedding vectors from different models as inputs.",
        "rating": 7,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "HJhWAwsgM",
        "comment": "The paper proposes to learn Gaussian embeddings for directed attributed graph nodes. Each node is associated to a Gaussian representation (mean and diagonal covariance matrix). The mean and diagonal representations for a node are learned as functions of the node attributes. The algorithm is unsupervised and optimizes a ranking loss: nodes at distance 1 in the graph are closer than nodes at distance 2, etc. Distance between nodes representation is measured via KL divergence. The ranking loss is a square exponential loss proposed in energy based models. In order to limit the complexity, the authors propose the use of a sampling scheme and show the convergence in expectation of this strategy towards the initial loss. Experiments are performed on two tasks: link prediction and node classification. Baselines are unsupervised projection methods and a (supervised) logistic regression. An analysis of the algorithm behavior is then proposed. The paper reads well. Using a ranking loss based on the node distance together with Gaussian embeddings is probably new, even if the novelty is not that big.  The comparisons with unsupervised methods shows that the algorithm learns relevant representations. Do you have a motivation for using this specific loss Eq. (1), or is it a simple heuristic choice? Did you try other ranking losses? For the link prediction experiments, it is not indicated how you rank candidate links for the different methods and how you proceed with the logistic. Did you compare with a more complex supervised model than the logistic? Fort the classification tasks, it would be interesting to compare to supervised/ semi-supervised embedding methods. The performance of unsupervised embeddings for graph node classification is usually much lower than supervised/ semi-supervised methods.  Having a measure of the performance gap on the different tasks would be informative. Concerning the analysis of uncertainity, discovering that uncertainty is higher for nodes with neighbors of distinct classes is interesting. In your setting this might simply be caused by the difference in the node attributes. I was not so convinced by the conclusions on the dimensionality of the hidden representation space. An immediate conclusion of this experiment would be that only a small dimensional latent space is needed. Did you experiment with this? Detailed comments: The title of the paper is \u201cDeep \u2026\u201d. There is nothing Deep in the proposed model since the NN are simple one layer MLPs. This is not a criticism, but the title should be changed. There is a typo in KL definition (d should be replaced by the dimension of the embeddings). Probably another typo: the energy should be + D_KL and not \u2013D_KL. The paragraph below eq (1) should be modified accordingly. All the figures are too small to see anything and should be enlarged. Overall the paper brings some new ideas. The experiments are fine, but not so conclusive. ",
        "rating": 6,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "H1j0rCeZz",
        "comment": "This paper proposes Graph2Gauss (G2G), a node embedding method that embeds nodes in attributed graphs (can work w/o attributes as well) into Gaussian distributions rather than conventionally latent vectors. By doing so, G2G can reflect the uncertainty of a node's embedding. The authors then use these Gaussian distributions and neighborhood ranking constraints to obtain the final node embeddings. Experiments on link prediction and node classification showed improved performance over several strong embedding methods. Overall, the paper is well-written and the contributions are remarkable. The reason I am giving a less possible rating is that some statements are questionable and can severely affect the conclusions claimed in this paper, which therefore requires the authors' detailed response. I am certainly willing to change my rating if the authors clarify my questions.  Major concern 1: Is the latent vector dimension L really the same for G2G and other compared methods?  In the first paragraph of Section 4, it is stated that \"in all experiments if the competing techniques use an embedding of dimensionality L, G2G\u2019s embedding is actually only half of this dimensionality so that the overall number of \u2019parameters\u2019 per node (mean vector + variance terms) matches L.\"  This setting can be wrong since the degree of freedom of a L-dim Gaussian distribution should be L+L(L-1)/2, where the first term corresponds to the mean and the second term corresponds to the covariance. If I understand it correctly, when any compared embedding method used an L-dim vector, the authors used the dimension of L/2. But this setting is wrong if one wants the overall number of \u2019parameters\u2019 per node (mean vector + variance terms) matches L, as stated by the authors. Fixing L, the equivalent dimension L_G2G for G2G should be set such that L_G2G +L_G2G (L_G2G -1)/2=L, not 2*L_G2G=L.  Since this setting is universal to the follow-up analysis and may severely degrade the performance of GSG due to less embedding dimensions, I hope the authors can clarify this point.  Major concern 2: The claim on inductive learning Inductive learning is one of the major contributions claimed in this paper. The authors claim G2G can learn an embedding of an unseen node solely based on their attributes. However, is it not clear why this can be done. In the learning stage of Sec. 3.3, the attributes do not seem to play a role in the energy function. Also, since no algorithm descriptions are available, it's not clear how using only an unseen node's attributes can yield a good embedding under G2G work (so does Sec. 4.5).  Moreover, how does it compare to directly using raw user attributes for these tasks?  Minor concern/suggestions: The \"similarity\" measure in section 3.1 using KL divergence should be better rephased by \"dissimilarity\" measure. Otherwise, one has a similarity measure $Delta$ and wants it to increase as the hop distance k decreases (closer nodes are more similar). But the ranking constraints are somewhat counter-intuitive because you want $Delta$ to be small if nodes are closer. There is nothing wrong with the ranking condition, but rather an inconsistency between the use of \"similarity\" measure for KL divergence.  ",
        "rating": 7,
        "confidence": 3,
        "writer": "official_reviewer"
      }
    ]
  },
  {
    "paper_id": "iclr_2018_BJjquybCW",
    "paper_title": "The loss surface and expressivity of deep convolutional neural networks",
    "paper_acceptance": "workshop-papers",
    "meta_review": "Dear authors,\n\nWhile I appreciate the result that a convolutional layer can have full rank output, this allowing a dataset to be classified perfectly under mild conditions, the fact that all reviewers expressed concern about the statement is an indication that the presentation sill needs quite a bit of work.\n\nThus, I recommend it as an ICLR workshop paper.",
    "reviews": [
      {
        "review_id": "BkIW6fYxz",
        "comment": "This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks. I summarize the results as follows:  (1) Under certain assumptions, if the network contains a \"wide\u201c hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples.  (2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data.  (3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum. These solutions achieve zero squared-loss.  I would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1). Intuitively, (1) is an easy result. Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent. The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small.  Result (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima; (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure; (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero. If an optimization algorithm falls onto these solutions, it will be hard to escape.  Overall, the paper presents several incremental improvement over existing theories. However, the novelty and the technical contribution are not sufficient for securing an acceptance.  ",
        "rating": 4,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "rkvS6-9gG",
        "comment": "This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights.",
        "rating": 7,
        "confidence": 2,
        "writer": "official_reviewer"
      },
      {
        "review_id": "S136E0hZf",
        "comment": "This paper analyzes the loss function and properties of CNNs with one \"wide\" layer, i.e., a layer with number of neurons greater than the train sample size. Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums. I like the presentation and writing of this paper. However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \"wide\"-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected. The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy. This is not surprising. It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. ",
        "rating": 5,
        "confidence": 2,
        "writer": "official_reviewer"
      },
      {
        "review_id": "HJ0nIZkfM",
        "comment": "This paper presents an analysis of convolutional neural networks from the perspective of how the rank of the features is affected by the kinds of layers found in the most popular networks. Their analysis leads to the formulation of a certain theorem about the global minima with respect to parameters in the latter portion of the network.  The authors ask important questions, but I am not sure that they obtain important answers. On the plus side, I'm glad that people are trying to further our understanding our neural networks, and I think that their investigation is worthy of being published.  They present a collection of assumptions, lemmas, and theorems. They have no choice but to have assumptions, because they want to abstract away the \"data\" part of the analysis while still being able to use certain properties about the rank of the features at certain layers.  Most of my doubts about this paper come from the feeling that equivalent results could be obtained with a more elegant argument about perturbation theory, instead of something like the proof of Lemma A1. That being said, it's easy to voice such concerns, and I'm willing to believe that there might not exist a simple way to derive the same results with an approach more along the line of \"whatever your data, pick whatever small epsilon, and you can always have the desired properties by perturbing your data by that small epsilon in a random direction\". Have the authors tried this ?  I'm not sure if the authors were the first to present this approach of analyzing the effects of convolutions from a \"patch perspective\", but I think this is a clever approach. It simplifies the statement of some of their results. I also like the idea of factoring the argument along the concept of some critical \"wide layer\".  Good review of the literature.  I wished the paper was easier to read. Some of the concepts could have been illustrated to give the reader some way to visualize the intuitive notions. For example, maybe it would have been interesting to plot the rank of features a every layer for LeNet+MNIST ?  At the end of the day, if a friend asked me to summarize the paper, I would tell them :  \"Features are basically full rank. Then they use a square loss and end up with an over-parametrized system, so they can achieve loss zero (i.e. global minimum) with a multitude of parameters values.\"   Nitpicking :  \"This paper is one of the first ones, which studies CNNs.\" This sentence is strange to read, but I can understand what the authors mean.  \"This is true even if the bottom layers (from input to the wide layer) and chosen randomly with probability one.\" There's a certain meaning to \"with probability one\" when it comes to measure theory. The authors are using it correctly in the rest of the paper, but in this sentence I think they simply mean that something holds if \"all\" the bottom layers have random features.",
        "rating": 6,
        "confidence": 3,
        "writer": "official_reviewer"
      }
    ]
  },
  {
    "paper_id": "iclr_2018_H11lAfbCW",
    "paper_title": "On Characterizing the Capacity of Neural Networks Using Algebraic Topology",
    "paper_acceptance": "rejected-papers",
    "meta_review": "This paper attempts to connect the expressivity of neural networks with a measure of topological complexity. The authors present some empirical results on simplified datasets.\nAll reviewers agreed that this is an intriguing line of research, but that the current manuscript is still presenting preliminary results, and that further work is needed before it can be published. ",
    "reviews": [
      {
        "review_id": "SJHp-7Klz",
        "comment": "Paper Summary:  This paper looks at empirically measuring neural network architecture expressivity by examining performance on a variety of complex datasets, measuring dataset complexity with algebraic topology. The paper first introduces the notion of topological equivalence for datasets -- a desirable measure to use as it is invariant to superficial differences such as rotation, translation and curvature. The definition of homology from algebraic topology can then be used as a robust measure of the \"complexity\" of a dataset. This notion of difficulty focuses roughly on determining the number of holes of dimension n (for varying n) there are in the dataset, with more holes roughly leading to a more complex connectivity pattern to learn. They provide a demonstration of this on two synthetic toy datasets in Figure 1, training two (very small -- 12 and 26 neuron) single hidden layer networks on these two datasets, where the smaller of the two networks is unable to learn the data distribution of the second dataset. These synthetic datasets have a well defined data distribution, and for an empirical sample of N points, a (standard) method of determining connectivity by growing epsilon balls around each datapoint in section 2.3.  The authors give a theoretical result on the importance of homology: if a binary classifier has support homology not equal to the homology of the underlying dataset, then there is at least one point that is misclassified by the classifier. Experiments are then performed with single hidden layer networks on synthetic datasets, and a phase transition is observed: if h_phase is the number of hidden units where the phase transition happens, and h' < h < h_phase, h' has higher error and takes longer to converge than h. Finally, the authors touch on computing homology of real datasets, albeit with a low dimensional projection (e.g. down to 3 dimensions for CIFAR-10).  Main Comments  The motivation to consider algebraic topology and dataset difficulty is interesting, but I think this method is ultimately ill suited and unable to be adapted to more complex and interesting settings. In particular, the majority of experiments and justification of this method comes from use on a low dimensional manifold with either known data distribution, or with a densely sampled manifold. (The authors look at using CIFAR-10, but project this down to 3 dimensions -- as current methods for persistent homology cannot scale -- which somewhat invalidates the goal of testing this out on real data.) This is an important and serious drawback because it seems unlikely that the method described in Figure 3 of determining the connectivity patterns of a dataset are likely to yield insightful results in a high dimensional space with very few datapoints (in comparison to 2^{dimension}), where distance between datapoints is unlikely to have any nice class related correspondence.  Furthermore, while part of the motivation of this paper is to use dataset complexity measured with topology to help select architectures, experiments demonstrating that this might be useful are very rudimentary. All experiments only look at single hidden layers, and the toy task in Figure 1 and in section 3.2.1 and Figure 5 use extremely small networks (hidden size 12-26). It's hard to be convinced that these results necessarily generalize even to other larger hidden layer models. On real datasets, exploring architectures does not seem to be done at all (Section 4).   Minor Comments Some kind of typo in Thm 1? (for all f repeated twice) Small typos (missing spaces) in related work and conclusion How is h_phase determined? Empirically? (Or is there a construction?)  Review Summary:  This paper is not ready to be accepted.",
        "rating": 3,
        "confidence": 5,
        "writer": "official_reviewer"
      },
      {
        "review_id": "B19Fsy5gM",
        "comment": "The authors propose to use the homology of the data as a measurement of the expressibility of a deep neural network. The paper is mostly experimental. The theoretical section (3.1) is only reciting existing theory (Bianchini et al.). Theorem 3.1 is not surprising either: it basically says spaces with different topologies differ at some parts.   As for the experiments, the idea is tested on synthetic and real data. On synthetic data, it is shown that the number of neurons of the network is correlated with the homology it can express. On real data, the tool of persistent homology is applied. It is observed that the data in the final layer do have non-trivial signal in terms of persistent homology.  I do like the general idea of the paper. It has great potentials. However, it is much undercooked. In particular, it could be improved as follows:  * 1) the main message of the paper is unclear to me.  Results observed in the synthetic experiments seem to be a confirmation of the known results by Bianchini et al.: the Betti number a network can express is linear to the number of hidden units, h, when the input dimension n is a constant.   To be convinced, I would like to see much stronger experimental evidence: Reporting results on a single layer network is unsettling. It is known that the network expressibility is highly related to the depth (Eldan & Shamir 2016). So what about networks with more layers? Is the stratification observation statistically significant? These experiments are possible for synthetic data.   * 2) The usage of persistent homology is not well justified. A major part of the paper is devoted to persistent homology. It is referred to as a robust computation of the homology and is used in the real data experiments. However, persistent homology itself was not originally invented to recover the homology of a fixed space. It was intended to discover homology groups at all different scales (in terms of the function value). Even with the celebrated stability theorem (Cohen-Steiner et al. 2007) and statistical guarantees (Chazal et al. 2015), the relationship between the Vietoris-Rips filtration persistent homology and the homology of the classifier region/boundary is not well established. To make a solid statement, I suggest authors look into the following papers  Homology and robustness of level and interlevel sets P Bendich, H Edelsbrunner, D Morozov, A Patel, Homology, Homotopy and Applications 15 (1), 51-72, 2013  Herbert Edelsbrunner, Michael Kerber: Alexander Duality for Functions: the Persistent Behavior of Land and Water and Shore. Proceedings of the 28th Annual Symposium on Computational Geometry, pp. 249-258 (SoCG 2012)  There are also existing work on how the homology of a manifold or stratified space can be recovered using its samples. They could be useful. But the settings are different: in this problem, we have samples from the positive/negative regions, rather than the classification boundary.   Finally, the gap in concepts carries to experiments. When persistent homology of different real data are reported. It is unclear how they reflect the actually topology of the classification region/boundary. There are also significant amount of approximation due to the natural computational limitation of persistent homology. In particular, LLE and subsampling are used for the computation. These methods can significantly hurt persistent homology computation. A much more proper way is via the sparsification approach.   SimBa: An Efficient Tool for Approximating Rips-Filtration Persistence via Simplicial Batch-Collapse T. K. Dey, D. Shi and Y. Wang. Euro. Symp. Algorithms (ESA) 2016, 35:1--35:16  * 3) Finally, to support the main thesis, it is crucial to show that the topological measure is revealing information existing ones do not. Some baseline methods such as other geometric information (e.g., volume and curvature) are quite necessary.  * 4) Important papers about persistent homology in learning could be cited:  Using persistent homology in deep convolutional neural network:  Deep Learning with Topological Signatures C. Hofer, R. Kwitt, M. Niethammer and A. Uhl, NIPS 2017  Using persistent homology as kernels:  Sliced Wasserstein Kernel for Persistence Diagrams Mathieu Carri\u00e8re, Marco Cuturi, Steve Oudot, ICML 2017.  * 5) Minor comments:  Small typos here and there: y axis label of Fig 5, conclusion section.  ",
        "rating": 4,
        "confidence": 5,
        "writer": "official_reviewer"
      },
      {
        "review_id": "Sy4l2B9gG",
        "comment": "General comments:  The paper is largely inspired by a recent work of Bianchini et al. (2014) on upper bounds of Betti number sums for decision super-level sets of neural networks in different architectures. It explores empirically the relations between Betti numbers of input data and hidden unit complexity in a single hidden layer neural network, in a purpose of finding closer connections on topological complexity or expressibility of neural networks.    They report the phenomenon of phase transition or turning points in training error as the number of hidden neurons changes in their experiment. The phenomenon of turning points has been observed in many experiments, where usually researchers investigate it through the critical points of training loss such as local optimality and/or saddle points. For the first time, the paper connects the phenomenon with topological complexity of input data and decision super-level sets, as well as number of hidden units, which is inspiring.   However, a closer look at the experimental study finds some inconsistencies or incompleteness which deserves further investigations. The following are some examples.   The paper tries to identify a phase transition in number of hidden units, h_phase(D_2) = 10 from the third panel of Figure 4. However, when h=12 hidden units, the curve is above h=10 rather than below it in expectation. Why does the order of errors disagree with the order of architectures if the number of hidden neurons is larger then h_phase?  The author conjecture that if b0 = m, then m+2 hidden neurons are sufficient to get 0 training error. But the second panel of fig4 seems to be a counterexample of the conjecture. In fact h_phase(D_0 of b_0=2)=4 and h_phase (D_1 of b_0 = 3) = 6, as pointed out by the paper, has a mismatch on such a numerical conjecture.    In Figure 5, the paper seems to relate the homological complexities of data to the hidden dimensionality in terms of zero training error. What are the relations between the homological complexities of data and homological complexities of decision super-level sets of neural networks in training? Is there any correspondence between them in terms of topological transitions.   The study is restricted to 2-dimensional synthetic datasets. Although they applied topological tools to low-dimensional projection of some real data, it's purely topological data analysis. They didn't show any connection with the training or learning of neural networks. So this part is just preliminary but incomplete to the main topic of the paper.  The authors need to provide more details about their method and experiments. For example, The author didn't show from which example fig6 is generated. For other figures appended at the end of the paper, there should also be detailed descriptions of the underlying experiments.   Some Details:  Lines in fig4 are difficult to read, there are too many similar colors. Axis labels are also missing.  In fig5, the (4, 0)-item appears twice, but they are different. They should be the same, but they are not. Any mistake here?  Fig6(a) has the same problem as fig4. Besides, the use of different x-axis makes it difficult to compare with fig4. fig6(b) needs a color bar to indicate the values of correlations.   Some typos, e.g. Page 9 Line 2, 'ofPoole' should be 'of Poole'; Line 8, 'practical connectio nbetween' should be 'practical connection between'; line 3 in the 4th paragraph of page 9, 'the their are' seems to be 'there are'. Spell check is recommended before final version.  ",
        "rating": 4,
        "confidence": 5,
        "writer": "official_reviewer"
      }
    ]
  },
  {
    "paper_id": "iclr_2018_H1A5ztj3b",
    "paper_title": "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates",
    "paper_acceptance": "rejected-papers",
    "meta_review": "The paper reports unusally rapid convergence of the ResNet-56 model on CIFAR-10 when a single cycle of a cyclic learning rate schedule is used.  The effect is analyzed from several different perspectives. However, the reviewers were not convinced because the effect is only observed for one task, so they question the significance of the result. There was significant discussion of the paper by the reviewers and area chair before this decision was reached.\n\nPros:\n+ Paper illustrates a \"super-convergence\" phenomenon in which training of a ResNet-56 reaches an accuracy of 92.4% on CIFAR-10 in 10,000 iterations using a single cycle of a cyclic learning rate schedule, while a more standard piecewise-constant schedule reaches 91.2% accuracy in 80,000 iterations.\n+ There was partial, independent replication of the results on other tasks reported on OpenReview.\n\nCons:\n- In the paper, the effect is shown for only one architecture and one task.\n- In the paper, the effect is shown for only a single run.\n- There are no error bars to indicate which differences are significant.\n",
    "reviews": [
      {
        "review_id": "rJfAp3Zef",
        "comment": "The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates. Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper.  The main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum. In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well. However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge.  In the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior.   Personally i would not use the term \"convergence\" in this setting at all as the runs are very short and thus we might not be close to any region of convergence. Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all. The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping.  Pros: - Many experiments which try to study the effect Cons: -The described phenomenon seems to depend strongly on the problem surface and might never  be encountered on any problem aside of Cifar-10 - Only single runs are shown, considering the noise on those the results might not be reproducible. -Experiments are not described in detail -Experiment design feels \"ad-hoc\" and unstructured -The role and value of the many LR-plots remains unclear to me.  Form: - The paper does not maker clear how the exact schedules work. The terms are introduced but the paper misses the most basic formulas - Figures are not properly described, e.g. axes in Figures 3 a) and b) - Explicit references to code are made which require familiarity with the used framework(if at all published).  ",
        "rating": 4,
        "confidence": 3,
        "writer": "official_reviewer"
      },
      {
        "review_id": "H1yQ04YxG",
        "comment": "In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR). The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets \u2013 this is called \u201csuper-convergence\u201d. However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting.    Some specific comments by sections:  2. Related Work: This section loosely mentions other related works on SGD, topology of loss function and adaptive learning rates. The authors mention Loshchilov & Hutter in next section but do not compare it to their work. The authors do not discuss a somewhat contradictory claim from NIPS 2017 (as pointed out in the public comment): http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf  3. Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence. Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units. It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.  4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods.  5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot). Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.  6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work. However, the fact that the results only applies to CIFAR-10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work.   Overall, the work is presented as a positive result in very specific conditions but it seems more like a negative result. It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing.",
        "rating": 4,
        "confidence": 4,
        "writer": "official_reviewer"
      },
      {
        "review_id": "Hyn-NPJbz",
        "comment": "This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.  The paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers. The \"super-convergence\" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models. Also, the authors do not give a conclusive analysis under what condition it may happen.  The explanation of the cause of \"super-convergence\" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.",
        "rating": 4,
        "confidence": 3,
        "writer": "official_reviewer"
      }
    ]
  }
]